1
00:00:13,059 --> 00:00:18,220
Rachel added something which I think is super-helpful
to the wiki this week, which is in the section

2
00:00:18,220 --> 00:00:23,140
about the assignments where it talks about
going through the notebooks, there's a section

3
00:00:23,140 --> 00:00:28,590
called "How to Use the Provided Notebooks".

4
00:00:28,590 --> 00:00:34,120
Each time I talk about the teaching approach
in this class, people get a lot out of it,

5
00:00:34,120 --> 00:00:41,420
so I wanted to keep talking about that. As
we discussed before, the 2 hours that we spend

6
00:00:41,420 --> 00:00:47,180
together each week, that's not nearly enough
time for me to teach you deep learning. I

7
00:00:47,180 --> 00:00:51,430
can show you what kinds of things you need
to learn about, and I can show you where to

8
00:00:51,430 --> 00:00:56,140
look and try to give you a sense of some of
the key topics, but then the idea is that

9
00:00:56,140 --> 00:01:00,189
you're going to learn about deep-learning
during the week by doing a whole lot of experimenting.

10
00:01:00,189 --> 00:01:06,250
And one of the places you can do that experimenting
is with the help of the notebooks that we

11
00:01:06,250 --> 00:01:12,450
provided. Having said that, if you do that
by loading up the notebook and hitting shift-enter

12
00:01:12,450 --> 00:01:17,090
a bunch of times to go through each cell until
you get an error message and then you go,

13
00:01:17,090 --> 00:01:23,740
"Oh shit, I've got an error message", you're
not going to learn anything about deep-learning.

14
00:01:23,740 --> 00:01:31,090
I was almost tempted to not put the notebooks
online until the week after each class because

15
00:01:31,090 --> 00:01:36,729
it's just so much better when you can build
it yourself. But the notebooks are really

16
00:01:36,729 --> 00:01:42,210
usefull if you use them really rigorously
and thoughtfully, which is as Rachel's described

17
00:01:42,210 --> 00:01:48,689
here. Read through it and then put it aside.
Minimize it or close it or whatever, and then

18
00:01:48,689 --> 00:01:56,391
try and replicate what you just read from
scratch. And anytime you get stuck you can

19
00:01:56,391 --> 00:02:01,899
go back and open up the notebook, find the
solution to your problem, but don't copy and

20
00:02:01,899 --> 00:02:08,538
paste it. Put the notebook aside again, go
and read the documentation about what it turns

21
00:02:08,538 --> 00:02:13,840
out the solution was. Try and understand why
is this the solution, and type in that solution

22
00:02:13,840 --> 00:02:20,450
yourself from scratch. If you can do that,
it means you really understand now the solution

23
00:02:20,450 --> 00:02:23,680
to this thing you were previously stuck on,
and you've now learned something you didn't

24
00:02:23,680 --> 00:02:25,260
know before.

25
00:02:25,260 --> 00:02:29,070
You might still be stuck. And that's fine.
So if you're still stuck, you can refer back

26
00:02:29,070 --> 00:02:34,810
to the notebook again. Still don't copy and
paste the copy, but whilst having both open

27
00:02:34,810 --> 00:02:39,610
on the screen at the same time, type in the
code. That might seem pretty weird - why would

28
00:02:39,610 --> 00:02:44,340
you type in code you could copy and paste,
but just the very kinesthetic process of typing

29
00:02:44,340 --> 00:02:48,819
it in forces you to think about where are
the parantheses, where are the dots, what's

30
00:02:48,819 --> 00:02:54,890
going on. Then once you've done that, you
can try changing the inputs to that function

31
00:02:54,890 --> 00:02:59,050
and see what happens, see how it affects the
outputs, and really experiment.

32
00:02:59,050 --> 00:03:06,379
Though this process of trying to come up and
think about what step do I take next, that

33
00:03:06,379 --> 00:03:11,709
means you're thinking about the concepts you've
learned, and how do you do that step is having

34
00:03:11,709 --> 00:03:16,540
to recall how the actual libraries are working.
And most importantly, though experimenting

35
00:03:16,540 --> 00:03:21,730
with the inputs and outputs, you get this
really intuitive understanding of what's going

36
00:03:21,730 --> 00:03:22,730
on.

37
00:03:22,730 --> 00:03:32,110
So one of the questions I was thrilled to
see over the weekend, which is exactly the

38
00:03:32,110 --> 00:03:41,599
kind of thing I think is super-helpful, sravya
asked, "Okay I'm trying to understand correlate,

39
00:03:41,599 --> 00:03:51,670
so 
I sent it 2 vectors with 2 things each and

40
00:03:51,670 --> 00:03:57,752
I was happy with the result. Then I sent it
2 vectors with 3 things in, and I don't get

41
00:03:57,752 --> 00:03:58,752
it."

42
00:03:58,752 --> 00:04:02,769
So that's great. This is like taking it down
to make sure okay I really understand this.

43
00:04:02,769 --> 00:04:08,480
So I type something in and the output was
not what I expected. What's going on?

44
00:04:08,480 --> 00:04:13,340
And so then I tried it, by creating a little
spreadsheet and showed here are the 3 numbers

45
00:04:13,340 --> 00:04:18,829
and here is how it was calculated. And now
you say, "Okay, I kind of get that, but not

46
00:04:18,829 --> 00:04:26,000
fully.", then another communication, then
you now understand correlation and convolution.

47
00:04:26,000 --> 00:04:30,530
You know you do because you put it in there,
you figured out what the answer ought to be,

48
00:04:30,530 --> 00:04:37,870
and eventually the answer was what you wanted.
That is just great -- this is exactly the

49
00:04:37,870 --> 00:04:38,940
kind of experimentation.

50
00:04:38,940 --> 00:04:48,230
I find a lot of people try to jump straight
to full-scale image recognition before they've

51
00:04:48,230 --> 00:04:56,711
gotten to the 1+1 stage. You'll see I do a
lot of this stuff in Excel. This is why. Simple

52
00:04:56,711 --> 00:05:02,680
little things in Excel, in Python, that's
where you get the most experimental benefit.

53
00:05:02,680 --> 00:05:05,750
So that's what we're talking about when we
talk about experiments.

54
00:05:05,750 --> 00:05:10,350
[Time: 5 minute mark]

55
00:05:10,350 --> 00:05:17,509
I want to show you something pretty interesting.
Remember last week we look at this paper from

56
00:05:17,509 --> 00:05:26,840
Matt Zeigler where we saw what the different
layers of a convolutional neural network look

57
00:05:26,840 --> 00:05:51,240
like. So you guys remember this.

58
00:05:51,240 --> 00:05:57,440
One of the steps in How to Use the Provided
Notebooks is if you don't know why a step

59
00:05:57,440 --> 00:06:06,440
is being done, or how it works, or why the
inputs and outputs are what you observed,

60
00:06:06,440 --> 00:06:12,040
please ask. Anytime you're stuck for half
an hour, please ask.

61
00:06:12,040 --> 00:06:19,860
So far (I believe) there has been a 100% success
rate in answering questions on the forums.

62
00:06:19,860 --> 00:06:22,990
So when people ask, they get an answer.

63
00:06:22,990 --> 00:06:38,010
So part of the homework this week is ask a
question on the forum. It's okay if that question

64
00:06:38,010 --> 00:06:45,630
is about Lesson 1, it could be about anything
at all. I know a lot of people are still working

65
00:06:45,630 --> 00:06:52,910
through Cats and Dogs, or Cats and Dogs Redux.
Different people here have different backgrounds.

66
00:06:52,910 --> 00:06:59,290
There are plenty of people here who have never
used Python before; Python was not a prerequisite.

67
00:06:59,290 --> 00:07:05,110
The goal is for those of you that don't know
Python, that we give you the resources to

68
00:07:05,110 --> 00:07:10,400
learn it, and to learn it well enough to be
effective in deep-learning in it. That does

69
00:07:10,400 --> 00:07:15,550
mean that you guys are going to have to ask
more questions.

70
00:07:15,550 --> 00:07:22,229
There are no dumb questions. So if you see
somebody asking on the forum about, How do

71
00:07:22,229 --> 00:07:27,259
I analyze functional brain MRIs with a 3D
convolutional neural network, that's fine.

72
00:07:27,259 --> 00:07:33,500
That's where they are at. That's okay if you
then ask, What does this Python function do?

73
00:07:33,500 --> 00:07:37,260
Or vice versa. If you see somebody ask, What
does this Python function do, and you want

74
00:07:37,260 --> 00:07:40,629
to talk about 3D brain MRIs, do that too.

75
00:07:40,629 --> 00:07:48,139
The nice thing about the forum is that as
you can see it really is buzzing now. The

76
00:07:48,139 --> 00:07:53,530
nice thing is that the different threads allow
people to dig into the stuff that interests

77
00:07:53,530 --> 00:08:00,830
them. I'll tell you from personal experience,
the thing I learn the most from is answering

78
00:08:00,830 --> 00:08:08,270
the simplest questions. Answering that question
about a 1D convolution, I found very interesting

79
00:08:08,270 --> 00:08:12,099
because I actually didn't realize that the
reflect parameter was the default parameter

80
00:08:12,099 --> 00:08:16,740
and I didn't quite understand how it worked.
And so, answering that question I found very

81
00:08:16,740 --> 00:08:22,050
interesting. And even sometimes if you know
the answer, figuring out how to express it

82
00:08:22,050 --> 00:08:23,280
teaches you a lot.

83
00:08:23,280 --> 00:08:32,190
So asking questions at any level is always
helpful to you and to the rest of the community.

84
00:08:32,190 --> 00:08:38,500
So please, if everybody only does one part
of the assignments this week, do that one.

85
00:08:38,500 --> 00:08:50,670
Just ask a question. Here's some ideas about
questions you could ask, if you're not sure.

86
00:08:50,670 --> 00:08:58,490
Last week we kind of looked at this amazing
visualization of what goes on in a convolutional

87
00:08:58,490 --> 00:09:07,720
neural network. I want to show you something
even cooler, which is the same thing in video.

88
00:09:07,720 --> 00:09:13,440
This is by an amazing guy, Jason Yosinski,
his supervisor (Hod Lipson) and some other

89
00:09:13,440 --> 00:09:22,300
guys. It's doing the same thing, but in video.
And so I'm going to show you what's going

90
00:09:22,300 --> 00:09:23,850
on here.

91
00:09:23,850 --> 00:09:27,589
And you can download this. It's called the
deep-visualization toolbox. So if you go to

92
00:09:27,589 --> 00:09:34,490
google and search for deep-visualization toolbox,
you can do this. You can grab pictures, you

93
00:09:34,490 --> 00:09:42,440
can click on any one of the layers of a convolutional
neural network and it will visualize every

94
00:09:42,440 --> 00:09:48,279
one of the filters, the outputs of the filters
in that convolutional layer. So you can see

95
00:09:48,279 --> 00:09:52,870
here with this dog, it looks like there's
a filter here which is kind of finding edges.

96
00:09:52,870 --> 00:09:56,890
And you could even give it a video stream.
So if you give it a video stream of your own

97
00:09:56,890 --> 00:10:00,240
webcam, you'll see video stream popping up
here.

98
00:10:00,240 --> 00:10:01,800
[Time: 10 minute mark]

99
00:10:01,800 --> 00:10:07,040
So this is a great tool, and looking at this
tool now I hope will give us a better idea

100
00:10:07,040 --> 00:10:10,579
of what's going on in a convolutional neural
network. Look at this one herehe selected.

101
00:10:10,579 --> 00:10:16,160
It's clearly an edge detector. As he slides
a piece of paper over it, you get a very strong

102
00:10:16,160 --> 00:10:22,790
edge. And clearly it's specifically a horizontal
edge detector. And here is actually a visualization

103
00:10:22,790 --> 00:10:26,820
of the pixels of the filter itself. And it's
exactly what you'd expect. Remember from our

104
00:10:26,820 --> 00:10:32,230
initial Lesson 0, an edge detector has black
on one side and white on the other.

105
00:10:32,230 --> 00:10:38,740
So you can scroll through all the different
layers of this neural network. And different

106
00:10:38,740 --> 00:10:45,420
layers do different things. And the deeper
the layer, the larger the area it covers,

107
00:10:45,420 --> 00:10:52,210
and therefore the smaller the actual filter
is, and the more complex objects it can recognize.

108
00:10:52,210 --> 00:10:58,490
So here's an interesting example of a layer
5 thing, which it looks like is a face detetor.

109
00:10:58,490 --> 00:11:04,889
So you can see that as he moves his face around,
this is moving around as well.

110
00:11:04,889 --> 00:11:07,870
One of the coool things you can do with this
is you can say, Show me all the images from

111
00:11:07,870 --> 00:11:12,829
ImageNet that match this filter as much as
possible, and you can see that it's showing

112
00:11:12,829 --> 00:11:21,930
us faces. This is a really cool way to understand
what your neural network is doing, or in this

113
00:11:21,930 --> 00:11:26,079
case, what ImageNet is doing. So you can see,
another guy's come along and here we are.

114
00:11:26,079 --> 00:11:31,579
And so here you can see the actual result
in real time of the filter convolution and

115
00:11:31,579 --> 00:11:34,209
the actual recognition that it's doing.

116
00:11:34,209 --> 00:11:41,269
So clearly the face detector (which also detects
cat faces), the interesting thing about these

117
00:11:41,269 --> 00:11:46,741
types of neural net filters is that they're
often pretty subtle as to how they work. They're

118
00:11:46,741 --> 00:11:52,519
not looking for some pixels, but they really
understand concepts.

119
00:11:52,519 --> 00:11:58,460
So here's a really interesting example. Here's
one of the filters in the 5th layer which

120
00:11:58,460 --> 00:12:07,971
seems to be like an armpit detector. But actually
it's not an armpit detector, because look

121
00:12:07,971 --> 00:12:14,910
what happens if he smooths out his fabric,
it disappears. So what this actually is is

122
00:12:14,910 --> 00:12:23,529
a texture detector. It's something that detects
some kind of regular texture.

123
00:12:23,529 --> 00:12:30,209
Here's an interesting example of one which
clearly is a text detector. ImageNet did not

124
00:12:30,209 --> 00:12:35,589
have a category called, Text. One of the 1000
categories is not text. But one of the 1000

125
00:12:35,589 --> 00:12:40,430
categories is bookshelf. So you cannot find
a bookshelf if you don't know how to find

126
00:12:40,430 --> 00:12:45,030
a book and you can't find a book if you don't
know how to recognize a spine, and the way

127
00:12:45,030 --> 00:12:47,510
to recognize a spine is by finding text.

128
00:12:47,510 --> 00:12:52,350
So this is the cool thing about these neural
networks is that you don't have to tell them

129
00:12:52,350 --> 00:13:00,029
what to find. They decide what they want to
find in order to solve your problem. So I

130
00:13:00,029 --> 00:13:06,790
wanted to start at this end of "Oh my god,
deep-learning is really cool," and then jump

131
00:13:06,790 --> 00:13:14,459
back to the other end of "Deep-learning is
really simple." So everything we just saw

132
00:13:14,459 --> 00:13:19,230
works because of the things that we've learnt
about so far. And I've got a section here

133
00:13:19,230 --> 00:13:25,880
called CNN Review in Lesson 3. And Rachel
and I have started to add some of our favorite

134
00:13:25,880 --> 00:13:31,130
readings about each of these pieces. Everything
you just saw in that video consists of the

135
00:13:31,130 --> 00:13:41,810
following pieces, Matrix Products, Convolutions
(just like we saw in Excel and Python), Activations

136
00:13:41,810 --> 00:13:49,180
(such as ReLUs and SoftMax), Stochastic Gradient
Descent, which is based on BackPropogation,

137
00:13:49,180 --> 00:13:54,740
which we'll learn more about today. And that's
basically it.

138
00:13:54,740 --> 00:14:01,970
One of the challenging things is, even if
you feel comfortable with each of these 5

139
00:14:01,970 --> 00:14:08,180
pieces of a Convolutional Neural Network,
is really understanding how all of those piees

140
00:14:08,180 --> 00:14:13,630
fit together to actually do deep-learning.
So we've got 2 really good resources here

141
00:14:13,630 --> 00:14:15,079
on putting it all together (Stanford CNN Class
and Chapter 6 of Michael Nielson's book).

142
00:14:15,079 --> 00:14:22,959
So I'm going to go through each of these 6
things today, but what I suggest you do if

143
00:14:22,959 --> 00:14:28,290
there's any piece where you feel like "I'm
not confident I really know what a convolution

144
00:14:28,290 --> 00:14:34,390
is," or "I really don't know what an activation
is," see if this information is helpful, and

145
00:14:34,390 --> 00:14:38,899
maybe ask a question on the forum.

146
00:14:38,899 --> 00:14:46,000
So let's go through each of these. I think
a particularly good place to start is maybe

147
00:14:46,000 --> 00:14:50,009
Convolutions. And a good reason to start with
Convolutions is because we haven't really

148
00:14:50,009 --> 00:14:54,350
looked at them since Lesson 0, and that's
quite a while ago.

149
00:14:54,350 --> 00:14:55,350
[Time: 15 minute mark]

150
00:14:55,350 --> 00:15:05,570
So let's remind ourselves about Lesson 0.
So in Lesson 0 we learnt about what a convolution

151
00:15:05,570 --> 00:15:12,950
is by actually running a convolution against
an image. So we used the MNIST dataset. The

152
00:15:12,950 --> 00:15:24,769
MNIST dataset consists of 55,000 28x28 grey-scale
images of handwritten digits. So each one

153
00:15:24,769 --> 00:15:33,630
of these has some known label. Here's 5 examples
with the known label.

154
00:15:33,630 --> 00:15:41,449
So in order to understand what a convolution
is we tried creating a simple little 3x3 matrix.

155
00:15:41,449 --> 00:15:48,600
So the 3x3 matrix we started had -1's at the
top, 1's in the middle and 0's in the bottom.

156
00:15:48,600 --> 00:15:57,260
So we could kind of visualize that. So what
would happen if we took this 3x3 matrix and

157
00:15:57,260 --> 00:16:05,170
we slid it over every 3x3 part of this image
and we multiplied -1 by the first pixel, -1

158
00:16:05,170 --> 00:16:10,730
by the second pixel, -1 by the third pixel.
And then we moved to the next row and multiplied

159
00:16:10,730 --> 00:16:15,709
by 1, by 1, by 1, moved to the bottom row
and multipled by 0, 0, 0 ... and added them

160
00:16:15,709 --> 00:16:22,399
all together. And so we could do that for
every 3x3 area. That's what a convolution

161
00:16:22,399 --> 00:16:29,829
does. So you might remember from Lesson 0,
we looked at a little area to see what this

162
00:16:29,829 --> 00:16:44,331
looks like. So we could zoom in and here's
a small little bit of the "7". One thing that

163
00:16:44,331 --> 00:16:57,310
I think is helpful is just to look at what
is that little bit?

164
00:16:57,310 --> 00:17:05,630
So you can see that an image is just a bunch
of numbers. And lots of 0's and the things

165
00:17:05,630 --> 00:17:11,440
in between, bigger and bigger numbers until
the weights are very close to 1. So what would

166
00:17:11,440 --> 00:17:25,430
happen if we took this little 3x3 area ([0,0,0],[.35,.54,.92],[.98,.99,.99]),
and we multiply each of those 9 things by

167
00:17:25,430 --> 00:17:33,011
each of these 9 things ([-1,-1,-1],[1,1,1],[0,0,0]).
So clearly, anywhere where the first row is

168
00:17:33,011 --> 00:17:40,780
0's and the second row is 1's, this is going
to be very high when we multiply it all together

169
00:17:40,780 --> 00:17:52,300
and add things up. And so given that white
means high, you can see that when we do this

170
00:17:52,300 --> 00:17:59,940
convolution we end up with something where
the top edges become bright. Because we went

171
00:17:59,940 --> 00:18:07,090
[-1,-1,-1] times [1,1,1], times [0,0,0] and
add them all together.

172
00:18:07,090 --> 00:18:11,890
So one of the things we looked at in Lesson
0 (and we have a link to here) is this cool

173
00:18:11,890 --> 00:18:21,670
little "Image Kernels Explained Visually"
site where you can actually create any 3x3

174
00:18:21,670 --> 00:18:32,270
matrix yourself and go through any 3x3 part
of this picture and see the actual arithmetic

175
00:18:32,270 --> 00:18:39,850
and see the result. So if you're not comfortable
with convolutions, this would be a great place

176
00:18:39,850 --> 00:18:43,930
to go next.

177
00:18:43,930 --> 00:18:51,410
Question: How did you decide on the values
in the top matrix?

178
00:18:51,410 --> 00:18:57,480
Answer: Excellent question. So in order to
demonstrate an edge filter, I picked values

179
00:18:57,480 --> 00:19:04,310
based on some well-known edge filter matrices.
You can see here's a bunch of different matrices

180
00:19:04,310 --> 00:19:12,680
that this guy has. For example "top sobel",
that does a top edge filter. Or I could say

181
00:19:12,680 --> 00:19:24,700
"emboss" and you can see it creates this embossing.
Here's a better example, which is nice and

182
00:19:24,700 --> 00:19:25,700
big here.

183
00:19:25,700 --> 00:19:30,920
So these types of filters have been created
over many decades and there's lots and lots

184
00:19:30,920 --> 00:19:40,570
of filters designed to do things. So I just
picked a simple filter, which I knew from

185
00:19:40,570 --> 00:19:53,350
experience would create a top-edge filter.

186
00:19:53,350 --> 00:20:01,520
And so by the same kind of idea, if I rotate
that by 90 degrees (with -1's down the side),

187
00:20:01,520 --> 00:20:03,510
that's going to create a left-edge filter.

188
00:20:03,510 --> 00:20:05,110
[Time: 20 minute mark]

189
00:20:05,110 --> 00:20:12,030
So if I create 4 different types, and I could
also create 4 different diagonal filters like

190
00:20:12,030 --> 00:20:18,720
these, that would allow me to create top-edge,
left-edge, bottom-edge, right-edge, and then

191
00:20:18,720 --> 00:20:27,310
each diagonal edge. So I created these filters
just by hand, through a combination of common-sense

192
00:20:27,310 --> 00:20:35,080
and having read about filters, because people
spend time designing filters.

193
00:20:35,080 --> 00:20:40,960
The more interesting question then really
would be what would be the optimal way to

194
00:20:40,960 --> 00:20:47,230
design filters because it's definitely not
the case that these 8 filters are the best

195
00:20:47,230 --> 00:20:53,200
way of figuring out what's a "7", what's an
"8", and what's a "1".

196
00:20:53,200 --> 00:21:00,370
So this is what deep-learning does. What deep-learning
does is it says, "Let's start with random

197
00:21:00,370 --> 00:21:05,790
filters. Let's not design them, let's start
with totally random numbers for each of our

198
00:21:05,790 --> 00:21:14,460
filters." So we might start with 8 random
filters, each a 3x3. And we then use Stochastic

199
00:21:14,460 --> 00:21:22,130
Gradient Descent to find out what are the
optimal values of each of those sets of 9

200
00:21:22,130 --> 00:21:28,520
numbers. And that's what happens in order
to create that cool video we just saw, and

201
00:21:28,520 --> 00:21:33,020
that cool paper we saw. That's how those different
kinds of image detectors and gradient detectors

202
00:21:33,020 --> 00:21:38,670
and so forth were created. When you use Stochastic
Gradient Descent to optimize these kinds of

203
00:21:38,670 --> 00:21:45,150
values when they start out random. It figures
out that the best way to recognize images

204
00:21:45,150 --> 00:21:52,660
is by creating these kinds of different detectors,
different filters.

205
00:21:52,660 --> 00:21:59,760
Where it gets interesting is when you start
building convolutions on top of convolutions.

206
00:21:59,760 --> 00:22:37,660
We saw last week how, like if you have 3 inputs,
you can create a bunch of weight matrices.

207
00:22:37,660 --> 00:22:46,350
We saw how you could create a random matrix
and then do a matrix multiply of the inputs

208
00:22:46,350 --> 00:22:53,200
times a random matrix. We can then put it
through an activation function, such as max(0,x)

209
00:22:53,200 --> 00:23:00,260
which is called a rectified linear or ReLU
and we could then take that and multiply it

210
00:23:00,260 --> 00:23:06,300
by another weight matrix to create another
output. And then we could put that through

211
00:23:06,300 --> 00:23:14,260
max(0,x), and we could keep doing that to
create arbitrarily complex functions. And

212
00:23:14,260 --> 00:23:24,650
we looked at this really great neural networks
and deep-learning chapter where we saw visually

213
00:23:24,650 --> 00:23:31,350
how that kind of bunch of matrix products
followed by activiation functions can approximate

214
00:23:31,350 --> 00:23:34,020
any given function.

215
00:23:34,020 --> 00:23:42,720
Where it gets interesting then is instead
of just having a bunch of weight matrices

216
00:23:42,720 --> 00:23:49,900
and matrix products, what if sometimes we
had convolutions and activations. Because

217
00:23:49,900 --> 00:23:55,950
a convolution is just a subset of a matrix
product. So if you think about it, a matrix

218
00:23:55,950 --> 00:24:02,460
product says, here's 10 activations and then
a weight matrix going down to 10 activations.

219
00:24:02,460 --> 00:24:06,490
The weight matrix goes through every single
element of the first layer to every single

220
00:24:06,490 --> 00:24:11,980
element of the next layer. So if this goes
from 10 to 10, there are 100 weights. Whereas

221
00:24:11,980 --> 00:24:17,430
a convolution is just creating a subset of
those weights.

222
00:24:17,430 --> 00:24:20,530
So I'll let you think about this during the
week because it's a really interesting insight

223
00:24:20,530 --> 00:24:28,770
to kind of a convolution is identical to a
fully-connected layer, but it's just a subset

224
00:24:28,770 --> 00:24:36,300
of the weights. And so therefore everything
we've learned about stacking linear and non-linear

225
00:24:36,300 --> 00:24:40,220
layers together applies also to convolutions.

226
00:24:40,220 --> 00:24:47,260
But we also know that convolutions are particularly
well-suited to identifying interesting features

227
00:24:47,260 --> 00:24:57,750
of images. By using convolutions, it allows
us to more conveniently and quickly find powerful

228
00:24:57,750 --> 00:24:59,400
deep-learning networks.

229
00:24:59,400 --> 00:25:03,150
[Time: 25 minute mark]

230
00:25:03,150 --> 00:25:09,710
Question: Are the spreadsheets available for
download?

231
00:25:09,710 --> 00:25:17,630
Answer: The spreadsheets will be available
for download tomorrow, hopefully. We're trying

232
00:25:17,630 --> 00:25:20,380
to get to the point that we can actually get
the derivatives to work in the spreadsheet

233
00:25:20,380 --> 00:25:25,680
and we're still slightly stuck in the details.
We'll make something available tomorrow.

234
00:25:25,680 --> 00:25:34,810
Question: Are the filters the layers?
Answer: Yes, they are. This is something where

235
00:25:34,810 --> 00:25:44,530
spending a lot of time looking at simple little
convolution examples is really helpful. For

236
00:25:44,530 --> 00:25:50,470
a fully-connected layer, it's really easy.
If I have 3 inputs, then my matrix product

237
00:25:50,470 --> 00:25:56,130
will have to have 3 rows (otherwise they won't
match) and I can create as many columns as

238
00:25:56,130 --> 00:26:02,100
I like. The number of columns I create tells
me how many activations I create, because

239
00:26:02,100 --> 00:26:04,530
that's what matrix products do.

240
00:26:04,530 --> 00:26:11,110
So it's very easy to see how with what Keras
calls dense layers I can decide how big I

241
00:26:11,110 --> 00:26:18,630
want each activation layer to be. If you think
about it, you can do exactly the same thing

242
00:26:18,630 --> 00:26:27,450
with convolutions. You can decide how many
sets of 3x3 matrices you want to create at

243
00:26:27,450 --> 00:26:34,210
random, and each one will generate a different
output when applied to the image.

244
00:26:34,210 --> 00:26:49,430
So the way that VGG works, for example, the
VGG network (which we learned about in Lesson

245
00:26:49,430 --> 00:27:01,630
1) contains a bunch of layers. It contains
a bunch of convolutional layers followed by

246
00:27:01,630 --> 00:27:07,660
a Flatten. All Flatten does is just a Keras
thing that says don't think about the layers

247
00:27:07,660 --> 00:27:16,230
anymore as being X by Y by channel matrices,
think of them as being a single vector. It

248
00:27:16,230 --> 00:27:21,480
just concatenates all dimensions together,
and then it contains a bunch of fully-connected

249
00:27:21,480 --> 00:27:29,860
blocks. So each of the convolutional blocks
is (you can kind of ignore the zero-padding,

250
00:27:29,860 --> 00:27:34,880
that just adds 0's around the outside so that
your convolutions end up with the same number

251
00:27:34,880 --> 00:27:44,180
of outputs as inputs) contains a 2D convolution
followed by a MaxPooling layer.

252
00:27:44,180 --> 00:27:51,740
And you can see that it starts off with 2
convolutional layers with 64 filters, and

253
00:27:51,740 --> 00:27:56,730
then 2 convolutional layers with 128 filters,
and then 3 convolutional layers with 256 filters.

254
00:27:56,730 --> 00:28:03,600
So you can see what it's doing is it's gradually
creating more and more filters in each layer.

255
00:28:03,600 --> 00:28:08,320
Question: Are these definitions in ConvBlock
specific to VGG?

256
00:28:08,320 --> 00:28:14,360
Answer: Yes, these definitions are specific
to VGG. This is just me refactoring the model

257
00:28:14,360 --> 00:28:21,510
so there wasn't lots and lots of code, I kind
of found that these lines of code were being

258
00:28:21,510 --> 00:28:24,550
repeated so I turned it into a function.

259
00:28:24,550 --> 00:28:31,590
So why would we have the number of filters
be increasing? Well, the best way to understand

260
00:28:31,590 --> 00:28:52,430
a model is to use the summary command. Let's
go back to Lesson 1, let's go right back to

261
00:28:52,430 --> 00:28:59,240
the first thing we learned, which is the 7
lines of code that you can run in order to

262
00:28:59,240 --> 00:29:02,660
create and train a network.

263
00:29:02,660 --> 00:29:17,160
I won't wait for it to actually finish training,
but what I do want to do now is go "vgg.model.summary()".

264
00:29:17,160 --> 00:29:21,240
So any time you're creating models, it's a
really good idea to use the summary command

265
00:29:21,240 --> 00:29:27,880
to look inside them, and it tells you all
about it. So here, we can see that the input

266
00:29:27,880 --> 00:29:37,950
to our model has 3 channels, Red, Green, Blue,
and they are 224x224 images.

267
00:29:37,950 --> 00:29:48,470
After I do my first 2D convolution, I now
have 64 channels of 224x224. So I've replace

268
00:29:48,470 --> 00:29:59,500
my 3 channels with the 64, just like here
I got 8 different filters, here I've got 64

269
00:29:59,500 --> 00:30:04,220
different filters, because that's what I asked
for.

270
00:30:04,220 --> 00:30:07,370
[Time: 30 minute mark]

271
00:30:07,370 --> 00:30:17,830
So again we have the second convolution set
with 224x224 on 64. And then we do MaxPooling.

272
00:30:17,830 --> 00:30:25,900
So MaxPooling (remember from Lesson 0) was
this thing where we simplified things. We

273
00:30:25,900 --> 00:30:33,770
started out with these 28x28 images and we
said let's take each 7x7 block and replace

274
00:30:33,770 --> 00:30:40,720
that entire 7x7 block with a single pixel
which contains the maximum pixel value.

275
00:30:40,720 --> 00:30:48,090
So here is this 7x7 block, which is basically
all grey so we end up with a very low number

276
00:30:48,090 --> 00:30:58,810
here. And so instead of being 28x28 it becomes
4x4 because we are replacing every 7x7 block

277
00:30:58,810 --> 00:31:05,990
with a single pixel. That's all MaxPooling
does. The reason we have MaxPooling is because

278
00:31:05,990 --> 00:31:13,770
it allows us to gradually simplify our image
so that we get larger and larger areas and

279
00:31:13,770 --> 00:31:16,490
smaller and smaller images.

280
00:31:16,490 --> 00:31:23,121
So if we look at VGG after our MaxPooling
layer, we no longer have 224x224, we now have

281
00:31:23,121 --> 00:31:34,550
112x112. Later on, we do another MaxPooling
and we end up with 56x56. Later on we do another

282
00:31:34,550 --> 00:31:44,920
MaxPooling and we end up with 28x28. Each
time we do a MaxPooling we're reducing the

283
00:31:44,920 --> 00:31:51,500
resolution of our image -- it's not really
an image anymore,but of our kind of analyzed

284
00:31:51,500 --> 00:31:52,890
filters.

285
00:31:52,890 --> 00:31:58,100
And so as we're reducing the resolution, we
need to hike up the number of filters otherwise

286
00:31:58,100 --> 00:32:06,070
we're losing information. That's really why
each time we have a MaxPooling, we double

287
00:32:06,070 --> 00:32:10,940
the number of filters. That means that for
every layer, we're keeping the same amount

288
00:32:10,940 --> 00:32:12,540
of information content.

289
00:32:12,540 --> 00:32:24,250
Question: If feature maps are position invariant.
Does it matter whether it sees a pattern in

290
00:32:24,250 --> 00:32:33,830
one part of the picture (for example, top
right), or another part? But I guess it must

291
00:32:33,830 --> 00:32:36,610
not be, because CNNs work so well.

292
00:32:36,610 --> 00:32:44,720
Answer: It starts out with a very important
insight which is a convolution is position

293
00:32:44,720 --> 00:32:55,420
invariant. So in other words, this thing we
created, which is a top edge detector. This

294
00:32:55,420 --> 00:33:02,780
matrix here, which is a top edge detector,
we can apply that to any part of the image

295
00:33:02,780 --> 00:33:09,550
and get top edges from every part of the image.
Earlier on, when we looked at the Jason Yosinski

296
00:33:09,550 --> 00:33:14,480
video, it showed there was a face detector,
which could find a face in any part of an

297
00:33:14,480 --> 00:33:15,480
image.

298
00:33:15,480 --> 00:33:20,410
So this is fundamental to how a convolution
works. A convolution is position-invariant.

299
00:33:20,410 --> 00:33:25,350
It finds a patterns regardless of whereabouts
in the image it is.

300
00:33:25,350 --> 00:33:32,030
Now that is a very powerful idea, because
when we want to say, find a face, we want

301
00:33:32,030 --> 00:33:36,880
to be able to find eyes, and we want to be
able to find eyes regardless of whether the

302
00:33:36,880 --> 00:33:43,760
face is in the top left or the bottom right.
So position-invariance is important.

303
00:33:43,760 --> 00:33:51,810
But also we need to be able to identify position
to some extent because if there's 4 eyes,

304
00:33:51,810 --> 00:33:55,310
or if there's an eye in the top corner and
in the bottom corner, then something weird

305
00:33:55,310 --> 00:33:59,860
is going on. Or if the eyes and the nose are
not in the right positions.

306
00:33:59,860 --> 00:34:08,020
So how does a convolutional neural network
both have this location-invariant filter but

307
00:34:08,020 --> 00:34:16,699
also handle location? And the trick is that
every one of the 3x3 filters cares deeply

308
00:34:16,699 --> 00:34:23,579
about where each of these 3x3 things is. And
so as we go down through the layers of our

309
00:34:23,580 --> 00:34:35,040
model, from 224 to 112 to 56 to 28 to 14 to
7, at each one of these stages. Think about

310
00:34:35,040 --> 00:34:42,649
the stage that goes from 14x14 to 7x7, these
filters are now looking at large parts of

311
00:34:42,649 --> 00:34:48,840
the image. We're now at a point where the
3x3 can say there needs to be an eye here,

312
00:34:48,840 --> 00:34:52,219
an eye here and a nose here.

313
00:34:52,219 --> 00:34:57,710
So this is one of the cool things about convolutional
neural networks, they can find features everywhere

314
00:34:57,710 --> 00:35:03,840
but they can also build things which care
about how features relate to each other positionally,

315
00:35:03,840 --> 00:35:13,340
so you get to do both.

316
00:35:13,340 --> 00:35:23,230
[Time: 35 minute mark]

317
00:35:23,230 --> 00:35:30,720
Question: Do we need zero padding?

318
00:35:30,720 --> 00:35:35,620
Answer: Zero padding is literally something
that's going to stick 0's around the outside

319
00:35:35,620 --> 00:35:42,580
of the image. If you think about what a convolution
does, it's taking a 3x3 and moving it over

320
00:35:42,580 --> 00:35:49,480
the image. If you do that, when you get to
the edge, what do you do? Because at the very

321
00:35:49,480 --> 00:35:55,580
edge, you can't move your 3x3 any further.
Which means that if you only do what's called

322
00:35:55,580 --> 00:36:00,740
a Valid Convolution, which means you always
make sure your 3x3 filter fits entirely within

323
00:36:00,740 --> 00:36:08,460
your image, you end up losing 2 pixels from
the side and 2 pixels from the top each time.

324
00:36:08,460 --> 00:36:16,090
There's actually nothing wrong with that.
A little inelegant. It's kind of nice to be

325
00:36:16,090 --> 00:36:21,050
able to have the size each time and be able
to see what's going on, so people like doing

326
00:36:21,050 --> 00:36:27,560
what's called Same Convolutions so if you
add a black border around the outside, then

327
00:36:27,560 --> 00:36:33,010
the result of your convolution is exactly
the same size as your input. That is literally

328
00:36:33,010 --> 00:36:34,460
the only reason to do it.

329
00:36:34,460 --> 00:36:39,920
In fact, this is a rather inelegant way of
doing zero padding and then convolution. In

330
00:36:39,920 --> 00:36:44,270
fact, there's a parameter to nearly every
library's convolution function where you can

331
00:36:44,270 --> 00:36:54,560
say I want Valid or Full or Half. This means
do you add no black pixels, 1 black pixel

332
00:36:54,560 --> 00:37:00,960
or 2 black pixels. I don't quite know why
this one does it this way. It's really doing

333
00:37:00,960 --> 00:37:09,000
two functions where one would have done, but
it does the job. So there's not right answer

334
00:37:09,000 --> 00:37:10,160
to that question.

335
00:37:10,160 --> 00:37:14,810
Question: Do convolution neural networks work
for cartoons?

336
00:37:14,810 --> 00:37:22,220
Answer: Convolutional neural networks work
fine for cartoons, however, fine-tuning (which

337
00:37:22,220 --> 00:37:29,280
is fundamental to everything we've learned
so far). It's going to be difficult to fine-tune

338
00:37:29,280 --> 00:37:35,350
from an ImageNet model to a cartoon. Because
an ImageNet model, remember all those pictures

339
00:37:35,350 --> 00:37:40,370
of corn we looked at and all those pictures
of dogs we looked at. An ImageNet model has

340
00:37:40,370 --> 00:37:45,930
learned to find the kinds of features that
are in photos of objects that are out there

341
00:37:45,930 --> 00:37:51,440
in the world. And those are very different
kinds of photos from what you see in a cartoon.

342
00:37:51,440 --> 00:37:58,150
So if you want to be able to build a cartoon
neural network, you'll need to either find

343
00:37:58,150 --> 00:38:03,401
somebody else who'se already trained a neural
network for cartoons and fine-tune that, or

344
00:38:03,401 --> 00:38:09,680
you're going to have to create a really big
corpus of cartoons and create your own ImageNet

345
00:38:09,680 --> 00:38:16,410
equivalent.

346
00:38:16,410 --> 00:38:30,190
Question: Why doesn't an ImageNet network
translate to cartoons, given that an eye is

347
00:38:30,190 --> 00:38:31,340
a circle.

348
00:38:31,340 --> 00:38:39,730
Answer: Because the nuance level of a CNN
is very high. It doesn't think of an eye as

349
00:38:39,730 --> 00:38:45,000
being a circle. It knows that an eye is very
specific, it has particular gradients and

350
00:38:45,000 --> 00:38:51,090
particular shapes and a particular way that
the light reflects off of it, so when it sees

351
00:38:51,090 --> 00:39:01,480
a round blob there, it has no ability to abstract
that out and say, oh I guess that's an eye.

352
00:39:01,480 --> 00:39:07,930
One of the big shortcomings of CNNs is that
they can only recognize things that you specifically

353
00:39:07,930 --> 00:39:16,860
give them to recognize. If you feed a neural
net with a wide rane of photos and drawings,

354
00:39:16,860 --> 00:39:22,740
maybe it would learn about that kind of abstraction.
To my knowledge, that's never been done. It

355
00:39:22,740 --> 00:39:28,680
would be a very interesting question. It must
be possible. I'm not sure how many examples

356
00:39:28,680 --> 00:39:31,560
you would need and what kind of architecture
you would use.

357
00:39:31,560 --> 00:39:35,330
[Time: 40 minute mark]

358
00:39:35,330 --> 00:39:43,430
Question: Correlate versus convolution?
Answer: In this particular example I used

359
00:39:43,430 --> 00:39:55,100
correlate, not convolution. So one of the
things we briefly mentioned in Lesson 1 is

360
00:39:55,100 --> 00:40:01,490
that convolve and correlate are exactly the
same thing except that convolve is equal to

361
00:40:01,490 --> 00:40:07,350
correlate of an image with a filter that has
been rotated by 90 degrees.

362
00:40:07,350 --> 00:40:17,430
So you can see convolve images with 90 degrees
filter looks exactly the same and np.allclose

363
00:40:17,430 --> 00:40:29,180
= True. So convolve and correlate are identical,
except that correlate is more intuitive. Correlate

364
00:40:29,180 --> 00:40:35,540
each one goes rows then columns, whereas with
convolve one goes along rows and the other

365
00:40:35,540 --> 00:40:42,740
one goes down columns. I prefer to think about
correlate because it's just more intuitive.

366
00:40:42,740 --> 00:40:49,820
Convolve originally came from physics, I think.
It's also a basic math operation. They're

367
00:40:49,820 --> 00:40:55,190
various reasons that people sometimes find
it more intuitive to think about convolution,

368
00:40:55,190 --> 00:41:00,410
but in terms of everything they can do with
a neural net, it doesn't matter which one

369
00:41:00,410 --> 00:41:06,150
you're using. And in fact, in many libraries
you set a parameter to True or False to decide

370
00:41:06,150 --> 00:41:10,470
whether or not internally it uses correlation
or convolution. And of course, the results

371
00:41:10,470 --> 00:41:16,080
are going to be identical.

372
00:41:16,080 --> 00:41:27,070
So let's go back to our CNN review. So our
network architecture is a bunch of matrix

373
00:41:27,070 --> 00:41:33,090
products, or more generally, linear layers.
And remember a convolution is just a subset

374
00:41:33,090 --> 00:41:38,580
of a matrix product, so it's also a linear
layer. A bunch of matrix products or convolutions

375
00:41:38,580 --> 00:41:45,660
stacked with alternating non-linear activation
functions. And specifically we looked at the

376
00:41:45,660 --> 00:41:54,470
activation function which was the rectified
linear unit, which is just max(0,x). So that's

377
00:41:54,470 --> 00:41:59,730
an incredibly simple activation function,
but it's by far the most common. It works

378
00:41:59,730 --> 00:42:04,830
really really well for the internal parts
of a neural network.

379
00:42:04,830 --> 00:42:11,020
I want to introduce one more activation function
today, and you can read more about it in Lesson

380
00:42:11,020 --> 00:42:27,790
2, in the About Activation Functions section.
And you can see I have all the derivations

381
00:42:27,790 --> 00:42:35,350
and the details of the activation functions
here. I want to talk about one called the

382
00:42:35,350 --> 00:42:46,270
SoftMax function. And SoftMax is defined as
follows, e^x.i divided by the sum of e^x.i.

383
00:42:46,270 --> 00:42:48,030
What is this all about?

384
00:42:48,030 --> 00:42:53,620
SoftMax is used not for the middle layers
of a deep-learning network, but for the last

385
00:42:53,620 --> 00:42:59,360
layer. The last layer of a neural network,
think about what it's trying to do for classification,

386
00:42:59,360 --> 00:43:05,560
it's trying to match to a one-hot encoded
output. Remember a one-hot encoded output

387
00:43:05,560 --> 00:43:10,020
is a vector with all zeros and just a 1 in
one spot.

388
00:43:10,020 --> 00:43:16,740
And we had for Cats and Dogs two spots. The
first one was a 1 if it was a cat, the second

389
00:43:16,740 --> 00:43:19,500
one was a 1 if it was a dog.

390
00:43:19,500 --> 00:43:26,630
So in general, if we're doing classification,
we want our output to have one high number

391
00:43:26,630 --> 00:43:31,700
and all of the other ones low. That's going
to be easier to create this one-hot encoded

392
00:43:31,700 --> 00:43:36,580
output. Furthermore, we would like to be able
to interpret these as probabilites, which

393
00:43:36,580 --> 00:43:39,040
means all of the outputs have to add to 1.

394
00:43:39,040 --> 00:43:43,640
So we've got these 2 requirements here: Our
final layer's activations should add to 1

395
00:43:43,640 --> 00:43:50,640
and one of them should be higher than all
the rest. This particular function does exactly

396
00:43:50,640 --> 00:43:56,030
that and we will look at that by looking (as
usual) at a spreadsheet.

397
00:43:56,030 --> 00:44:05,600
So here is an example of what an output layer
might contain. Here is e to the power of each

398
00:44:05,600 --> 00:44:12,710
of those things to the left. Here is the sum
of e to the power of those things. And here

399
00:44:12,710 --> 00:44:21,840
is the thing to the left, divided by the sum
of them. In other words, SoftMax.

400
00:44:21,840 --> 00:44:27,480
You can see that we start with a bunch of
numbers that all have a similar kind of scale

401
00:44:27,480 --> 00:44:32,860
and we end up with a bunch of numbers that
sum to 1 and one of them is much higher than

402
00:44:32,860 --> 00:44:34,340
the others.

403
00:44:34,340 --> 00:44:43,610
So in general, when we design neural networks,
we want to come up with architectures (by

404
00:44:43,610 --> 00:44:55,710
which I mean convolutions, fully connected
layers, activation functions) where replicating

405
00:44:55,710 --> 00:44:59,300
the output we want is as convenient as possible

406
00:44:59,300 --> 00:45:00,300
[Time: 45 minute mark]

407
00:45:00,300 --> 00:45:05,740
So in this case our activation function for
the last layer makes it pretty convenient,

408
00:45:05,740 --> 00:45:11,230
pretty easy to come up with something that
looks like a one-hot encoded output.

409
00:45:11,230 --> 00:45:16,150
And so the easier it is for our neural net
to create the thing we want, the faster it

410
00:45:16,150 --> 00:45:21,240
is going to get there and the more likely
it is to get there in way that's quite accurate.

411
00:45:21,240 --> 00:45:29,530
So we've learned that any big enough, deep
enough neural network because of the universal

412
00:45:29,530 --> 00:45:35,640
approximation theorem can approximate any
function at all. And we know that stochastic

413
00:45:35,640 --> 00:45:40,200
gradient descent can find the parameters for
any of these, which kind of leaves you thinking,

414
00:45:40,200 --> 00:45:47,190
Why do we need 7 weeks of neural network training.
Like any architecture ought to work.

415
00:45:47,190 --> 00:45:54,520
And indeed that's true. If you have long enough,
any architecture can translate Hungarian into

416
00:45:54,520 --> 00:45:59,670
English, any architecture can recognize Cats
vs Dogs, any architecture can analyze Hillary

417
00:45:59,670 --> 00:46:06,690
Clinton's emails, as long as it's big enough.
However, some of them do it much faster than

418
00:46:06,690 --> 00:46:12,590
others. They train much faster than others.
A bad architectecture could take so long to

419
00:46:12,590 --> 00:46:17,360
train that it doesn't train in the amount
of years you have left in your lifetime.

420
00:46:17,360 --> 00:46:22,270
And that's why you care about things like
convolutional neural networks instead of just

421
00:46:22,270 --> 00:46:27,240
fully connected layers all the way through.
That's why we care about having a SoftMax

422
00:46:27,240 --> 00:46:31,790
as the last layer rather than just a linear
last layer. So we try to make it as convenient

423
00:46:31,790 --> 00:46:37,920
as possible for our network to create the
thing that we want to create.

424
00:46:37,920 --> 00:47:22,000
Question: What is the theoretical justification
for doing SoftMax?

425
00:47:22,000 --> 00:47:35,850
Answer: I don't do theoreticaly justifications,
I do intuitive justifications. There is a

426
00:47:35,850 --> 00:47:40,850
great book for theoretical justifications
and it's available for free. If you just google

427
00:47:40,850 --> 00:47:48,860
for deep-learning book, or go to deeplearningbook.org.
It actually does have a theoretical justification

428
00:47:48,860 --> 00:47:51,310
for why we use SoftMax.

429
00:47:51,310 --> 00:48:00,820
The short version basically is as follows:
SoftMax contains an e-to-the in it. Our log

430
00:48:00,820 --> 00:48:08,180
loss layer contains a log in it. The two nicely
mesh up against each other. And the derivative

431
00:48:08,180 --> 00:48:14,100
of the two together is just (a - b). That's
kind of the short version, but I will refer

432
00:48:14,100 --> 00:48:18,560
you to the deep-learning book for more information
about the theoretical justification.

433
00:48:18,560 --> 00:48:24,360
The intuitive justification is because we
have an e-to-the here, it makes a big number

434
00:48:24,360 --> 00:48:30,021
really really big and therefore once we take
one divided by the sum of the others, we end

435
00:48:30,021 --> 00:48:33,890
up with one number that tends to be bigger
than all the rest, and that is very close

436
00:48:33,890 --> 00:48:38,800
to the one-hot-encoded output that we're trying
to match.

437
00:48:38,800 --> 00:48:46,650
Question: Could a network learn identical
filters?

438
00:48:46,650 --> 00:48:52,420
Answer: A network absolutely could learn identical
filters, but it won't. The reason it won't

439
00:48:52,420 --> 00:48:59,070
is because it's not optimal. Stochastic gradient
descent is an optimization procedure. It will

440
00:48:59,070 --> 00:49:04,040
come up with (if you train it for long enough,
with an appropriate learning rate) the optimal

441
00:49:04,040 --> 00:49:11,400
set of filters. Having the same filter twice
is never optimal. That's redundant. So as

442
00:49:11,400 --> 00:49:18,810
long as you start off with random weights,
then it can learn to find the optimal set

443
00:49:18,810 --> 00:49:29,820
of filters, which will not incude duplicate
filters. These are all fantastic questions.

444
00:49:29,820 --> 00:49:43,650
So in this review, we've done our different
layers, and these different layers get optimized

445
00:49:43,650 --> 00:49:52,990
with SGD. And so last week we learnt about
SGD by using this extremely simple example

446
00:49:52,990 --> 00:50:01,860
where we said let's define a function which
is a line, A*x + B. Let's create some data

447
00:50:01,860 --> 00:50:09,950
that matches the line (that is our X's and
Y's). Let's define a loss function which is

448
00:50:09,950 --> 00:50:15,940
the sum-of-square errors. And let's say we
no longer know what a and b are, so let's

449
00:50:15,940 --> 00:50:17,060
start with some guess.

450
00:50:17,060 --> 00:50:18,630
[Time: 50 minute mark]

451
00:50:18,630 --> 00:50:24,100
And obviously the loss for our guess is pretty
high. And let's now try and come up with a

452
00:50:24,100 --> 00:50:30,860
procedure where each step makes the loss a
little bit better by making A and B a little

453
00:50:30,860 --> 00:50:34,300
bit better. And the way we did that was very
simple.

454
00:50:34,300 --> 00:50:41,550
We calculated the derivative of the loss with
respect to each of A and B. And that means

455
00:50:41,550 --> 00:50:48,310
that the derivative of the loss with respect
to B is, if I increase B by a bit, how does

456
00:50:48,310 --> 00:50:53,190
the loss change? And the derivative of the
loss with respect to A means, as I change

457
00:50:53,190 --> 00:51:00,630
A a bit, how does the loss change? If I know
those 2 things then I know that I should subtract

458
00:51:00,630 --> 00:51:08,460
the derivative times some learning rate, and
as long as our learning rate is low enough,

459
00:51:08,460 --> 00:51:15,660
we know that this is going to make our A guess
a little bit better. And we do the same for

460
00:51:15,660 --> 00:51:17,330
our B guess.

461
00:51:17,330 --> 00:51:23,020
And so we learnt that that is the entirety
of SGD. Run that again and again and again.

462
00:51:23,020 --> 00:51:27,640
And indeed, we set up something that would
run it again and again and again in an animation

463
00:51:27,640 --> 00:51:33,950
loop and we saw that indeed it dows optimize
our line.

464
00:51:33,950 --> 00:51:40,970
The tricky thing for me with deep learning
is jumping from this kind of easy-to-visualize

465
00:51:40,970 --> 00:51:48,020
intuition, if I run this little derivative
on these 2 things a bunch of times it optimizes

466
00:51:48,020 --> 00:52:00,500
this line. I can then create a set of layers
with hundreds of millions of parameters that

467
00:52:00,500 --> 00:52:07,050
in theory can match any possible function,
and it's going to do exactly the same thing.

468
00:52:07,050 --> 00:52:12,010
So this is where our intuition breaks down,
which is this incredibly simple thing called

469
00:52:12,010 --> 00:52:19,730
SGD is capable of creating these incredibly
sophisticated deep-learning models. We really

470
00:52:19,730 --> 00:52:26,970
have to just respect the basics of what's
gong on. We know it's going to work and we

471
00:52:26,970 --> 00:52:28,570
can see it does work.

472
00:52:28,570 --> 00:52:35,640
But even when you've trained dozens of deep-learning
models, it's still surprising that it does

473
00:52:35,640 --> 00:52:40,790
works. It's always a bit shocking when you
start without any ability to analyze some

474
00:52:40,790 --> 00:52:45,080
problem. You start with some random weights,
you start with a general architecture, you

475
00:52:45,080 --> 00:52:50,250
throw some data in, do an SGD, and you end
up with something that works. Hopefully now

476
00:52:50,250 --> 00:52:57,869
it makes sense, you can see why that happens,
but it takes doing it a few times to really

477
00:52:57,869 --> 00:53:10,050
intuitive understand 
it really does work.

478
00:53:10,050 --> 00:53:13,090
Question: Can you use SoftMax for multi-class
classification.

479
00:53:13,090 --> 00:53:19,010
Answer: Absolutely yes. In fact, the answer
I showed here was such an example. I showed

480
00:53:19,010 --> 00:53:31,800
that these outputs were for cat, dog, plane,
fish, and building. So this might be what

481
00:53:31,800 --> 00:53:41,220
these 5 things represent. And so this is exactly
showing a SoftMax for a multi-class output.

482
00:53:41,220 --> 00:53:47,910
You just have to make sure that your neural
net has as many outputs as you want. To do

483
00:53:47,910 --> 00:53:54,780
that, you just have to make sure that the
last weight layer in your neural net has as

484
00:53:54,780 --> 00:53:59,920
many columns as you want. The number of columns
in your final weight matrix tells you how

485
00:53:59,920 --> 00:54:08,500
many outputs.

486
00:54:08,500 --> 00:54:15,250
If you want to create somethat that would
find more than one thing, then no, SoftMax

487
00:54:15,250 --> 00:54:22,050
would not be the best way to do that. I'm
not sure if we're going to cover that in this

488
00:54:22,050 --> 00:54:33,710
set of classes. If we don't we'll do it next
year.

489
00:54:33,710 --> 00:54:39,510
Question: Different neural networks, how do
you combine them?

490
00:54:39,510 --> 00:54:47,000
Answer: Let's go back to the question about
3x3 filters, and more generally, how do we

491
00:54:47,000 --> 00:54:49,250
pick an architecture.

492
00:54:49,250 --> 00:55:00,080
So the question of like, the VGG authors used
3x3 filters. The 2012 ImageNet winners used

493
00:55:00,080 --> 00:55:04,210
a combination of 7x7 and 11x11 filters.

494
00:55:04,210 --> 00:55:06,600
[Time: 55 minute mark]

495
00:55:06,600 --> 00:55:12,140
What has happened over the last few years
since then is people have realized that 3x3

496
00:55:12,140 --> 00:55:20,380
fitlers are just better. The original insight
for this was actually that Matt Zeigle visualization

497
00:55:20,380 --> 00:55:25,500
paper I showed you. It's really worth reading
that paper because he really shows by looking

498
00:55:25,500 --> 00:55:31,740
at pictures of all the stuff going on inside
a CNN, it clealy works better when you have

499
00:55:31,740 --> 00:55:34,420
smaller filters and more layers.

500
00:55:34,420 --> 00:55:39,690
I'm not going to go into the theoretical justification
as to why. For the sake of applying CNNs,

501
00:55:39,690 --> 00:55:46,170
all you need to know is there's really no
reason to use anything but 3x3 filters. So

502
00:55:46,170 --> 00:55:52,010
that's a nice simple rule-of-thumb which always
works, 3x3 filters.

503
00:55:52,010 --> 00:56:01,060
Okay, how many layers of 3x3 filters? This
is where there is not any standard agreed-upon

504
00:56:01,060 --> 00:56:08,780
technique. Reading lots of papers, looking
at lots of Kaggle winners, you will over time

505
00:56:08,780 --> 00:56:15,070
get a sense of, for a problem of this level
of complexity, you tend to need this many

506
00:56:15,070 --> 00:56:16,070
filters.

507
00:56:16,070 --> 00:56:22,290
There have been various people that have tried
to simplify this. But we're really still at

508
00:56:22,290 --> 00:56:27,060
the point where the answer is try a few different
architectures and see what works.

509
00:56:27,060 --> 00:56:32,700
The same applies to this question of how many
filters per layer. So in general this idea

510
00:56:32,700 --> 00:56:38,840
of having 3x3 filters with MaxPooling and
doubling the number of filters each time you

511
00:56:38,840 --> 00:56:42,700
do MaxPooling is a pretty good rule-of-thumb.

512
00:56:42,700 --> 00:56:48,730
How many do you start with? You've just kind
of got to experiment. And actually today we're

513
00:56:48,730 --> 00:56:52,730
going to see an example of how that works.

514
00:56:52,730 --> 00:57:00,961
Question: If you had a much larger image,
what would you do?

515
00:57:00,961 --> 00:57:04,210
Answer: If you had a much larger image, what
would you do? For example, on Kaggle there's

516
00:57:04,210 --> 00:57:08,580
a diabetic retinopathy competition which has
pictures of eyeballs which are at quite a

517
00:57:08,580 --> 00:57:13,380
high resolution, a couple thousand by a couple
thousand.

518
00:57:13,380 --> 00:57:20,650
The question of how to deal with large images
is as yet unsolved. So if you actually look

519
00:57:20,650 --> 00:57:26,060
at the winners of that Kaggle competition,
all of the winners resampled that image down

520
00:57:26,060 --> 00:57:34,020
to 512x512. I find that quite depressing.
It's clearly not the right approach.

521
00:57:34,020 --> 00:57:37,740
I'm pretty sure I know what the right approach
is. I'm pretty sure the right approach is

522
00:57:37,740 --> 00:57:42,720
to do what the eye does. The eye does something
call foviation, which means that when I look

523
00:57:42,720 --> 00:57:47,390
directly at something the thing in the middle
is very high-res and very clear and the stuff

524
00:57:47,390 --> 00:57:52,390
on the outside is not.

525
00:57:52,390 --> 00:57:55,800
I think a lot of people now are generally
in agreement with the idea that if we could

526
00:57:55,800 --> 00:58:01,020
come up with an architecture which kind of
has this concept of foviation.

527
00:58:01,020 --> 00:58:05,010
And then secondly we need something (and there
are some good techniques on this already)

528
00:58:05,010 --> 00:58:08,830
called Attentional Models. An Attentional
Model is something that says, Okay the thing

529
00:58:08,830 --> 00:58:15,440
that I'm looking for is not in the middle
of my view, but my low-res peripheral vision

530
00:58:15,440 --> 00:58:19,970
thinks it might be over there, let's focus
my attention over there.

531
00:58:19,970 --> 00:58:25,260
And we're going to start looking at Recurrent
Neural Networks next week. And we can use

532
00:58:25,260 --> 00:58:30,011
Recurrent Neural Networks to build Attentional
Models that allow us to search trough an image

533
00:58:30,011 --> 00:58:36,840
to find areas of interest. That is a very
active area of research, but as of yet is

534
00:58:36,840 --> 00:58:40,619
not really finalized.

535
00:58:40,619 --> 00:58:46,150
By the time this turns into a MOOC and a video,
I wouldn't be surprised if that has been much

536
00:58:46,150 --> 00:58:48,020
better solved. It's moving very quickly.

537
00:58:48,020 --> 00:58:52,450
Question: Didn't the Matt Zeigler paper have
larger filters?

538
00:58:52,450 --> 00:58:59,660
Answer: The Matt Zeigler paper showed larger
filters because he was showing what AlexNet,

539
00:58:59,660 --> 00:59:06,000
the 2012 winner, looked like. And then later
on in the paper he said, based on what it

540
00:59:06,000 --> 00:59:12,690
looks like, here are some suggestions about
how to build better models.

541
00:59:12,690 --> 00:59:28,840
So let us know finalize our review by looking
at fine-tuning. So we learnt how to do fine-tuning

542
00:59:28,840 --> 00:59:36,880
using the little VGG class that I built, which
is like one line of code, vgg.finetune.

543
00:59:36,880 --> 00:59:43,119
We also learnt about kind of conceptually
how could you take 1000 predictions of all

544
00:59:43,119 --> 00:59:48,609
the 1000 ImageNet categories and turn them
into 2 predictions, which is just a Cat or

545
00:59:48,609 --> 01:00:07,540
a Dog by building a simple linear model (from
last week's lesson) that took as input the

546
01:00:07,540 --> 01:00:16,300
1000 ImageNet catgory predictions and took
the true Cat and Dog labels as output and

547
01:00:16,300 --> 01:00:18,090
we just created a linear model of that.

548
01:00:18,090 --> 01:00:19,300
[Time: 1 hour mark]

549
01:00:19,300 --> 01:00:37,750
And so that was this, here is that linear
model. It's got 1000 inputs and 2 outputs.

550
01:00:37,750 --> 01:00:46,850
And to train that linear model, it took less
than a second to train and we got 97.7% accuracy.

551
01:00:46,850 --> 01:00:55,420
So this was actually pretty effective. Why
was it pretty effective to take 1000 predictions

552
01:00:55,420 --> 01:01:01,740
of is it a cat, is it a fish, is it a bird,
is it a poodle, is it a pug, is it a plane

553
01:01:01,740 --> 01:01:04,470
and turn it into is it a cat or is it a dog.

554
01:01:04,470 --> 01:01:12,750
The reason it worked so well is because the
original ImageNet architecture was already

555
01:01:12,750 --> 01:01:18,770
trained to do something very similar to what
we wanted our model to do. We wanted our model

556
01:01:18,770 --> 01:01:24,010
to separate cats from dogs and the ImageNet
model already separated lots of different

557
01:01:24,010 --> 01:01:27,360
types of cats and lots of different types
of dogs, amongst other things as well.

558
01:01:27,360 --> 01:01:34,050
So the thing we were trying to do was just
a subset of what ImageNet already does. So

559
01:01:34,050 --> 01:01:39,640
that was why starting with 1000 predictions
and buiding a simple linear model worked so

560
01:01:39,640 --> 01:01:41,869
well.

561
01:01:41,869 --> 01:01:46,240
This week you're going to be looking at the
State Farm Competiton, and in the State Farm

562
01:01:46,240 --> 01:02:00,320
Competion you're going to be looking at pictures
like this one and this one and his one. And

563
01:02:00,320 --> 01:02:05,530
your job will not be whether or not to decide
if it's a person or a dog or a cat, your job

564
01:02:05,530 --> 01:02:11,869
will be to decide is this person driving in
a distracted way or not. That is not something

565
01:02:11,869 --> 01:02:18,400
that the original ImageNet categories included.
And therefore this same technique is not going

566
01:02:18,400 --> 01:02:21,320
to work.

567
01:02:21,320 --> 01:02:27,570
So what do you do if you need to go further?
What do you do if you need to predict something

568
01:02:27,570 --> 01:02:34,300
which is very different from what the original
model did? The answer is to throw away some

569
01:02:34,300 --> 01:02:42,359
of the later layers in the model and retrain
them from scratch, that's called fine-tuning.

570
01:02:42,359 --> 01:02:45,440
And so that is pretty simple to do.

571
01:02:45,440 --> 01:02:52,550
So if we just want to fine-tune the last layer,
we can just go model.pop (that removes the

572
01:02:52,550 --> 01:02:58,920
last layer), we can then say make all of the
other layers non-trainable (so that means

573
01:02:58,920 --> 01:03:05,990
it won't update those weights) and then add
a new fully-connected dense layer at the end

574
01:03:05,990 --> 01:03:14,369
with just our Dog and Cat (our 2 activations)
and then go ahead and fit that model.

575
01:03:14,369 --> 01:03:20,050
So that is the simplest kind of fine-tuning.
Remove the last layer (previously the last

576
01:03:20,050 --> 01:03:26,790
layer was going to predict 1000 categories)
and replace it with a new last layer which

577
01:03:26,790 --> 01:03:29,120
we train.

578
01:03:29,120 --> 01:03:32,730
In this case, I've only run it through 2 epochs,
so I'm not getting a great result. But if

579
01:03:32,730 --> 01:03:37,880
we ran it for a few more we would get a bit
better than the 97.7 we had last time. When

580
01:03:37,880 --> 01:03:43,510
we look at State Farm, it's going to be critical
to do something like this.

581
01:03:43,510 --> 01:03:50,281
So how many layers would remove? You don't
just have to remove 1. If you go back to your

582
01:03:50,281 --> 01:04:07,640
Lesson 2 notebook, you'll see after this I've
got a section called Retraining More Layers,

583
01:04:07,640 --> 01:04:13,910
and in it we can see that we can take any
model and we can say let's grab all layers

584
01:04:13,910 --> 01:04:24,280
up to the n-th layer (in this case we said
all the layers after the first fully connected

585
01:04:24,280 --> 01:04:31,160
layer) and set them all to trainable. What
would happen if we tried running that model?

586
01:04:31,160 --> 01:04:38,230
So with Keras, we can tell Keras which layers
do we want to freeze (and leave them at their

587
01:04:38,230 --> 01:04:45,720
ImageNet decided weights) and which layers
do we want to retrain based on the things

588
01:04:45,720 --> 01:04:47,040
we're interested in.

589
01:04:47,040 --> 01:04:53,490
In general, the more different your problem
is from the 1000 ImageNet categories, the

590
01:04:53,490 --> 01:04:58,030
more layers you're going to have to retrain.

591
01:04:58,030 --> 01:05:08,750
[Time: 1.05 hour mark]

592
01:05:08,750 --> 01:05:14,600
Question: How do you decide how far to go
back in the layrs?

593
01:05:14,600 --> 01:05:16,220
Answer: Two ways.

594
01:05:16,220 --> 01:05:23,130
Way #1 = intuition. Have a look at something
like those Matt Zeigler visualizations to

595
01:05:23,130 --> 01:05:28,400
get a sense of at what semantic level is each
of those layers operating at, and go back

596
01:05:28,400 --> 01:05:35,190
to the point where you feel like that level
of meaning is going to be relevant to your

597
01:05:35,190 --> 01:05:36,460
model.

598
01:05:36,460 --> 01:05:44,060
Method #2 = experiment. It doesn't take that
long to train another model starting at a

599
01:05:44,060 --> 01:05:53,000
different point. So I generally do a bit of
both. When I know that Dogs and Cats are subsets

600
01:05:53,000 --> 01:05:59,770
of the ImageNet categories, I'm not going
to bother generally training more than 1 replacement

601
01:05:59,770 --> 01:06:01,090
layer.

602
01:06:01,090 --> 01:06:07,180
For State Farm, I really had no idea. I was
pretty sure that I wouldn't have to retrain

603
01:06:07,180 --> 01:06:12,680
any of the convolutional layers because the
convolutional layers are all about spatial

604
01:06:12,680 --> 01:06:17,300
relationships and therefore a convolutional
layer is all about recognizing how things

605
01:06:17,300 --> 01:06:22,242
in space relate to each other. I was pretty
confident that figuring out that figureing

606
01:06:22,242 --> 01:06:27,140
out whether somebody is looking at a mobile
phone or playing with their phone is not going

607
01:06:27,140 --> 01:06:34,030
to use different spatial features. So for
State Farm I only looked at retraining the

608
01:06:34,030 --> 01:06:35,510
dense layers.

609
01:06:35,510 --> 01:06:50,940
In VGG, there are actually only 3 dense layers
and 2 intermediate layers and the output layer,

610
01:06:50,940 --> 01:06:57,720
so I just trained all 3. Generally speaking,
the answer to this is try a few things and

611
01:06:57,720 --> 01:06:59,020
see what works.

612
01:06:59,020 --> 01:07:04,765
Question: Are you setting the weights randomly
when you retrain the layers?

613
01:07:04,765 --> 01:07:09,590
Answer: When we retrain the layers we do not
set the weights randomly. We start the weights

614
01:07:09,590 --> 01:07:18,630
at their optimal ImageNet levels and so that
means that if you retrain more layers than

615
01:07:18,630 --> 01:07:22,599
you really need to, it's not a big problem
because the weights are already at the right

616
01:07:22,599 --> 01:07:24,670
point.

617
01:07:24,670 --> 01:07:31,180
If you randomized the weights of the layers
that you're retraining, that would actually

618
01:07:31,180 --> 01:07:39,090
kill the earlier layers as well if you made
them trainable. There's no point setting them

619
01:07:39,090 --> 01:07:45,410
to random most of the time, we'll be learning
a bit more about that after the break.

620
01:07:45,410 --> 01:07:53,290
So far, we have not reset the weights. When
we say layer.trainable=True we're just telling

621
01:07:53,290 --> 01:07:59,210
Keras that when you say fit, I actually want
you to use SGD to update the weights on that

622
01:07:59,210 --> 01:08:05,400
layer.

623
01:08:05,400 --> 01:08:11,941
So that's been a lot of review but I think
it's very useful. When we come back, we're

624
01:08:11,941 --> 01:08:20,670
going to be talking about how to go beyond
these basic 5 pieces to ceate models which

625
01:08:20,670 --> 01:08:25,899
are more accurate. Specifically we are going
to look at avoiding underfitting and avoiding

626
01:08:25,899 --> 01:08:28,499
overfitting.

627
01:08:28,500 --> 01:08:35,109
Next week, we're going to be doing half a
class on review of covolutional neural networks

628
01:08:35,109 --> 01:08:39,479
and half a class an introduction to recurrent
neural networks, which we'll be using for

629
01:08:39,479 --> 01:08:40,709
language.

630
01:08:40,710 --> 01:08:48,589
So hopefully by the end of this class you'll
be feeling ready to really dig deep into CNNs

631
01:08:48,589 --> 01:08:53,170
during the week. And this is really the right
time this week to make sure you're asking

632
01:08:53,170 --> 01:09:07,029
questions you have about CNNs, because next
week we'll be wrapping up this topic.

633
01:09:07,029 --> 01:09:13,130
So we have a lot to cover in our next 55 minutes.
I think actually this approach of doing the

634
01:09:13,130 --> 01:09:19,340
new material quickly and then you can review
it in the lesson notebook, on the video, and

635
01:09:19,340 --> 01:09:24,238
by experimenting during the week, and then
reviewing the next week is fine. I think that's

636
01:09:24,238 --> 01:09:25,238
a good approach.

637
01:09:25,238 --> 01:09:29,689
I just wanted to make you aware that the new
material over the next 55 minutes wil move

638
01:09:29,689 --> 01:09:36,879
pretty quickly. Don't worry too much if not
everything sinks in straight-away. If you

639
01:09:36,880 --> 01:09:43,890
have any questions, please do ask. But also
recognize that it's really going to sink in

640
01:09:43,890 --> 01:09:46,630
as you study it, play with it during the week.

641
01:09:46,630 --> 01:09:51,120
Then next week we're going to review all of
this. So if it's still not making sense, then

642
01:09:51,120 --> 01:09:55,550
of course ask your questions on the forum.
If it's still not making sense, we'll be reviewing

643
01:09:55,550 --> 01:09:58,810
it next week.

644
01:09:58,810 --> 01:10:10,600
[Time: 1.10 hour mark]

645
01:10:10,600 --> 01:10:20,290
Question: If you don't retrain a layer, does
that mean the layer "remembers"? Is that what

646
01:10:20,290 --> 01:10:23,900
gets saved?
Answer: Yes, if you don't retrain a layer

647
01:10:23,900 --> 01:10:29,290
when you save the weights, it's going to contain
the weights that it originally had.

648
01:10:29,290 --> 01:10:33,630
Question: Why would we want to start out by
overfitting?

649
01:10:33,630 --> 01:10:36,150
Answer: We're going to talk about that next.

650
01:10:36,150 --> 01:10:40,940
Question: Am I correct in understandingt that
the last conv layer of VGG is a 7x7x output,

651
01:10:40,940 --> 01:10:46,230
so 49 "boxes" over the original image, and
each one has 512 different things it could

652
01:10:46,230 --> 01:10:48,860
recognize in each box?
Answer: That's kind of right. But it's not

653
01:10:48,860 --> 01:10:54,190
like it recognizes 512 different things. When
you have a convolution on a convolution on

654
01:10:54,190 --> 01:11:01,590
a convolution, you have a very rich function
with hundreds of thousands of parameters.

655
01:11:01,590 --> 01:11:10,980
So it's not like it's recognizing 512 things,
it's that there are 512 rich complex functions.

656
01:11:10,980 --> 01:11:19,690
So those rich complex functions can recognize
rich complex concepts. So for example, we

657
01:11:19,690 --> 01:11:27,000
saw in a video that even in Layer 6 there's
a face detector which can recognize cat faces

658
01:11:27,000 --> 01:11:29,210
as well as human faces.

659
01:11:29,210 --> 01:11:36,190
So the later on we get in these neural networks,
the harder it is to even say what's being

660
01:11:36,190 --> 01:11:46,320
found, because they get more and more sophisticated
and complex. So what those 512 things do in

661
01:11:46,320 --> 01:11:52,150
the last layer of VGG, I'm not sure that anybody's
gotten to the point that they can tell you

662
01:11:52,150 --> 01:11:53,150
that.

663
01:11:53,150 --> 01:12:00,790
Comment: Deeper layers are getting to see
more of the original image.

664
01:12:00,790 --> 01:12:07,750
So a deeper layer, because it's a 7x7 output
is seeing 1/7th of the X and 1/7th of the

665
01:12:07,750 --> 01:12:16,520
Y, so it's going to see more and more.

666
01:12:16,520 --> 01:12:23,820
The next section is all about making our model
better. At this point, we have a model with

667
01:12:23,820 --> 01:12:33,460
an accuracy of 97.7%. How do we make it better?

668
01:12:33,460 --> 01:12:46,750
Because we have started with an existing VGG
model, there are 2 reasons that you could

669
01:12:46,750 --> 01:12:54,230
be less good than you want to be. Either you're
underfitting or you're overfitting.

670
01:12:54,230 --> 01:13:00,680
Underfitting means that, for example, you're
using a linear model to try to do imagery.

671
01:13:00,680 --> 01:13:07,170
You're using a model that is not complex and
powerful enough for the thing you're doing.

672
01:13:07,170 --> 01:13:11,000
Or it doesn't have enough parameters for the
thing you're doing. That's what underfitting

673
01:13:11,000 --> 01:13:12,400
is.

674
01:13:12,400 --> 01:13:18,590
Overfitting means that you're using a model
with too many parameters that you've trained

675
01:13:18,590 --> 01:13:24,980
for too long without correctly using the techniques
that we're about to learn about such that

676
01:13:24,980 --> 01:13:32,330
you end up learning what your specific training
pictures look like rather than what the general

677
01:13:32,330 --> 01:13:42,230
patterns in them look like. You'll recognize
overfitting if your training set has a much

678
01:13:42,230 --> 01:13:49,540
higher accuracy than your test set and validation
set. So that means you've learnt how to recognize

679
01:13:49,540 --> 01:13:55,530
the contents of your training set too well.
And so then when you look at your validation

680
01:13:55,530 --> 01:14:00,160
set you get a less good result. So that's
overfitting.

681
01:14:00,160 --> 01:14:03,580
I'm not going to go into detail on this because
any of you who have done any machine learning

682
01:14:03,580 --> 01:14:10,930
have seen this before. So any of you who haven't,
please look up overfitting on the Internet,

683
01:14:10,930 --> 01:14:15,800
learn about it, ask questions about it. This
is perhaps the single most important concept

684
01:14:15,800 --> 01:14:20,610
in machine learning. So it's not that we're
not covering it because it's not interesting,

685
01:14:20,610 --> 01:14:26,460
we're not covering it because I know a lot
of you are already familiar with it.

686
01:14:26,460 --> 01:14:34,910
Underfitting we can see in the same way, but
opposite, if our training error is much lower

687
01:14:34,910 --> 01:14:43,600
than our validation error, then we're underfitting.
So I'm going to look at this now because you

688
01:14:43,600 --> 01:14:51,050
might have noticed that in all of our models
so far, our trainnig error has been lower

689
01:14:51,050 --> 01:14:56,380
than our validation error, which means we
have been underfitting.

690
01:14:56,380 --> 01:14:58,050
So how is this possible?

691
01:14:58,050 --> 01:14:59,530
[Time: 1.15 minute mark]

692
01:14:59,530 --> 01:15:04,140
And the answer to how this is possible is
because the VGG network includes something

693
01:15:04,140 --> 01:15:10,300
called Dropout. Specifically Dropout with
a p of .5.

694
01:15:10,300 --> 01:15:17,370
What does Dropout mean? It means that at this
layer (which happens at the end of every fully-connected

695
01:15:17,370 --> 01:15:29,480
block), it deletes 0.5 (50%) of the activations
at random. It sets them to 0. That's what

696
01:15:29,480 --> 01:15:36,710
a dropout layer does. It sets to 0 half (.5)
of the activations at random.

697
01:15:36,710 --> 01:15:43,150
Why would it do that? Because when you randomly
throw away bits of the network, it means that

698
01:15:43,150 --> 01:15:48,900
the network can't learn to overfit. It can't
learn to build a network that just learns

699
01:15:48,900 --> 01:15:53,701
about your images, because as soon as it does,
you throw away half of it. And suddenly, it's

700
01:15:53,701 --> 01:15:55,420
not working anymore.

701
01:15:55,420 --> 01:16:02,570
So dropout is a fairly recent development
(about 3 years old?) and it's perhaps the

702
01:16:02,570 --> 01:16:07,460
most important development over the last few
years because it's the thing that now means

703
01:16:07,460 --> 01:16:17,940
we can train big complex models for long periods
of time without overfitting. Incredibly important.

704
01:16:17,940 --> 01:16:24,261
But in this case, it seems that we are using
too much dropout. So the VGG network, which

705
01:16:24,261 --> 01:16:29,990
used a dropout of .5, they decided they needed
that much in order to avoid overfitting ImageNet.

706
01:16:29,990 --> 01:16:38,020
But it seems for our Cats and Dogs, it seems
that it's underfitting. So what do we do?

707
01:16:38,020 --> 01:16:45,530
The answer is, Let's try removing dropout.
So how do we removed dropout? This is where

708
01:16:45,530 --> 01:16:47,000
it gets fun.

709
01:16:47,000 --> 01:16:52,770
We can start with our VGG fine-tuned model.
I've actually created a little function called

710
01:16:52,770 --> 01:17:02,850
vgg_ft, which creates a VGG fine-tuned model
with 2 outputs. It looks exactly like we would

711
01:17:02,850 --> 01:17:15,500
expect it to look. It creates a VGG model,
it fine-tunes it, it returns it.

712
01:17:15,500 --> 01:17:25,900
What does ft (fine-tune) do? It does exactly
what we've learnt. It pops off the last layer,

713
01:17:25,900 --> 01:17:29,489
sets all the rest of the layers to nontrainable
(layer.trainable=False) and adds a new Dense

714
01:17:29,489 --> 01:17:34,230
layer. I just created a little thing that
does that.

715
01:17:34,230 --> 01:17:38,980
Every time I start writing the same code more
than once, I stick it into a function so I

716
01:17:38,980 --> 01:17:42,360
can use it again in the future. It's good
practice.

717
01:17:42,360 --> 01:17:48,500
I then load the weights that I just saved
in my last model, so I don't have to retrain

718
01:17:48,500 --> 01:17:54,190
it. So saving and loading weights is a really
helpful way of avoiding not refitting things.

719
01:17:54,190 --> 01:18:05,530
So already I now have a model that fits Cats
and Dogs with 97.7% accuracy.

720
01:18:05,530 --> 01:18:12,292
We can grab all the layers of the model and
we can ennumerate through them and we can

721
01:18:12,292 --> 01:18:30,219
find the last one 

722
01:18:30,219 --> 01:18:40,830
that is a convolution. So at this point we
now have the index of the last convolutional

723
01:18:40,830 --> 01:18:47,020
layer (30). So we can now grab that last convolutional
layer.

724
01:18:47,020 --> 01:19:03,940
And so what we want to try doing is removing
drop-out from all the rest of the layers.

725
01:19:03,940 --> 01:19:19,180
So after the convolutional layers are the
Dense layers. This is really kind of important

726
01:19:19,180 --> 01:19:25,770
concept in the Keras library, playing around
with layers.

727
01:19:25,770 --> 01:19:30,910
So spend some time looking at this code, really
look at the inputs and the outputs and get

728
01:19:30,910 --> 01:19:38,440
a sense of it. So you can see here, here are
all the layers up to the last convolutional

729
01:19:38,440 --> 01:19:44,500
layer. Here are all the layers from the last
convolutional layer. So all the fully connected

730
01:19:44,500 --> 01:19:46,160
layers and all the convolutional layers.

731
01:19:46,160 --> 01:19:53,630
I can create a whole new model that contains
just the convolutional layers. Why would I

732
01:19:53,630 --> 01:20:01,810
do that? Because if I'm going to remove dropout,
then clearly I'm going to want to fine-tune

733
01:20:01,810 --> 01:20:05,980
all of the layers that involve dropout, that
is all of the dense layers.

734
01:20:05,980 --> 01:20:06,980
[Time: 1.20 hour mark]

735
01:20:06,980 --> 01:20:11,219
I don't need to fine-tune any of the convolutional
layers because none of the covolutional layers

736
01:20:11,219 --> 01:20:19,180
have dropout. Look at the ConvBlock. The ConvBlock
does not have dropout.

737
01:20:19,180 --> 01:20:25,420
So I'm going to save myself some time. I'm
going to pre-calculate the output of the last

738
01:20:25,420 --> 01:20:32,280
convolutional layer. You see this model I
have here, this model that contains all the

739
01:20:32,280 --> 01:20:40,080
convolutional layers, if I pre-calculate the
output of that, then that's the input to the

740
01:20:40,080 --> 01:20:43,080
Dense layers that I want to train.

741
01:20:43,080 --> 01:20:52,710
So you can see what I do here is a say conv_model.predict
with my validation batches, conv_model.predict

742
01:20:52,710 --> 01:20:59,830
with my batches and that now gives me the
output of the convolutional layer for my training

743
01:20:59,830 --> 01:21:04,750
and the output of it for my validation. And
because that's something I don't want to do

744
01:21:04,750 --> 01:21:08,660
over and over again, I save it.

745
01:21:08,660 --> 01:21:18,800
So here I just go load_array and that's going
to load from the disk the output of that.

746
01:21:18,800 --> 01:21:23,410
And I'm going to say trn_features.shape (this
is always the first thing you want to do when

747
01:21:23,410 --> 01:21:31,370
you build something) is look at its shape.
Indeed it is what we would expect, 23,000

748
01:21:31,370 --> 01:21:38,570
images, each one is 14x14 (because I didn't
include the final MaxPooling layer) with 512

749
01:21:38,570 --> 01:21:49,990
filters. Indeed if we go model.summary, we
should find the last convolutional layer,

750
01:21:49,990 --> 01:21:57,570
512 filters, 14x14 dimension.

751
01:21:57,570 --> 01:22:02,690
So we have basically built a model that is
just a subset of VGG containing all of these

752
01:22:02,690 --> 01:22:09,130
earlier layers. We've run it through our test
sets and our validation sets and we've got

753
01:22:09,130 --> 01:22:15,840
the outputs. So that's the stuff that we want
to fix, and we don't want to recalculate that

754
01:22:15,840 --> 01:22:23,910
every time. So now we create a new model that
is exactly the same as the dense part of VGG

755
01:22:23,910 --> 01:22:29,980
but we replace the Dropout "p" with 0.

756
01:22:29,980 --> 01:22:33,210
So here's something pretty interesting, and
I'm going to let you think about this during

757
01:22:33,210 --> 01:22:43,190
the week. How do you take the previous weights
from VGG and put them into this model where

758
01:22:43,190 --> 01:22:49,770
Dropout is 0? If you think about it, before
we had Dropout at .5, so half the activations

759
01:22:49,770 --> 01:22:54,470
were being deleted at random. So since half
the activations were being deleted at random,

760
01:22:54,470 --> 01:23:00,900
now that I've removed Dropout, I effectively
have twice as many weights active. Since I

761
01:23:00,900 --> 01:23:05,160
have twice as many weights being active, I
need to take my ImageNet weights and divide

762
01:23:05,160 --> 01:23:08,550
them by 2.

763
01:23:08,550 --> 01:23:15,949
By taking my ImageNet weights and copying
them across to my new model but each time

764
01:23:15,949 --> 01:23:21,640
divide them by 2, that means my new model
is going to be exactly as accurate as my old

765
01:23:21,640 --> 01:23:29,449
model before I start training, but it has
no Dropout.

766
01:23:29,449 --> 01:23:44,650
Question: Is it 
wasteful to have in the Cats and Dogs model

767
01:23:44,650 --> 01:23:47,660
filters that have been learnt to find things
like bookshelves?

768
01:23:47,660 --> 01:23:54,880
Answer: Potentially it is, but it's okay to
be wasteful. The only place that it's a problem

769
01:23:54,880 --> 01:24:03,480
is if we're overfitting. And if we're overfitting,
we can easily fix that by adding more Dropout.

770
01:24:03,480 --> 01:24:10,070
So let's try this. We now have a model which
takes the output of the convolutional layers

771
01:24:10,070 --> 01:24:18,250
as input, gives us our Cats vs Dogs as output
and has no Dropout. So now we could just go

772
01:24:18,250 --> 01:24:28,840
ahead and fit it. Notice that the input to
this is my 512 by 14 by 14, my outputs are

773
01:24:28,840 --> 01:24:35,270
my Cats and Dogs (as usual). Train it for
a few epochs, and here's something really

774
01:24:35,270 --> 01:24:42,440
interesting, dense layers take very little
time to compute. A convolutional layer takes

775
01:24:42,440 --> 01:24:53,500
a long time. If you think about it, you're
computing 512 3x3x512 filters for each of

776
01:24:53,500 --> 01:24:57,610
14x14 spots. That is a lot of computation.

777
01:24:57,610 --> 01:24:58,770
[Time: 1.25 hour mark]

778
01:24:58,770 --> 01:25:04,130
So in a deep-learning network, your convolutional
layers is where all of your computation is

779
01:25:04,130 --> 01:25:10,630
being taken up. Look, when I train my dense
layers, it is only taking 17 seconds. Super

780
01:25:10,630 --> 01:25:11,630
fast.

781
01:25:11,630 --> 01:25:15,800
On the other hand, the dense layers is where
all of your memory is taken up. Between this

782
01:25:15,800 --> 01:25:23,980
4096 layer and this 4096 layer, there are
4000x4000=16,000,000 weights. And between

783
01:25:23,980 --> 01:25:37,400
the previous layer, which was (after MaxPooling)
512x7x7=25088, there's 25088 x 4096 weights.

784
01:25:37,400 --> 01:25:40,960
So this is a really important rule-of-thumb:
Your dense layers is where your memory is

785
01:25:40,960 --> 01:25:46,239
taken up, and your convolutional layers is
where your computation time is taken up.

786
01:25:46,239 --> 01:25:56,450
So it took me a minute or so to run 8 epochs.
That's really fast. Holy shit, look at that,

787
01:25:56,450 --> 01:25:57,450
98.5%.

788
01:25:57,450 --> 01:26:05,430
So you can see now I am overfitting. But even
though I'm overfitting, I'm doing pretty damn

789
01:26:05,430 --> 01:26:14,940
well. Overfitting is only bad if you're doing
it so much that your accuracy is bad. So in

790
01:26:14,940 --> 01:26:19,560
this case it looks like actually this amount
of overfitting is pretty good.

791
01:26:19,560 --> 01:26:25,530
For Cats and Dogs, this is about as good as
I could have gotten. If I had stopped a little

792
01:26:25,530 --> 01:26:32,890
earlier, you can see that it was really good
(98.7%). The winner was 98.8, and here I've

793
01:26:32,890 --> 01:26:38,720
got 98.75. And there's some tricks I'll show
you later that always give you an extra 50%

794
01:26:38,720 --> 01:26:46,620
accuracy, so this would definitely have won
Cats and Dogs, if we had used this model.

795
01:26:46,620 --> 01:26:50,710
Question: Can you perform Dropout on a convolutional
layer?

796
01:26:50,710 --> 01:26:52,570
Answer: You can definitely perform Dropout
on a convolutional layer. Indeed nowadays,

797
01:26:52,570 --> 01:26:53,840
people normally do.

798
01:26:53,840 --> 01:26:58,400
I don't quite remember the VGG days, I guess
that was 2 years ago, maybe people in those

799
01:26:58,400 --> 01:26:59,980
days didn't.

800
01:26:59,980 --> 01:27:06,060
Nowadays the general approach would be a Dropout
of .1 for your first layer, Dropout of .2

801
01:27:06,060 --> 01:27:11,040
before this one, .3, .4, and then finally
a Dropout of .5 before your fully-connected

802
01:27:11,040 --> 01:27:13,800
layers. That's kind of the standard.

803
01:27:13,800 --> 01:27:17,460
If you then find that you're find that you're
underfitting or overfitting you can modify

804
01:27:17,460 --> 01:27:24,520
all of those probabilities by the same amount.
But generally speaking, if you drop out in

805
01:27:24,520 --> 01:27:30,280
an early layer, you're losing that information
for all of the future layers. So you don't

806
01:27:30,280 --> 01:27:34,630
want to drop out too much in the early layers.
You can feel better dropping out more in the

807
01:27:34,630 --> 01:27:35,710
later layers.

808
01:27:35,710 --> 01:27:44,430
Question: How do you know how to tune dropout?
How do you determine if you're overfitting

809
01:27:44,430 --> 01:27:45,510
or underfitting?

810
01:27:45,510 --> 01:27:53,570
Answer: Tuning your dropout is very much black
magic.

811
01:27:53,570 --> 01:27:57,070
This is how you manually determine whether
you're overfit or underfit. Another way to

812
01:27:57,070 --> 01:28:03,341
do it would be to modify your architecture
to have less or more fitlers. But that's actually

813
01:28:03,341 --> 01:28:04,800
pretty difficult to do.

814
01:28:04,800 --> 01:28:07,820
Question: So is the point that we didn't need
Dropout anyway?

815
01:28:07,820 --> 01:28:15,449
Answer: Perhaps it was. But VGG comes with
Dropout. So when you're fine-tuning you start

816
01:28:15,449 --> 01:28:19,990
with what you start with.

817
01:28:19,990 --> 01:28:26,340
We are overfitting here. So my hypothesis
is maybe we should try a little less Dropout.

818
01:28:26,340 --> 01:28:29,640
But before we do, I'm going to show you some
better tricks.

819
01:28:29,640 --> 01:28:34,680
And the first trick I'm going to show you
is a trick that lets you avoid overfitting

820
01:28:34,680 --> 01:28:38,949
without deleting information. Dropout deletes
information, so we don't want to do it unless

821
01:28:38,949 --> 01:28:40,699
we have to.

822
01:28:40,699 --> 01:28:48,420
Here is a list, you guys should refer to it
every time you're building a model that is

823
01:28:48,420 --> 01:28:49,420
overfitting.

824
01:28:49,420 --> 01:28:50,420
Five steps:

825
01:28:50,420 --> 01:28:57,780
Step 1: Add more data. This is a Kaggle competition,
so we can't do that.

826
01:28:57,780 --> 01:29:01,810
Step 2: Use data augmentation, which we're
about to learn.

827
01:29:01,810 --> 01:29:07,000
Step 3: Use more generalizable architectures.
We're going to learn that after this.

828
01:29:07,000 --> 01:29:13,590
Step 4: Add regularization. That generally
means Dropout. There's another type of regularization

829
01:29:13,590 --> 01:29:23,219
which is where you basically add up the value
of all of your weights and then multiply it

830
01:29:23,219 --> 01:29:28,949
by some small number and you add that to the
loss function. Basically you say having higher

831
01:29:28,949 --> 01:29:34,840
weights is bad. And that's called either L2
Regularization (if you take the square of

832
01:29:34,840 --> 01:29:39,260
your weights and add them up. Or L1 Regularization,
if you take the absolute value of your weights

833
01:29:39,260 --> 01:29:40,570
and add them up.

834
01:29:40,570 --> 01:29:49,760
Keras supports that as well, also popular.
I don't think anyone has a great sense of

835
01:29:49,760 --> 01:29:56,070
when do you use L1 and L2 Regularization and
when do you use Dropout. I use Dropout pretty

836
01:29:56,070 --> 01:30:00,469
much all the time and I don't particularly
see why anyone would need both. I just wanted

837
01:30:00,469 --> 01:30:04,210
to let you know that that other type of Regularization
exists.

838
01:30:04,210 --> 01:30:05,539
[Time: 1.30 hour mark]

839
01:30:05,539 --> 01:30:09,950
And then lastly, if you really have to reduce
architecture complexity (Step 5), remove some

840
01:30:09,950 --> 01:30:15,010
filters. But that's pretty hard to do if you're
fine-tuning. Because how do you know which

841
01:30:15,010 --> 01:30:19,140
filters to remove?

842
01:30:19,140 --> 01:30:24,190
Now that we have Dropout, the first 4 are
what we do in practice.

843
01:30:24,190 --> 01:30:35,240
Question: Is this like in Random Forest when
we randomly select some features?

844
01:30:35,240 --> 01:30:41,200
Answer: Like in Random Forest, when we randomly
select subsets of variables at each point,

845
01:30:41,200 --> 01:30:46,170
that's kind of what Dropout is doing. Dropout
is randomly throwing away half the activations.

846
01:30:46,170 --> 01:30:56,620
So Dropout and Random Forest both effectively
create large ensembles. So it's actually a

847
01:30:56,620 --> 01:30:59,940
fantastic kind of analogy.

848
01:30:59,940 --> 01:31:05,760
Just like when we went from decision trees
to random forests, it's this huge step which

849
01:31:05,760 --> 01:31:11,010
is basically create lots of decision trees
with some random differences. Dropout is automatically

850
01:31:11,010 --> 01:31:16,770
creating lots of neural networks with different
subsets of features that have been randomly

851
01:31:16,770 --> 01:31:19,900
selected.

852
01:31:19,900 --> 01:31:26,880
Data augmentation is very, very simple. Data
augmentation is something which takes a cat

853
01:31:26,880 --> 01:31:36,780
and turns it into lots of cats. That's it.
Actually it does it for dogs as well. You

854
01:31:36,780 --> 01:31:43,620
can rotate, you can flip, you can move up
and down, left and right, zoom in and out.

855
01:31:43,620 --> 01:31:52,420
In Keras, you do it by adding options to ImageDataGenerator.
What we always said before was image.ImageDataGenerator().

856
01:31:52,420 --> 01:31:55,810
Now we say all these other things, like image.ImageDataGenerator(rotation_range=20,
width_shift_range=0.1, height_sifht_range=0.1,

857
01:31:55,810 --> 01:31:56,970
shear_range=0.05, zoom_range=0.1, horizontal_flip=True,
dim_ordering='tf'). Flip it horizontally at

858
01:31:56,970 --> 01:32:01,330
random, zoom in a bit at random, shear at
random, rotate at random, move it left and

859
01:32:01,330 --> 01:32:05,000
right at random and move it up and down at
random.

860
01:32:05,000 --> 01:32:13,520
Once you've done that, when you create your
batches, rather than doing it the way we did

861
01:32:13,520 --> 01:32:24,590
it before, you simply add that to your batches.
So we said okay this is our data generator,

862
01:32:24,590 --> 01:32:29,390
so when we create our batches, use that data
generator, the augmenting data generator.

863
01:32:29,390 --> 01:32:36,320
It is very important to notice the validation
set does not include that. Because the validation

864
01:32:36,320 --> 01:32:39,390
set is the validation set, that's the thing
we want to check against, so we shouldn't

865
01:32:39,390 --> 01:32:40,570
be fiddling with that at all.

866
01:32:40,570 --> 01:32:47,300
So the validation set has no data augmentation
and no shuffling. It's constant and fixed.

867
01:32:47,300 --> 01:32:51,050
The training set on the other hand, we want
to move it around as much as we can. So shuffle

868
01:32:51,050 --> 01:32:55,699
it's order, and add all these different types
of augmentation.

869
01:32:55,699 --> 01:33:01,390
How much augmentation to use? This is one
of the things that Rachel and I would love

870
01:33:01,390 --> 01:33:02,390
to automate.

871
01:33:02,390 --> 01:33:07,340
For now, two methods. Use your intition, and
the best way to use your intuition is to take

872
01:33:07,340 --> 01:33:15,030
one of your images, add some augmentation
and check whether they still look like cats.

873
01:33:15,030 --> 01:33:21,260
So if it's like so warped that nobody takes
a photo of a cat like that, you've done it

874
01:33:21,260 --> 01:33:27,300
wrong. This is kind of like a small amount
of data augmentation. Method two, experiment.

875
01:33:27,300 --> 01:33:32,550
Try a range of different augmentations and
see which one gives you the best results.

876
01:33:32,550 --> 01:33:39,010
So here, if we add some augmentation, everything
else is exactly the same, except we can't

877
01:33:39,010 --> 01:33:44,650
precompute anything anymore. So earlier on
we precomputed the output of the last convolutional

878
01:33:44,650 --> 01:33:50,630
layer. We can't do that now because every
time this cat approaches our neural network,

879
01:33:50,630 --> 01:33:54,930
it's a little bit different. It's rotated
a bit, it's flipped, it's moved around, it's

880
01:33:54,930 --> 01:33:56,250
zoomed in and out.

881
01:33:56,250 --> 01:34:01,970
So unfortunately when we use data augmentation,
we can't precompute anything and so things

882
01:34:01,970 --> 01:34:05,340
take longer. Everything else is the same though.

883
01:34:05,340 --> 01:34:10,290
We grabbed our fuly-connected model, we add
it to the end of our convolutional model (and

884
01:34:10,290 --> 01:34:16,810
this is the one without dropout), compile
it, fit it, and now run. Rather than taking

885
01:34:16,810 --> 01:34:22,090
9 seconds per epoch, it takes 273 seconds
per epoch because it has to calculate through

886
01:34:22,090 --> 01:34:29,730
all the convolutional layers, because of the
data augmentation.

887
01:34:29,730 --> 01:34:44,460
So in terms of results here, we have not managed
to get back up to that 98.7 accuracy. If I

888
01:34:44,460 --> 01:34:53,110
keep running them, again it's not over-fitting.
It's a little hard to tell because the validation

889
01:34:53,110 --> 01:34:58,740
accuracy is moving around quite a lot (because
my validation set is a little on the small

890
01:34:58,740 --> 01:35:03,440
side), it's a little hard to tell whether
this data augmentation is helping or hindering.

891
01:35:03,440 --> 01:35:05,230
[Time: 1.35 hour mark]

892
01:35:05,230 --> 01:35:09,110
I suspect that what we're finding here is
that maybe we're doing a little bit too much

893
01:35:09,110 --> 01:35:21,020
data augmentation, so if I went back and reduced
my different ranges by half, I might get a

894
01:35:21,020 --> 01:35:26,030
better result than this. But really this is
something to experiment with and I have better

895
01:35:26,030 --> 01:35:30,400
things to do than experiment with this, but
you get the idea.

896
01:35:30,400 --> 01:35:37,050
So data augmentation is something you should
always do. Like there's never a reason not

897
01:35:37,050 --> 01:35:41,290
to use data augmentation. The question is
just what kind and how much.

898
01:35:41,290 --> 01:35:49,380
So for example, what kind. Should you flip
X and Y? Clearly for Dogs and Cats, no. You

899
01:35:49,380 --> 01:35:56,110
pretty much never see a picture of an upside-down
dog. So would you do vertical flipping in

900
01:35:56,110 --> 01:35:58,420
this particular problem? No, you wouldn't.

901
01:35:58,420 --> 01:36:03,570
Would you do rotations? Yes, you very often
see cats and dogs that are kind of on their

902
01:36:03,570 --> 01:36:06,900
hind legs, or the photo is taken a bit uneven
or whatever.

903
01:36:06,900 --> 01:36:09,660
You certainly would have zooming. Sometimes
you're close to the dog, sometimes you're

904
01:36:09,660 --> 01:36:10,660
further away.

905
01:36:10,660 --> 01:36:15,740
So use your intuition to think about what
kind of augmentation to use.

906
01:36:15,740 --> 01:36:19,840
Question: Could you use data augmentation
with color?

907
01:36:19,840 --> 01:36:23,440
Answer: Data augmentation with color, that's
an excellent point. Something I didn't add

908
01:36:23,440 --> 01:36:31,320
to this but probably should have is that there's
a channel augmentation parameter for data

909
01:36:31,320 --> 01:36:36,150
generating in Keras and that will slightly
change the colors. And indeed, that's a great

910
01:36:36,150 --> 01:36:40,720
idea for natural images like these because
you have different white balance, you have

911
01:36:40,720 --> 01:36:45,449
different lighting and so forth. I think that
would be a great idea.

912
01:36:45,449 --> 01:36:50,870
So I hope during the week that people will
take this notebook and somebody will tell

913
01:36:50,870 --> 01:36:58,390
me what is the best result they've got. I
bet that that data augmentation will include

914
01:36:58,390 --> 01:37:03,230
some fiddling around with colors.

915
01:37:03,230 --> 01:37:16,680
Question: Would 
changing it to black and white help?

916
01:37:16,680 --> 01:37:22,280
Answer: No it wouldn't because the Kaggle
competition test set is in color. So if you're

917
01:37:22,280 --> 01:37:31,590
throwing away color, you're throwing away
information. The Kaggle competition is saying,

918
01:37:31,590 --> 01:37:35,870
Is this a Cat or is this a Dog?, and part
of seeing whether it's a cat or a dog is seeing

919
01:37:35,870 --> 01:37:39,860
what color it is. So if you're throwing away
the color, you're making that harder. So you

920
01:37:39,860 --> 01:37:44,400
could run it on the test set and get answers,
but they're going to be less accurate because

921
01:37:44,400 --> 01:37:46,730
you've thrown away information.

922
01:37:46,730 --> 01:37:55,520
Question: How is it working to remove the
flattened layer?

923
01:37:55,520 --> 01:38:04,810
Answer: What happened to the flattened layer
is that is was there. Oh, okay, gosh, I forgot

924
01:38:04,810 --> 01:38:13,010
to add it back to this one. So I actually
changed my mind about whether to include the

925
01:38:13,010 --> 01:38:16,680
flattened layer, and where to put it, where
to put MaxPooling. It'll come back later.

926
01:38:16,680 --> 01:38:20,050
This is a slightly old version. That you for
picking it up.

927
01:38:20,050 --> 01:38:27,050
Question: Can you perform DropOut on the raw
images by randomly blanking out pieces?

928
01:38:27,050 --> 01:38:32,650
Answer: Can you do Dropout on the raw images,
the simple answer is yes you could. There's

929
01:38:32,650 --> 01:38:38,360
no reason I can't put a DropOut layer right
here, and that's going to drop out pixels.

930
01:38:38,360 --> 01:38:44,881
It turns out that's not a good idea. Throwing
away input information is very different from

931
01:38:44,881 --> 01:38:46,820
throwing away model information.

932
01:38:46,820 --> 01:38:52,110
Throwing away model information is letting
you effectively avoid overfitting the model.

933
01:38:52,110 --> 01:39:07,790
You don't want to avoid overfitting the data,
so you probably don't 

934
01:39:07,790 --> 01:39:08,790
want to do that.

935
01:39:08,790 --> 01:39:14,300
To clarify, the augmentation is at random.
I just showed you 8 examples of the augmentation.

936
01:39:14,300 --> 01:39:20,830
So what the augmentation does is it says at
random, rotate by up to 20 degrees, move up

937
01:39:20,830 --> 01:39:27,540
to 10% in each direction, shear by up to 5%,
zoom by 10% and flip at random half the time.

938
01:39:27,540 --> 01:39:33,199
So then I just said here are 8 cats. So what
happens is every time an image goes into the

939
01:39:33,199 --> 01:39:47,140
batch, it gets randomized. So effectively
it's an infinite number of augmented images.

940
01:39:47,140 --> 01:40:00,780
Okay, the final concept to learn about today
is Batch Normalization.

941
01:40:00,780 --> 01:40:02,370
[Time: 1.40 hour mark]

942
01:40:02,370 --> 01:40:07,610
Batch normalization, like data augmentation
is something you should always do. Why doesn't

943
01:40:07,610 --> 01:40:14,750
VGG do it? Because it didn't exist then. BatchNorm
is about 1 year old, maybe 18 months old.

944
01:40:14,750 --> 01:40:23,080
Here's the basic idea. Anybody who's kind
of done any machine learning probably knows

945
01:40:23,080 --> 01:40:29,120
that one of the first things you want to do
is take your input data, subtract it's mean

946
01:40:29,120 --> 01:40:31,390
and divide by the standard deviation.

947
01:40:31,390 --> 01:40:43,440
Why is that? Imagine we had like, 40 and -30
and 1. You can see that the outputs are all

948
01:40:43,440 --> 01:40:51,380
over the place. Some of the intermediate values
are really big, some are really small. So

949
01:40:51,380 --> 01:40:57,800
if we changed a weight which impacted x1,
it's going to change the loss function by

950
01:40:57,800 --> 01:41:03,080
a lot. Whereas if we change a weight which
impacts x3, it will change the loss function

951
01:41:03,080 --> 01:41:04,310
very little.

952
01:41:04,310 --> 01:41:10,570
So the different weights have very different
gradients basically, very different amounts

953
01:41:10,570 --> 01:41:13,489
that are going to affect the outcome.

954
01:41:13,489 --> 01:41:18,460
Furthermore, as you go down further down through
the model, that's going to multiply. And particularly

955
01:41:18,460 --> 01:41:22,750
when we're using SoftMax, which has an e to
the power in it, you end up with these crazy

956
01:41:22,750 --> 01:41:29,910
big numbers. So when you have inputs that
are of very different scales, it makes the

957
01:41:29,910 --> 01:41:36,191
whole model very fragile. Which means it is
harder to learn the best set of weights and

958
01:41:36,191 --> 01:41:38,510
you have to use smaller learning rates.

959
01:41:38,510 --> 01:41:43,780
This is not just true of deep-learning, it's
true of pretty much every kind of machine

960
01:41:43,780 --> 01:41:48,090
learning model, which is why everybody who's
been through the MSAN program here, hopefully

961
01:41:48,090 --> 01:41:51,199
you guys all learn to normalize your inputs.

962
01:41:51,199 --> 01:41:55,550
So if you haven't done any machine learning
before, no problem. Just take my word for

963
01:41:55,550 --> 01:42:00,260
it, you always want to normalize your inputs.

964
01:42:00,260 --> 01:42:04,969
It's so common that pretty much all of the
deep-learning libraries will normalize your

965
01:42:04,969 --> 01:42:15,910
inputs for you with a single parameter. Indeed,
we're doing it in ours because images like

966
01:42:15,910 --> 01:42:21,280
pixel values only range from 0 to 255, you
don't generally worry about dividing by the

967
01:42:21,280 --> 01:42:27,510
standard deviation with images, but you do
generally worry about subtracting the mean.

968
01:42:27,510 --> 01:42:34,280
So you'll see that the first thing our model
does is this thing called preprocess, which

969
01:42:34,280 --> 01:42:40,360
subtracts the mean. And the mean was something
basically you can look up on the Internet

970
01:42:40,360 --> 01:42:46,120
and find out what the mean of ImageNet data
is, these 3 fixed values.

971
01:42:46,120 --> 01:42:52,860
Now, what does that have to do with BatchNorm?
Imagine somewhere along the line in our training

972
01:42:52,860 --> 01:42:59,960
we ended up with like one really big weight.
Then suddenly, one of our layers is going

973
01:42:59,960 --> 01:43:04,040
to have one really big number. And now, we're
going to have exactly the same problem as

974
01:43:04,040 --> 01:43:10,550
we had before, which is the whole model becomes
very unresiliant, very fragile. It becomes

975
01:43:10,550 --> 01:43:17,739
very hard to train. It's going to be all over
the place. Some numbers could even get slightly

976
01:43:17,739 --> 01:43:23,910
out of control.

977
01:43:23,910 --> 01:43:35,130
So what do we do? Really what we want to do
is to normalize not just our inputs, but our

978
01:43:35,130 --> 01:43:40,320
activations as well. So you may think, okay,
no problem. Let's just subtract the mean and

979
01:43:40,320 --> 01:43:44,210
divide by the standard deviation by each of
our activation layers.

980
01:43:44,210 --> 01:43:50,070
Unfortunately that doesn't work. SGD is very
bloody minded. If it wants to increase one

981
01:43:50,070 --> 01:43:54,130
of the weights higher and you try to undo
it by subtracting the mean and dividing by

982
01:43:54,130 --> 01:43:58,740
the standard deviation, the next iteration
is going to try and make it higher again.

983
01:43:58,740 --> 01:44:04,720
So if SGD decides that it wants to make your
weights of very different scales, it will

984
01:44:04,720 --> 01:44:10,260
do so. Just normalizing the activation layers
doesn't work.

985
01:44:10,260 --> 01:44:16,630
So BatchNorm is a really neat trick for avoiding
that problem. Before I tell you the trick,

986
01:44:16,630 --> 01:44:18,929
I will just tell you why you want to use it.

987
01:44:18,929 --> 01:44:25,180
A) Because it is about 10X faster than not
using it, particularly because it often lets

988
01:44:25,180 --> 01:44:28,550
you use a 10X higher learning rate, and

989
01:44:28,550 --> 01:44:33,510
B) Because it reduces overfitting without
removing any information from the model.

990
01:44:33,510 --> 01:44:39,870
So these are two things you want, less overfitting
and faster models.

991
01:44:39,870 --> 01:44:42,700
I'm not going to go into detail about how
it works (you can read about it during the

992
01:44:42,700 --> 01:44:48,679
week if you're interested), but brief outline:
First step it normalizes the intermediate

993
01:44:48,679 --> 01:44:53,450
layers just the same way as input layers can
be normalized (the thing I just told you wouldn't

994
01:44:53,450 --> 01:44:59,000
work). It does it, but it does something else
critical, which is it adds 2 more trainable

995
01:44:59,000 --> 01:45:04,340
parameters. One trainable parameter multiplies
by all the activations and the other one is

996
01:45:04,340 --> 01:45:05,640
added to all the activations.

997
01:45:05,640 --> 01:45:06,640
[Time: 1.45 hour mark]

998
01:45:06,640 --> 01:45:14,190
So that is able to undo that normalization.
Both of those two things are then incorporated

999
01:45:14,190 --> 01:45:22,120
into the calculation of the gradient so the
model now knows that it can rescale all of

1000
01:45:22,120 --> 01:45:28,250
the weights if it wants to without moving
one of the weights way off into the distance.

1001
01:45:28,250 --> 01:45:33,520
And so it turns out that this does actually
effectively control the weights in a really

1002
01:45:33,520 --> 01:45:36,520
effective way. So that's what batch normalization
is.

1003
01:45:36,520 --> 01:45:44,500
The good news is for you to use it, you just
type batchnormalization(). You can put it

1004
01:45:44,500 --> 01:45:50,070
after Dense layers, you can put it after convolutional
layers, you should put it after all of your

1005
01:45:50,070 --> 01:45:52,239
layers.

1006
01:45:52,239 --> 01:46:00,239
Here's the bad news. VGG didn't train originally
with batchnormalization and adding batchnormalization

1007
01:46:00,239 --> 01:46:06,910
changes all of the weights. I think that there
is a way to calculate a new set of weights

1008
01:46:06,910 --> 01:46:12,050
with batchnormalization. I haven't gone through
that process yet.

1009
01:46:12,050 --> 01:46:20,940
So what I did today was I actually grabbed
the entirety of ImageNet and I trained this

1010
01:46:20,940 --> 01:46:31,330
model on all of ImageNet. And that then gave
me a model which was basically VGG plus batchnormalization,

1011
01:46:31,330 --> 01:46:38,360
and so that is the model here that I'm loading.
So this is the ImageNet Large Visual Recognition

1012
01:46:38,360 --> 01:46:43,640
Competion 2012 dataset. And so I trained this
set of weights on the entirety of ImageNet

1013
01:46:43,640 --> 01:46:47,590
so that I created basically a VGG plus batchnorm.

1014
01:46:47,590 --> 01:46:57,110
And so then I fine-tuned the VGG plus batchnorm
model by popping off the end and adding a

1015
01:46:57,110 --> 01:47:06,500
new Dense layer, and then I trained it. These
only took 6 seconds because I precalculated

1016
01:47:06,500 --> 01:47:20,470
the inputs to this. Then I added data augmentation
and I started training that, and then I ran

1017
01:47:20,470 --> 01:47:25,910
out of time because it was class. So I think
this was on the right track. I think if I

1018
01:47:25,910 --> 01:47:28,920
had had another hour or so ....

1019
01:47:28,920 --> 01:47:34,820
You guys can play with this during the week
because this is like all the pieces together.

1020
01:47:34,820 --> 01:47:43,030
It's batchnorm, and data augmentation and
as much dropout as you want.

1021
01:47:43,030 --> 01:47:49,280
So you'll see what I've got here is I have
Dropout layers with an arbitrary amount of

1022
01:47:49,280 --> 01:47:56,179
dropout. The way I set it up, you can go ahead
and say, Create batchnorm layers with whatever

1023
01:47:56,179 --> 01:48:02,100
amount of dropout you want. And then later
on you can say I want you to change the weights

1024
01:48:02,100 --> 01:48:04,219
to use this new amount of dropout.

1025
01:48:04,219 --> 01:48:12,780
So this is kindo of like the ultimate ImageNet
fine-tuning experience. I haven't seen anybody

1026
01:48:12,780 --> 01:48:19,650
create this before, so this is a useful tool
that didn't exist until today. Hopefully during

1027
01:48:19,650 --> 01:48:22,520
the week we'll keep improving it.

1028
01:48:22,520 --> 01:48:28,360
Interestingly, I found that when I went back
to even .5 dropout, it was still massively

1029
01:48:28,360 --> 01:48:35,429
overfitting. So it seems that batchnormalization
allows the model to be so much better at finding

1030
01:48:35,429 --> 01:48:39,670
the optimum, that I actually needed more dropout,
rather than less.

1031
01:48:39,670 --> 01:48:45,489
As I said, this is all kind of something I
was doing today, so I haven't quite finalized

1032
01:48:45,489 --> 01:48:46,949
that.

1033
01:48:46,949 --> 01:48:52,360
What I will show you is something I did finalize
(which I did on Sunday), which is going through

1034
01:48:52,360 --> 01:48:57,980
end-to-end an entire model building process
on NMIST.

1035
01:48:57,980 --> 01:49:02,560
And so I want to show you this entire process
and you guys can play with it.

1036
01:49:02,560 --> 01:49:10,060
MNIST is a great way to really experiment
with and revise everything you know about

1037
01:49:10,060 --> 01:49:14,860
CNNs, because it's very fast to train (because
they're only 28x28 images) and there's also

1038
01:49:14,860 --> 01:49:19,850
extensive benchmarks on what are the best
approaches to MNIST.

1039
01:49:19,850 --> 01:49:25,780
So it's very very easy to get started with
MNIST because Keras actually contains a copy

1040
01:49:25,780 --> 01:49:33,000
of MNIST. We can just go "from keras.datasets
import mnist" and "mnist.load_data()" and

1041
01:49:33,000 --> 01:49:34,190
we're done.

1042
01:49:34,190 --> 01:49:41,659
Now MNIST are greyscale images and everything
in Keras, in terms of convolution stuff, expects

1043
01:49:41,659 --> 01:49:50,460
there to be a number of channels, so we have
to use expand_dims to add this empty dimension.

1044
01:49:50,460 --> 01:49:54,510
So this is 60,000 28x28 images with one color.

1045
01:49:54,510 --> 01:49:55,949
[Time: 1.50 hour mark]

1046
01:49:55,949 --> 01:50:03,239
So if you try to use greyscale images and
get weird errors, I'm pretty sure this is

1047
01:50:03,239 --> 01:50:09,360
what you've forgotten to do, which is to add
this empty dimension. You actually have to

1048
01:50:09,360 --> 01:50:13,930
tell it, there is 1 channel. Otherwise it
doesn't know how many channels there are.

1049
01:50:13,930 --> 01:50:15,820
So there is 1 channel.

1050
01:50:15,820 --> 01:50:22,870
The other thing I had to do was take the Y
values, the labels, and one-hot encode them,

1051
01:50:22,870 --> 01:50:29,690
because otherwise they were like this, they
were actual numbers (5,0,4,1,9) and we need

1052
01:50:29,690 --> 01:50:40,219
to one-hot encode them, because remember this
is the thing that that SoftMax function is

1053
01:50:40,219 --> 01:50:44,150
trying to approximate, that's like how linear
algebra works.

1054
01:50:44,150 --> 01:50:50,511
So there are the two things I had to do to
preprocess this, add the extra dimension and

1055
01:50:50,511 --> 01:50:51,511
do my one-hot encoding.

1056
01:50:51,511 --> 01:50:57,330
Then I normalize the input by subtracting
the mean and dividing by the standard deviation

1057
01:50:57,330 --> 01:51:01,410
and then I try to build a linear model.

1058
01:51:01,410 --> 01:51:09,350
So I can't fine-tune from ImageNet now because
ImageNet is 224x224 and this is 28x28. And

1059
01:51:09,350 --> 01:51:12,510
ImageNet is full color, and this is greyscale.

1060
01:51:12,510 --> 01:51:16,660
So we're going to start from scratch. All
of these are going to start from random.

1061
01:51:16,660 --> 01:51:22,920
So a linear model needs to normalize the input,
it needs to flatten it, because I'm not going

1062
01:51:22,920 --> 01:51:27,820
to treat it as an image, I'm going to treat
it as a single vector. Then I create my one

1063
01:51:27,820 --> 01:51:37,699
Dense layer with 10 outputs, compile it, grab
my batches and train my linear model.

1064
01:51:37,699 --> 01:51:45,500
So you can see, generally speaking, the best
way to train a model is to start by doing

1065
01:51:45,500 --> 01:51:52,650
one epoch with a pretty low learning rate.
So the default learning rate is 0.001, which

1066
01:51:52,650 --> 01:51:54,420
is a pretty good default.

1067
01:51:54,420 --> 01:51:57,969
You'll find nearly all of the time I just
accept the default learning rate and I do

1068
01:51:57,969 --> 01:52:03,790
a single epoch and that's enough to kind of
get it started. Once you've got it started,

1069
01:52:03,790 --> 01:52:07,659
you can set the learning rate really high.
So .1 is about as high as you ever want to

1070
01:52:07,659 --> 01:52:14,429
go. Do another epoch and that's going to move
super-fast, and gradually you reduce the learning

1071
01:52:14,429 --> 01:52:21,969
rate by an order of magnitude at a time. So
go to .01, do a few epochs, and basically

1072
01:52:21,969 --> 01:52:25,870
keep going like that until you start overfitting.

1073
01:52:25,870 --> 01:52:33,070
So I got down to the point where I had a 92.7%
accuracy on the training, 92.4% on the test

1074
01:52:33,070 --> 01:52:35,600
and it's like, Okay that's about as far as
I can go.

1075
01:52:35,600 --> 01:52:38,719
So that's a linear model. Not very interesting.

1076
01:52:38,719 --> 01:52:44,150
So the next thing to do would be to grab 1
extra dense layer in the middle, so 1 hidden

1077
01:52:44,150 --> 01:52:48,760
layer. This is what in the 80's and 90's,
people thought of as a neural network, one

1078
01:52:48,760 --> 01:52:54,880
hidden layer fully connected. And so that
still takes 5 seconds to train.

1079
01:52:54,880 --> 01:52:59,180
We're going to do the same thing. One epoch
with a low learning rate, then pop up the

1080
01:52:59,180 --> 01:53:07,530
learning rate for as long as we can, gradually
decrease it. And we get to 94% accuracy.

1081
01:53:07,530 --> 01:53:13,050
So you wouldn't expect a fully connected network
to do that well. So let's create a CNN. This

1082
01:53:13,050 --> 01:53:18,070
was actually the first architecture I tried.
And basically I thought, we know VGG works

1083
01:53:18,070 --> 01:53:24,219
pretty well, so how about I create an architecture
that looks like VGG but it's much simpler

1084
01:53:24,219 --> 01:53:25,699
because this is just 28x28.

1085
01:53:25,699 --> 01:53:32,460
Okay, well VGG generally has a couple of convolutional
layers of 3x3, and then a MaxPooling layer

1086
01:53:32,460 --> 01:53:38,750
and then a couple more with twice as many
fitlers. So I just tried that. This is kind

1087
01:53:38,750 --> 01:53:41,310
of like my inspired by VGG model.

1088
01:53:41,310 --> 01:53:50,900
I thought, Okay after 2 lots of MaxPooling
it will go from 28x28 to 14x14 to 7x7, that's

1089
01:53:50,900 --> 01:53:55,989
probably enough. So then I added my 2 Dense
layers.

1090
01:53:55,989 --> 01:54:00,780
So I didn't use any science here, just kind
of some intuition. And it actually worked

1091
01:54:00,780 --> 01:54:09,460
pretty well. After my learning rate of .1
I had an accuracy of 98.9%, a validation accuracy

1092
01:54:09,460 --> 01:54:19,400
of 99%. Then after a few layers of .01, I
had an accuracy of 99.75%. But look, my validation

1093
01:54:19,400 --> 01:54:23,400
accuracy is only 99.2%, so I'm overfitting.

1094
01:54:23,400 --> 01:54:29,010
This is the trick. Start by overfitting. Once
you know you're overfitting, you know you

1095
01:54:29,010 --> 01:54:33,070
have a model that is complex enough to handle
your data.

1096
01:54:33,070 --> 01:54:37,239
So at this point I was like, Okay, this is
a good architecture, it's capable of overfitting.

1097
01:54:37,239 --> 01:54:43,030
So let's now try to use the same architecture
and reduce overfitting but reduce the complexity

1098
01:54:43,030 --> 01:54:44,840
of the model no more than necessary.

1099
01:54:44,840 --> 01:54:52,469
Step 1 of my 5 step list was data augmentation.
So I added a bit of data augmentation and

1100
01:54:52,469 --> 01:54:58,860
then I used exactly the same model as I had
before. Trained it for a while, and I found

1101
01:54:58,860 --> 01:55:03,780
this time I could actually train it for even
longer (as you can see) and I started to get

1102
01:55:03,780 --> 01:55:05,930
some pretty good results here, 99.3, 99.34.

1103
01:55:05,930 --> 01:55:07,490
[Time: 1.55 minute mark]

1104
01:55:07,490 --> 01:55:15,300
So by the end you can see I'm massively overfitting
again. 99.6 training vs 99.1 test.

1105
01:55:15,300 --> 01:55:23,090
So data augmentation alone is not enough.
I said to you guys always use batchnorm anyway,

1106
01:55:23,090 --> 01:55:30,630
so then I add batchnorm. I use batchnorm on
every layer. Notice when you use batchnorm

1107
01:55:30,630 --> 01:55:37,780
on convolution layers, you have to add "axis=1".
I'm not going to tell you why. I want you

1108
01:55:37,780 --> 01:55:43,290
guys to read the documentation about batchnorm
and try and figure out why you need this.

1109
01:55:43,290 --> 01:55:47,720
Then we'll have a discussion about it on the
forum because it's a really interesting analysis.

1110
01:55:47,720 --> 01:55:54,239
If you really want to understand batchnorm,
understand why you need this here. If you

1111
01:55:54,239 --> 01:55:59,199
don't care about the details, that's fine.
Just know to type "axis=1" anytime you have

1112
01:55:59,199 --> 01:56:00,739
batchnorm.

1113
01:56:00,739 --> 01:56:06,780
This is like a pretty good quality modern
network. You can see I've got convolution

1114
01:56:06,780 --> 01:56:10,800
layers (they're 3x3), and then I have batchnorm
and then I have MaxPooling and then at the

1115
01:56:10,800 --> 01:56:17,380
end I have some Dense layers. So this is actually
a pretty decent looking model. Not surprisingly,

1116
01:56:17,380 --> 01:56:18,380
it does pretty well.

1117
01:56:18,380 --> 01:56:24,030
So I train it for a while at .1, I train it
for a while at .01, I train it for a while

1118
01:56:24,030 --> 01:56:31,140
at .001, and you can see I get up to 99.5%.
That's not bad.

1119
01:56:31,140 --> 01:56:37,790
But by the end, I'm starting to overfit. So
add a little bit of dropout.

1120
01:56:37,790 --> 01:56:43,679
Remember what I said to you guys, nowadays
the rule for dropout is to kind of gradually

1121
01:56:43,679 --> 01:56:46,239
increase it.

1122
01:56:46,239 --> 01:56:51,760
I only had time yesterday to just try adding
1 layer of dropout right at the end, but as

1123
01:56:51,760 --> 01:56:56,390
it happened, that seemed to be enough. When
I just added one layer of dropout in the previous

1124
01:56:56,390 --> 01:57:06,489
model, trained it for a while at .1, .01,
.001 and it's like, Oh great, my accuracy

1125
01:57:06,489 --> 01:57:13,179
and my validation accuracy are pretty similar.
And my validation accuracy is around 99.5

1126
01:57:13,179 --> 01:57:18,950
to 99.6 towards the end here. So I thought,
Okay, that sounds pretty good.

1127
01:57:18,950 --> 01:57:26,739
So at 99.5% or 99.6% accuracy on handwriting
recognition is pretty good, but there's one

1128
01:57:26,739 --> 01:57:32,500
more trick you can do which makes every model
better, it's called Ensembling.

1129
01:57:32,500 --> 01:57:37,850
Ensembling refers to building multiple versions
of your model and combining them together.

1130
01:57:37,850 --> 01:57:44,429
So what I did was I took all of the code from
that last section and put it into a single

1131
01:57:44,429 --> 01:57:49,219
function. So this is exactly the same model
I had before. And this is the exact steps

1132
01:57:49,219 --> 01:57:55,949
that I took to train it - my learning rate
of .1, .01, .001. And so at the end of this,

1133
01:57:55,949 --> 01:57:58,460
it returns a trained model.

1134
01:57:58,460 --> 01:58:07,510
And so then I said, Okay, 6 times fit a model
and return a list of the results. Models at

1135
01:58:07,510 --> 01:58:17,370
the end of this contain 6 trained models using
my preferred network. So then what I could

1136
01:58:17,370 --> 01:58:26,390
do was go through every one of those 6 models
and predict the output for everything in my

1137
01:58:26,390 --> 01:58:28,310
test set.

1138
01:58:28,310 --> 01:58:39,179
So now I have 10,000 test images by 10 outputs
by 6 models. So now I can take the average

1139
01:58:39,179 --> 01:58:44,510
across the 6 models. So now I'm basically
saying, here are 6 models. They've all been

1140
01:58:44,510 --> 01:58:48,969
trained in the same way, but from different
random starting points. And so the idea is

1141
01:58:48,969 --> 01:58:52,120
that they will be having errors in different
places.

1142
01:58:52,120 --> 01:59:01,340
So let's take the average of them and I get
an accuracy of 99.7%. How good is that? It's

1143
01:59:01,340 --> 01:59:07,969
very, very good. It's so good that if we go
to the academic list of the best MNIST results

1144
01:59:07,969 --> 01:59:13,370
of all time (and remember many of these were
specifically designed for handwriting recognition),

1145
01:59:13,370 --> 01:59:23,340
it comes here. So one afternoon's work gets
us in the list of the best results that are

1146
01:59:23,340 --> 01:59:27,199
found on this dataset.

1147
01:59:27,199 --> 01:59:31,150
As you can see, it's not rocket science. It's
all stuff you've learnt before, you've learnt

1148
01:59:31,150 --> 01:59:39,810
now. It's a process which is fairly repeatable
and can get you right up here in the state-of-the-art.

1149
01:59:39,810 --> 01:59:45,840
It was easier to do on NMIST because I only
had to wait a few seconds for each of my trainings

1150
01:59:45,840 --> 01:59:52,230
to finish. To get to this point on State Farm,
it's going to be harder. You're going to have

1151
01:59:52,230 --> 01:59:56,100
to think about how do you do it in the time
you have available, how do you do it in the

1152
01:59:56,100 --> 02:00:02,840
context of fine-tuning. Hopefully you can
see that you have all of the tools now at

1153
02:00:02,840 --> 02:00:06,260
your disposal to create literally a state-of-the-art
model.

1154
02:00:06,260 --> 02:00:08,199
[Time: 2.0 hour mark]

1155
02:00:08,199 --> 02:00:13,010
So I'm going to make all of these notebooks
available. You can play with them. You can

1156
02:00:13,010 --> 02:00:19,120
try to get a better result from Dogs and Cats.
As you can see, it's kind of an incomplete

1157
02:00:19,120 --> 02:00:22,380
thing that I've done here. I haven't found
the best data augmentation, I haven't found

1158
02:00:22,380 --> 02:00:27,140
the best dropout, I haven't trained it as
long as I probably need to, so there's some

1159
02:00:27,140 --> 02:00:29,300
work for you to do.

1160
02:00:29,300 --> 02:00:36,630
So here are your assignments for this week.
This is all review now. I suggest you go back

1161
02:00:36,630 --> 02:00:41,090
and actually read, there's quite a bit of
prose in every one of these notebooks. Hopefully

1162
02:00:41,090 --> 02:00:45,470
you can go back and read that prose. Some
of that prose that was a first a bit mysterious,

1163
02:00:45,470 --> 02:00:49,390
now it's going to make sense; you'll go, Okay
I see what it's saying.

1164
02:00:49,390 --> 02:00:54,800
And if you read somethign and it doesn't make
sense, ask on the forum. Or if you read something

1165
02:00:54,800 --> 02:00:58,989
and you want to check, Oh is this kind of
another way of saying this other thing?, ask

1166
02:00:58,989 --> 02:01:00,380
on the forum.

1167
02:01:00,380 --> 02:01:07,160
So these are all notebooks that we've looked
at already and you should definitely review.

1168
02:01:07,160 --> 02:01:11,300
Make sure you can replicate the steps shown
in the lesson notebooks we've seen so far

1169
02:01:11,300 --> 02:01:16,670
using the technique in How to Use the Provided
Notebooks.

1170
02:01:16,670 --> 02:01:20,880
If you haven't yet got into the top 50% of
Dogs vs Cats, hopefully you've now got the

1171
02:01:20,880 --> 02:01:26,800
tools to do so. If you get stuck at any point,
ask on the forum.

1172
02:01:26,800 --> 02:01:31,140
And then this is your big challenge, can you
get into the top 50% of State Farm. This is

1173
02:01:31,140 --> 02:01:32,390
tough.

1174
02:01:32,390 --> 02:01:37,820
The first step to doing well in a Kaggle competition
is to create a validation set that gives you

1175
02:01:37,820 --> 02:01:46,159
accurate answers. So create a validation set
and make sure that the validation set accuracy

1176
02:01:46,159 --> 02:01:50,840
is the same as when you submit to Kaggle.
If you don't, you don't have a good enough

1177
02:01:50,840 --> 02:01:52,160
validation set yet.

1178
02:01:52,160 --> 02:01:57,310
Creating a validation set for State Farm is
really your first challenge. It requires thinking

1179
02:01:57,310 --> 02:02:02,640
long and hard about the evaluation section
on their page, what that means. And then thinking

1180
02:02:02,640 --> 02:02:09,540
about which layers of the pre-trained network
should I be retraining?

1181
02:02:09,540 --> 02:02:16,370
I actually have read through the top 20 results
of the competition, it closed 3 months ago.

1182
02:02:16,370 --> 02:02:24,170
I actually think all of the top 20 result
methods are pretty hacky, they're pretty ugly.

1183
02:02:24,170 --> 02:02:30,140
I feel like there's a better way to do this
that is in our grasp.

1184
02:02:30,140 --> 02:02:36,390
I'm hoping that somebody is going to come
up with a top 20 result for State Farm that

1185
02:02:36,390 --> 02:02:44,250
is elegant. We'll see how we go. If not this
year, maybe next year, because nobody in Kaggle

1186
02:02:44,250 --> 02:02:49,370
came up with a really good way of tackling
this. They got some really good results, but

1187
02:02:49,370 --> 02:02:53,480
with some really convoluted methods.

1188
02:02:53,480 --> 02:02:58,580
As you go through and review, please any of
these techniques that you're not clear about,

1189
02:02:58,580 --> 02:03:03,890
these 5 pieces, please go and have a look
at this additioal information and see if that

1190
02:03:03,890 --> 02:03:05,950
helps.

1191
02:03:05,950 --> 00:00:00,000
Hope everything goes well and I will see you
next week.

