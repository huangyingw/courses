1
00:00:00,399 --> 00:00:05,140
I wanted to start by showing you something
that I'm kind of excited about which is here

2
00:00:05,140 --> 00:00:08,980
is the Dogs vs Cats competiton, which we all
know so well.

3
00:00:08,980 --> 00:00:14,309
It was interesting that the winner of this
competition won by a very big margin.

4
00:00:14,309 --> 00:00:19,890
A 1.1% error, vs a 1.7% error.

5
00:00:19,890 --> 00:00:26,369
This is very unusual in this type of competition
to see anybody win by 50% or 60% margin.

6
00:00:26,369 --> 00:00:34,090
After that, it was generally clustering about
1.7% error, about the same kind of number.

7
00:00:34,090 --> 00:00:35,930
This was a pretty impressive performance.

8
00:00:35,930 --> 00:00:42,590
This is the guy who actually created a piece
of deep-learning software called OverFeat.

9
00:00:42,590 --> 00:00:48,140
I want to show you something pretty interesting
which is this week I tried something new and

10
00:00:48,140 --> 00:00:53,830
on Dogs Vs Cats got 98.95.

11
00:00:53,830 --> 00:00:58,420
So I want to show you how I did that.

12
00:00:58,420 --> 00:01:03,810
The way I did that was by using merely only
techniques I've already showed you which is

13
00:01:03,810 --> 00:01:14,680
basically I created a standard model (which
is basically a dense model) and then I pre-computed

14
00:01:14,680 --> 00:01:23,310
the last convolutional layer and then I trained
the dense model a couple times and the other

15
00:01:23,310 --> 00:01:26,850
thing I did was to use some data augmentation.

16
00:01:26,850 --> 00:01:30,210
I didn't have time to figure out the best
data augmentation parameters, so I just picked

17
00:01:30,210 --> 00:01:31,570
some that seemed reasonable.

18
00:01:31,570 --> 00:01:38,090
I should also mention this 98.95, it would
be easy to make it a lot better.

19
00:01:38,090 --> 00:01:42,450
I'm not doing any pseudo labeling here and
I'm not even using the full dataset; I put

20
00:01:42,450 --> 00:01:44,950
aside 2000 for the validation set.

21
00:01:44,950 --> 00:01:50,670
So with those two changes, we would definitely
get over 99% accuracy.

22
00:01:50,670 --> 00:01:58,500
The missing piece that I added is I added
batch normalization to VGG.

23
00:01:58,500 --> 00:02:03,400
Batch normalization (if you guys remember)
I said the important take-away is that all

24
00:02:03,400 --> 00:02:09,348
modern networks should use batchnorm because
you can get 10X or more improvement in training

25
00:02:09,348 --> 00:02:12,109
speed, and it tends to reduce overfitting.

26
00:02:12,109 --> 00:02:21,269
Because of the second one, you can use less
drop-out, and drop-out is destroying some

27
00:02:21,269 --> 00:02:26,749
of your network, so you don't want to use
more dropout than is necessary.

28
00:02:26,749 --> 00:02:30,500
So why didn't VGG already have batchnorm?

29
00:02:30,500 --> 00:02:32,319
Because it didn't exist.

30
00:02:32,319 --> 00:02:43,639
VGG was kind of mid to late 2014 and batchnorm
was maybe early 2015.

31
00:02:43,639 --> 00:02:49,069
So why haven't people added batchnorm to VGG
already.

32
00:02:49,069 --> 00:02:51,280
And the answer is really interesting to think
about.

33
00:02:51,280 --> 00:03:00,849
To remind you of what batchnorm is - batchnorm
is something which first of all normalizes

34
00:03:00,849 --> 00:03:05,419
every intermediate layer, so it normalizes
all the activations by subtracting the mean

35
00:03:05,419 --> 00:03:11,519
and dividing by the standard deviation, which
is always a good idea.

36
00:03:11,519 --> 00:03:15,530
I know somebody on the forum today asked why
it is a good idea, and I put a link to some

37
00:03:15,530 --> 00:03:17,029
more information about that.

38
00:03:17,029 --> 00:03:22,719
So anybody who wants to know more about why
do normalization, check out the forum.

39
00:03:22,719 --> 00:03:31,799
But just doing that alone isn't enough because
SGD is quite bloody-minded, so if it was trying

40
00:03:31,799 --> 00:03:38,359
to denormalize the activations because it
thought that was a good thing to do, it would

41
00:03:38,359 --> 00:03:43,639
do so anyway, so every time you tried to normalize
them, SGD would just undo it again.

42
00:03:43,639 --> 00:03:50,349
So what batchnorm does is it adds two additional
trainable parameters to each layer, one which

43
00:03:50,349 --> 00:03:55,459
multiplies the activations, and one which
is added to the activations.

44
00:03:55,459 --> 00:04:03,171
So it basically allows it to undo the normalization,
but not by changing every single weight, but

45
00:04:03,171 --> 00:04:06,449
by just changing two weights for each activation.

46
00:04:06,449 --> 00:04:11,069
So it makes things much more stable in practice.

47
00:04:11,069 --> 00:04:16,079
So you can't just go ahead and stick batchnorm
in to a pre-trained network because if you

48
00:04:16,079 --> 00:04:24,360
do, it's going to take that layer and divide
all the incoming activation layers, subtract

49
00:04:24,360 --> 00:04:26,900
the mean and divide by the standard deviation.

50
00:04:26,900 --> 00:04:31,750
Which means now those pre-trained weights
from then on, and they're wrong.

51
00:04:31,750 --> 00:04:35,650
Because those weights were created for a completely
different set of activations.

52
00:04:35,650 --> 00:04:37,780
[Time: 5 minute mark]

53
00:04:37,780 --> 00:04:39,810
So, it's not rocket science.

54
00:04:39,810 --> 00:04:48,090
But I realized that all we need to do is to
insert a batchnorm layer and figure out what

55
00:04:48,090 --> 00:04:53,740
the mean and standard deviations of the incoming
activations would be for that dataset and

56
00:04:53,740 --> 00:05:00,539
basically create the batchnorm layer such
that the two trainable parameters immediately

57
00:05:00,539 --> 00:05:01,590
undo that.

58
00:05:01,590 --> 00:05:06,699
Because that way we could insert a batchnorm
layer and it would not change the outputs

59
00:05:06,699 --> 00:05:08,430
at all.

60
00:05:08,430 --> 00:05:17,229
So I grabbed the whole ImageNet, and I created
our standard dense layer model.

61
00:05:17,229 --> 00:05:25,180
I pre-computed the convolutional outputs for
all of ImageNet and then I created 2 batchnorm

62
00:05:25,180 --> 00:05:31,000
layers, and I created a little function which
allows us to insert a layer into an existing

63
00:05:31,000 --> 00:05:32,000
model (inset_layer).

64
00:05:32,000 --> 00:05:37,509
So I inserted the layers just after the two
dense layers.

65
00:05:37,509 --> 00:05:43,610
And then here is the key piece, I set the
weights on the new batchnorm layers equal

66
00:05:43,610 --> 00:05:50,569
to the variance and the mean, which I calculated
on all of ImageNet.

67
00:05:50,569 --> 00:05:54,819
So I calculated the mean of each of those
two layer outputs and the variance of each

68
00:05:54,819 --> 00:05:56,710
of those two layer outputs.

69
00:05:56,710 --> 00:06:03,310
So that allowed me to insert these batchnorm
layers into the existing model.

70
00:06:03,310 --> 00:06:08,560
Then afterwards I evaluated it and I checked
that indeed it's giving me the same answers

71
00:06:08,560 --> 00:06:11,580
as it was before.

72
00:06:11,580 --> 00:06:20,039
As well as doing that, I then kind of thought
that if you train a model with batchnorm from

73
00:06:20,039 --> 00:06:25,199
the start, you're going to end up with weights
which are going to take advantage of the fact

74
00:06:25,199 --> 00:06:27,360
that the activations are being normalized.

75
00:06:27,360 --> 00:06:33,100
So I wondered what would happen if we now
fine-tuned the ImageNet network on all of

76
00:06:33,100 --> 00:06:37,340
ImageNet after we added these batchnorm layers.

77
00:06:37,340 --> 00:06:45,080
So I then tried training it for one epoch
on both the ImageNet images and the horizontally-fit

78
00:06:45,080 --> 00:06:50,440
ImageNet images (that's what these 2.5 million
here are).

79
00:06:50,440 --> 00:06:55,300
As you can see, with modern GPUs and with
pre-computed convolutional layers it takes

80
00:06:55,300 --> 00:06:59,689
less than an hour to run the entirety of ImageNet
twice.

81
00:06:59,689 --> 00:07:06,620
And the interesting thing was that my accuracy
on the validation set went up from 63% to

82
00:07:06,620 --> 00:07:07,620
67%.

83
00:07:07,620 --> 00:07:12,939
So adding batchnorm actually improves ImageNet,
which is cool.

84
00:07:12,939 --> 00:07:16,680
That wasn't the main reason I did it, the
main reason I did it was that we can now use

85
00:07:16,680 --> 00:07:20,210
VGG with batchnorm in our models.

86
00:07:20,210 --> 00:07:39,289
So I did all that, I saved the weights, I
then edited our VGG model.

87
00:07:39,289 --> 00:07:47,919
If we now look at fully-connected block in
our VGG model, it now 

88
00:07:47,919 --> 00:08:00,469
has batchnorm in there, and I also saved to
our website a new weights file, called vgg16_bn

89
00:08:00,469 --> 00:08:10,620
(for batchnorm), so then when I did Cats vs
Dogs, I used that model.

90
00:08:10,620 --> 00:08:19,740
So now if you go and re-download from platform.ai
the vgg16_py it will automatically download

91
00:08:19,740 --> 00:08:23,509
the new weights; you will have this without
any changes to your code.

92
00:08:23,509 --> 00:08:27,540
I'll be interested to hear if during the week
if you try this out (just rerun the code you've

93
00:08:27,540 --> 00:08:31,599
got) whether you see improvements (and hopefully
you will).

94
00:08:31,599 --> 00:08:36,590
Hopefully you'll find it trains more quickly
and you get better results.

95
00:08:36,590 --> 00:08:41,640
At this stage, I've only added batchnorm to
the dense layers, not to the convolutional

96
00:08:41,640 --> 00:08:42,640
layers.

97
00:08:42,640 --> 00:08:50,820
There's no reason I shouldn't add it to the
convolutional layers as well.

98
00:08:50,820 --> 00:08:54,580
Since most of us are mainly fine-tuning just
the dense layers, this is going to impact

99
00:08:54,580 --> 00:08:57,080
most of us the most anyway.

100
00:08:57,080 --> 00:09:08,190
So that's an exciting step which everybody
can now use.

101
00:09:08,190 --> 00:09:14,220
The other thing to mention is now that you'll
be using batchnorm by default in your vgg

102
00:09:14,220 --> 00:09:18,140
networks, you should find that you can increase
your learning rates.

103
00:09:18,140 --> 00:09:24,280
Because batchnorm normalizes the activations,
it makes sure that there's no activation that's

104
00:09:24,280 --> 00:09:28,850
gone really high or really low, and that means
that generally speaking you can use higher

105
00:09:28,850 --> 00:09:30,780
learning rates.

106
00:09:30,780 --> 00:09:34,030
If you try higher learning rates in your code
than you used before, you should find that

107
00:09:34,030 --> 00:09:36,280
they work really well.

108
00:09:36,280 --> 00:09:41,650
You should also find that things that previously
you couldn't get to train, now will start

109
00:09:41,650 --> 00:09:47,660
to train, because often the reason they don't
train is that one of the activations shoots

110
00:09:47,660 --> 00:09:55,060
off into really high or really low and screws
everything up and that kind of gets fixed

111
00:09:55,060 --> 00:09:56,440
when you use batchnorm.

112
00:09:56,440 --> 00:09:58,230
So there's some things to try this week.

113
00:09:58,230 --> 00:10:01,540
I'll be interested to hear how you go.

114
00:10:01,540 --> 00:10:03,200
[Time: 10 minute mark]

115
00:10:03,200 --> 00:10:08,440
So, last week we looked at collaborative filtering.

116
00:10:08,440 --> 00:10:16,850
To remind you, we had a file that basically
meant something like this - we had a bunch

117
00:10:16,850 --> 00:10:23,330
of movies and a bunch of users and for some
subset of those combinations we had a review

118
00:10:23,330 --> 00:10:27,500
of that movie by that user.

119
00:10:27,500 --> 00:10:32,510
The way the actual file came to us it didn't
look like this, this is a cross-tab.

120
00:10:32,510 --> 00:10:39,390
The way the file came to us it looked like
this - each row with a single user with a

121
00:10:39,390 --> 00:10:42,600
single rating at a single time.

122
00:10:42,600 --> 00:10:51,430
So I showed you in Excel how we could take
the crosstab version and we could create a

123
00:10:51,430 --> 00:11:00,270
table of dot products, where the dot products
would be between a set of 5 random numbers

124
00:11:00,270 --> 00:11:06,060
for the movie and 5 random numbers for the
user, and that we could then use gradient

125
00:11:06,060 --> 00:11:13,690
descent to optimize those sets of 5 random
numbers for every user and every movie, and

126
00:11:13,690 --> 00:11:21,500
if we did so, we end up getting pretty decent
guesses as to the original ratings.

127
00:11:21,500 --> 00:11:27,330
Then we went a step further in the spreadsheet
and we learned how you could take the dot

128
00:11:27,330 --> 00:11:35,520
product and you could also add on a single
bias, a movie bias and a user bias.

129
00:11:35,520 --> 00:11:44,520
So we saw all that in Excel and we also learned
that Excel comes with a gradient-descent solver,

130
00:11:44,520 --> 00:11:46,570
called Solver.

131
00:11:46,570 --> 00:11:53,780
We saw that if we ran Solver telling it that
these are our varying cells and that this

132
00:11:53,780 --> 00:12:01,140
is our target cell, then it came up with some
pretty decent weight matrices.

133
00:12:01,140 --> 00:12:05,160
We learned that these kind of weight matrices
are called embeddings.

134
00:12:05,160 --> 00:12:10,480
An embedding is basically something where
we can start with an integer, like 27, and

135
00:12:10,480 --> 00:12:15,460
look up movie #27's vector of weights, that's
called an embedding.

136
00:12:15,460 --> 00:12:20,290
There is also, in collaborative filtering,
this particular type of embedding known as

137
00:12:20,290 --> 00:12:23,080
latent factors.

138
00:12:23,080 --> 00:12:33,420
We hypothesized that, once trained, each of
these latent factors may mean something.

139
00:12:33,420 --> 00:12:36,991
I said next week we might come back, have
a look and see if we can figure out what they

140
00:12:36,991 --> 00:12:38,690
mean.

141
00:12:38,690 --> 00:12:42,190
So that was what I thought I would do now.

142
00:12:42,190 --> 00:12:50,350
So, I'm going to take the bias model we created
- the bias model we created was the one where

143
00:12:50,350 --> 00:13:00,930
we took a user embedding and a movie embedding,
and we took the dot product of the two, and

144
00:13:00,930 --> 00:13:08,390
then we added to it a user bias and a movie
bias, where those biases are just embeddings

145
00:13:08,390 --> 00:13:11,270
which have a single output.

146
00:13:11,270 --> 00:13:17,560
Just like in Excel, the bias was a single
cell for each movie and a single cell for

147
00:13:17,560 --> 00:13:21,110
each user.

148
00:13:21,110 --> 00:13:28,920
So then we tried fitting that model, and you
might remember we ended up getting an accuracy

149
00:13:28,920 --> 00:13:36,680
that was quite a bit higher than previous
state-of-the-art.

150
00:13:36,680 --> 00:13:43,360
Actually, for that one we didn't -- the previous
state-of-the-art we broke by using a neural

151
00:13:43,360 --> 00:13:44,360
network.

152
00:13:44,360 --> 00:13:48,680
I discovered something interesting during
the week, which is I can get a state-of-the-art

153
00:13:48,680 --> 00:13:54,680
result using just this simple bias model,
and the trick was that I just had to increase

154
00:13:54,680 --> 00:13:57,990
my regularization.

155
00:13:57,990 --> 00:14:02,080
We haven't talked too much about regularization,
we've briefly mentioned it a couple of times

156
00:14:02,080 --> 00:14:08,070
but it's a very simple thing where we can
basically say add to the loss function the

157
00:14:08,070 --> 00:14:10,340
sum-of-the-squares of the weights.

158
00:14:10,340 --> 00:14:15,310
We're trying to minimize the loss, so if you're
adding the sum of the squares of the weights

159
00:14:15,310 --> 00:14:21,560
to the loss function, then the SGD solver
is going to have to try avoid increasing the

160
00:14:21,560 --> 00:14:24,380
weights where it can.

161
00:14:24,380 --> 00:14:33,420
So we can pass to most Keras layers a parameter
called W_regularizer (that stands for weight

162
00:14:33,420 --> 00:14:37,420
regularizer) and we can tell it how to regularize
our weights.

163
00:14:37,420 --> 00:14:44,780
In this case I tell it to use a l2 norm (that
means sum-of-the-squares), and how much, in

164
00:14:44,780 --> 00:14:49,790
this case I used 1e-4.

165
00:14:49,790 --> 00:14:57,390
It turns out that if I do that and I train
it for a while (it takes quite a lot longer

166
00:14:57,390 --> 00:15:09,420
to train), I got down to a loss of .7979,
which is quite a bit better than the best

167
00:15:09,420 --> 00:15:11,630
results that that Stanford paper showed.

168
00:15:11,630 --> 00:15:12,910
[Time: 15 minute mark]

169
00:15:12,910 --> 00:15:18,660
It's not quite as good as the neural net,
the neural net got .7938 at best.

170
00:15:18,660 --> 00:15:28,700
But it's still interesting that this very
simple approach actually gets results better

171
00:15:28,700 --> 00:15:35,320
than the academic state-of-the-art as of 2012
or 2013, and I haven't been able to find more

172
00:15:35,320 --> 00:15:41,100
recent academic benchmarks than that.

173
00:15:41,100 --> 00:15:52,100
So I took this model and I wanted to find
out what we can learn from these results.

174
00:15:52,100 --> 00:15:55,720
Obviously one thing we would do with this
model is just to make predictions with it.

175
00:15:55,720 --> 00:16:03,130
So if you are building a website for recommending
movies, and a new user came along and said,

176
00:16:03,130 --> 00:16:07,530
"I like these movies this much, what else
would you recommend?", you could just go through

177
00:16:07,530 --> 00:16:13,140
and do a prediction for each movie for each
userId and tell them which ones had the highest

178
00:16:13,140 --> 00:16:14,140
number.

179
00:16:14,140 --> 00:16:17,510
That's the normal way we use collaborative
filtering.

180
00:16:17,510 --> 00:16:22,150
We can do some other things, we can grab the
top 2,000 most popular movies (just to make

181
00:16:22,150 --> 00:16:28,680
this more interesting) and we can say let's
just grab the bias term.

182
00:16:28,680 --> 00:16:36,360
I'll talk more about this particular syntax
in a moment, but just for now this is a particularly

183
00:16:36,360 --> 00:16:37,720
simple kind of model.

184
00:16:37,720 --> 00:16:43,540
It's a model which simply takes the movieId
in and returns the movie bias out.

185
00:16:43,540 --> 00:16:50,450
It does a look-up in the movie bias table
and returns the movie bias indexed by this

186
00:16:50,450 --> 00:16:51,550
movieId.

187
00:16:51,550 --> 00:16:54,950
That's what these two lines do [get_movie_bias
= Model(movie_in, mb); movie_bias = get_movie_bias.predict(topMovies)].

188
00:16:54,950 --> 00:17:00,490
I then combine this bias with the actual name
of the rating and print out the top and bottom

189
00:17:00,490 --> 00:17:01,490
15.

190
00:17:01,490 --> 00:17:08,660
So, according to MovieLens, the worst movie
of all time is the Church of Scientology classic,

191
00:17:08,660 --> 00:17:13,619
Battlefield Earth, sorry John Travolta.

192
00:17:13,619 --> 00:17:19,448
This is interesting because these ratings
are quite a lot more sophisticated than your

193
00:17:19,449 --> 00:17:20,470
average movie rating.

194
00:17:20,470 --> 00:17:27,950
What this is saying is these have been normalized
-- some reviewers are more positive than negative

195
00:17:27,950 --> 00:17:33,630
than others, some people are watching better
or krappier films than others, so this bias

196
00:17:33,630 --> 00:17:39,411
is removing all that noise, and really telling
us that after removing all that noise, these

197
00:17:39,411 --> 00:17:41,700
are the least good movies.

198
00:17:41,700 --> 00:17:46,310
And Battlefield Earth (-.2616) is even worse
than Spice World (-.1390) by a significant

199
00:17:46,310 --> 00:17:48,200
margin.

200
00:17:48,200 --> 00:17:52,120
On the other hand, here are the best.

201
00:17:52,120 --> 00:18:00,570
Miyazaki fans would be pleased to see Howl's
Moving Castle at #2.

202
00:18:00,570 --> 00:18:02,680
So that's interesting.

203
00:18:02,680 --> 00:18:13,810
Perhaps what's more interesting though 
is to try and figure out what's going on,

204
00:18:13,810 --> 00:18:19,190
not in the biases but in the latent factors.

205
00:18:19,190 --> 00:18:24,730
The latent factors are a little bit harder
to interpret because for every movie we have

206
00:18:24,730 --> 00:18:26,060
50 of them.

207
00:18:26,060 --> 00:18:27,060
In the Excel spreadsheet we have 5.

208
00:18:27,060 --> 00:18:30,170
In our version, we have 50 of them.

209
00:18:30,170 --> 00:18:36,040
So what we want to do is we want to take from
those 50 latent factors, we want to find two

210
00:18:36,040 --> 00:18:41,190
or three kind of main components.

211
00:18:41,190 --> 00:18:46,670
The way we do this (the details aren't important,
but a lot of you will already be familiar

212
00:18:46,670 --> 00:18:52,090
with it) which is that there's something called
PCA, Principal Components Analysis.

213
00:18:52,090 --> 00:18:56,380
Principal Components Analysis does exactly
what I just said -- it looks through a matrix

214
00:18:56,380 --> 00:19:02,470
(in this case its got 50 columns) and it says,
what are the combination of columns that we

215
00:19:02,470 --> 00:19:06,440
can add together because they tend to move
in the same direction.

216
00:19:06,440 --> 00:19:11,430
So in this case, we say start with our 50
columns and I want to create just 3 columns

217
00:19:11,430 --> 00:19:15,180
that capture all of the information of the
original 50.

218
00:19:15,180 --> 00:19:19,760
If you're interested in learning more about
how this works, PCA is something which is

219
00:19:19,760 --> 00:19:23,140
everywhere on the Internet, so there's lots
of information about it.

220
00:19:23,140 --> 00:19:24,360
The details aren't important.

221
00:19:24,360 --> 00:19:30,480
The important thing is to recognize that we're
squishing our 50 latent factors down to 3.

222
00:19:30,480 --> 00:19:38,420
So if we look at the first PCA factor and
we sort on it, we can see that on one end

223
00:19:38,420 --> 00:19:49,820
we have fairly well-regarded movies, like
The Godfather, Pulp Fiction, Usual Suspects,

224
00:19:49,820 --> 00:19:52,580
these are all things which are kind of classics.

225
00:19:52,580 --> 00:19:57,970
At the other end, we have things like Ace
Ventura and RoboCop 3, which are perhaps not

226
00:19:57,970 --> 00:19:59,660
so classic.

227
00:19:59,660 --> 00:20:04,540
So our first PCA factor is some kind of is_classic
score.

228
00:20:04,540 --> 00:20:06,830
[Time: 20 minute mark]

229
00:20:06,830 --> 00:20:11,680
On our second one, we have something similar
but actually very different.

230
00:20:11,680 --> 00:20:18,680
At one end, we've got 10 movies that are huge
Hollywood blockbusers with lots of special

231
00:20:18,680 --> 00:20:25,201
effects, and at the other end we have things
like Annie Hall and Brokeback Mountain, which

232
00:20:25,201 --> 00:20:30,250
are kind of dialog-heavy, not big Hollywood
hits.

233
00:20:30,250 --> 00:20:35,559
So there's another dimension.

234
00:20:35,559 --> 00:20:39,480
This is the first most important dimension
by which people judge movies differently,

235
00:20:39,480 --> 00:20:43,780
and this is the second most important dimension
by which people judge movies differently.

236
00:20:43,780 --> 00:20:47,420
The third most important one by which people
judge movies differently is something where

237
00:20:47,420 --> 00:20:53,490
at one end we have a bunch of violent and
scarey movies.

238
00:20:53,490 --> 00:20:56,970
At the other end, we have happy movies.

239
00:20:56,970 --> 00:21:01,950
For those of you who haven't seen Babe, an
Australian movie, happiest movie ever.

240
00:21:01,950 --> 00:21:08,000
It's about a small pig and its adventures
and its path to success.

241
00:21:08,000 --> 00:21:10,740
Happiest movie ever according to MovieLens.

242
00:21:10,740 --> 00:21:11,740
So that's interesting, right?

243
00:21:11,740 --> 00:21:16,550
It's not just saying that these factors are
good or bad or anything like that, it's just

244
00:21:16,550 --> 00:21:23,090
saying that these are the things that when
we've done this matrix decomposition, these

245
00:21:23,090 --> 00:21:31,780
things pop out as the ways people are different
in their ratings for different kinds of movies.

246
00:21:31,780 --> 00:21:39,220
So one of the reasons I wanted to show you
this is to say that these kind of SGD-learned

247
00:21:39,220 --> 00:21:44,240
many-parameter networks are not inscrutable.

248
00:21:44,240 --> 00:21:50,140
Indeed, it's not great to go in and look at
every one of those 50 latent factor coefficients

249
00:21:50,140 --> 00:21:57,010
in detail, but you have to think about how
to visualize them, how to look at them.

250
00:21:57,010 --> 00:22:01,170
In this case, I actually went a step further
and grabbed a couple of principal components

251
00:22:01,170 --> 00:22:05,600
and tried drawing a picture.

252
00:22:05,600 --> 00:22:10,090
With pictures, of course, you can start to
see things in multiple dimensions.

253
00:22:10,090 --> 00:22:13,830
So here, I've got the first and third principal
components.

254
00:22:13,830 --> 00:22:20,000
So you can see here, on the right hand side
we have more of the Hollywood-type movies,

255
00:22:20,000 --> 00:22:24,910
at the far left, some of the more classic
movies, and at the top the more violent movies,

256
00:22:24,910 --> 00:22:27,270
and at the bottom the happier movies.

257
00:22:27,270 --> 00:22:31,210
Babe's so far happy that it's right off the
bottom.

258
00:22:31,210 --> 00:22:38,890
So if you wanted to find a movie that was
violent and classic, you'd go into the top

259
00:22:38,890 --> 00:22:42,430
left, and Kubrick's A Clockwork Orange would
probably be the one that most people would

260
00:22:42,430 --> 00:22:43,970
come up with first.

261
00:22:43,970 --> 00:22:50,680
If you wanted to come up with something that
was very Hollywood and very non-violent, you

262
00:22:50,680 --> 00:22:54,050
would be down here with Sleepless in Seattle.

263
00:22:54,050 --> 00:23:01,420
You can really learn a lot by looking at these
kind of models, but you don't do it by looking

264
00:23:01,420 --> 00:23:02,740
at the coefficients.

265
00:23:02,740 --> 00:23:06,620
You do it by visualizations, you do it by
interrogating it.

266
00:23:06,620 --> 00:23:13,809
For any of you that have done much statistics
before or have a background in the social

267
00:23:13,809 --> 00:23:18,270
sciences, you've spent most of your time doing
regressions and looking at coefficients and

268
00:23:18,270 --> 00:23:20,970
t-tests and stuff, and this is a very different
world.

269
00:23:20,970 --> 00:23:28,760
This is a world where you're asking the model
questions and getting the modeled results,

270
00:23:28,760 --> 00:23:32,060
which is kind of what we're doing here.

271
00:23:32,060 --> 00:23:36,370
I mentioned I would talk briefly about this
syntax.

272
00:23:36,370 --> 00:23:42,350
This syntax is something we're going to be
using about a lot more often and it's part

273
00:23:42,350 --> 00:23:46,490
of what's called the Keras Functional API.

274
00:23:46,490 --> 00:23:51,380
The Keras functional API is a way of doing
exactly the same things you've already learned

275
00:23:51,380 --> 00:23:55,470
how to do using a different API.

276
00:23:55,470 --> 00:23:57,730
That is not such a dumb idea.

277
00:23:57,730 --> 00:24:02,620
The API you've learned so far is the sequential
API, it's where you use the word "sequential"

278
00:24:02,620 --> 00:24:06,600
[model=Sequential()] and then you write in
order the layers of your neural network.

279
00:24:06,600 --> 00:24:11,679
That's all very well, but what if you wanted
to do something like what we wanted to do

280
00:24:11,679 --> 00:24:16,990
just now where we had like two different things
coming in - we had a userId coming in and

281
00:24:16,990 --> 00:24:20,990
a movieId coming in, and each one went through
its own embedding and then they got multiplied

282
00:24:20,990 --> 00:24:24,200
together - how do you express that as a sequence?

283
00:24:24,200 --> 00:24:26,490
It's not very easy to do that.

284
00:24:26,490 --> 00:24:31,400
So the functional API was designed to answer
this question.

285
00:24:31,400 --> 00:24:35,260
The first thing to note about the functional
API is that you can do everything you can

286
00:24:35,260 --> 00:24:37,700
do in the sequential API.

287
00:24:37,700 --> 00:24:41,950
Here's an example of something you can do
perfectly well with the sequential API, which

288
00:24:41,950 --> 00:24:44,510
is something with two dense layers.

289
00:24:44,510 --> 00:24:45,760
But it looks a bit different.

290
00:24:45,760 --> 00:24:53,790
Every functional API model starts with an
input layer, and then you assign that to some

291
00:24:53,790 --> 00:24:54,790
variable.

292
00:24:54,790 --> 00:25:00,340
And then you list each of the layers in order,
and for each of them, after you've provided

293
00:25:00,340 --> 00:25:07,240
the details for that layer you then immediately
call the layer passing in the output of the

294
00:25:07,240 --> 00:25:08,240
previous layer.

295
00:25:08,240 --> 00:25:09,240
[Time: 25 minute mark]

296
00:25:09,240 --> 00:25:11,740
So this passes in inputs and calls it x [x=Dense(64,activation='relu')(inputs)].

297
00:25:11,740 --> 00:25:15,880
And then this passes in our x and this is
our new version of x [x=Dense(64,actiation='relu')(x)].

298
00:25:15,880 --> 00:25:20,380
And then this dense layer gets the next version
of x and returns predictions [predictions=Dense(10,activation="softmax")(x)].

299
00:25:20,380 --> 00:25:28,380
So you can see that each layer is saying what
its previous layer is, so it's doing exactly

300
00:25:28,380 --> 00:25:33,790
the same thing as a sequential API, just in
a different way.

301
00:25:33,790 --> 00:25:40,020
As the docs note here, the sequential model
is probably a better choice for this particular

302
00:25:40,020 --> 00:25:42,110
network because it's easier.

303
00:25:42,110 --> 00:25:47,480
This is just showing that you can do it.

304
00:25:47,480 --> 00:25:54,809
On the other hand, the model that we just
looked at would be quite difficult, if not

305
00:25:54,809 --> 00:26:01,390
impossible, to do with a sequential API, but
with the functional API, it's very easy.

306
00:26:01,390 --> 00:26:08,770
We created a whole separate model which gave
an output 'u', for user, and that was the

307
00:26:08,770 --> 00:26:11,140
result of creating an embedding.

308
00:26:11,140 --> 00:26:17,060
We said an embedding has its own input and
then goes through an embedding layer, and

309
00:26:17,060 --> 00:26:21,090
then we return input to that and the embedding
layer, like so.

310
00:26:21,090 --> 00:26:24,960
So that gave us our user input and our user
embedding and our movie input and our movie

311
00:26:24,960 --> 00:26:25,960
embedding.

312
00:26:25,960 --> 00:26:29,700
So there's like two separate little models.

313
00:26:29,700 --> 00:26:33,410
And then we did a similar thing to create
two little models for our bias terms; they

314
00:26:33,410 --> 00:26:38,680
were both things that grabbed an embedding,
returning a single output and then flattened

315
00:26:38,680 --> 00:26:39,680
it.

316
00:26:39,680 --> 00:26:41,460
And then we grabbed their biases.

317
00:26:41,460 --> 00:26:45,510
And so now we have four separate models, and
we can merge them.

318
00:26:45,510 --> 00:26:48,030
There's this function called merge.

319
00:26:48,030 --> 00:26:52,260
It's pretty confusing -- there's a small m
'merge' and a big M 'Merge'.

320
00:26:52,260 --> 00:26:54,600
In general, you will be using the small m
merge.

321
00:26:54,600 --> 00:26:58,300
I'm not going into the details of why they
are both there.

322
00:26:58,300 --> 00:27:00,330
They are there for a reason.

323
00:27:00,330 --> 00:27:05,560
If something weird happens to you with merge,
try remembering to use the small m merge.

324
00:27:05,560 --> 00:27:11,429
The small m merge takes two previous outputs
that you've just created using the functional

325
00:27:11,429 --> 00:27:14,390
API and combines them in whatever way you
want.

326
00:27:14,390 --> 00:27:18,370
In this case, the dot product {x=merge([u,m],
mode='dot'}.

327
00:27:18,370 --> 00:27:23,270
So that grabs our user and movie embeddings
and takes the dot product.

328
00:27:23,270 --> 00:27:30,020
We grab the output of that and take our user
bias and the sum, and the output of that and

329
00:27:30,020 --> 00:27:33,490
the movie bias and the sum.

330
00:27:33,490 --> 00:27:41,040
So that's a functional API to creating that
model at the end of which we then use the

331
00:27:41,040 --> 00:27:47,310
model function to actually create a model,
saying what are the inputs to the model and

332
00:27:47,310 --> 00:27:49,860
what is the output of the model.

333
00:27:49,860 --> 00:27:54,810
So you can see this is different to usual
because we've now got multiple inputs.

334
00:27:54,810 --> 00:28:03,630
Then when we call fit, we now have to pass
in an array of inputs, userId and movieId.

335
00:28:03,630 --> 00:28:10,460
So the functional API is something that we're
going to be using increasingly from now on,

336
00:28:10,460 --> 00:28:15,350
now that we've kind of learned all the basic
architectures (just about), we're going to

337
00:28:15,350 --> 00:28:20,120
be starting to build more exotic architectures
for more special cases and we'll be using

338
00:28:20,120 --> 00:28:22,490
the functional API more and more.

339
00:28:22,490 --> 00:28:27,120
Question: Is the only reason to use an embedding
layer so that you can use provide a list of

340
00:28:27,120 --> 00:28:28,590
integers as input?

341
00:28:28,590 --> 00:28:32,670
Answer: That's a great question -- Is the
only reason to use an embedding layer so that

342
00:28:32,670 --> 00:28:34,440
you can use it as an input?

343
00:28:34,440 --> 00:28:36,090
Absolutely yes!

344
00:28:36,090 --> 00:28:40,920
So instead of using an embedding layer, we
could have one-hot encoded all of those userId's

345
00:28:40,920 --> 00:28:45,600
and one-hot encoded all of those movieId's
and create dense layers on top of them, then

346
00:28:45,600 --> 00:28:48,890
it would have done exactly the same thing.

347
00:28:48,890 --> 00:28:59,880
Question: Why choose 50 latent factors and
reduce them down with a Principal Component

348
00:28:59,880 --> 00:29:00,880
Analysis.

349
00:29:00,880 --> 00:29:03,850
Why not just have 3 latent factors to begin
with?

350
00:29:03,850 --> 00:29:14,130
Answer: Sure, if we only used 3 latent factors,
then our predictive model would have been

351
00:29:14,130 --> 00:29:16,770
less accurate.

352
00:29:16,770 --> 00:29:21,350
We want an accurate predictive model so that
when people come to our website we can do

353
00:29:21,350 --> 00:29:24,049
a good job of telling them what movie to watch.

354
00:29:24,049 --> 00:29:26,380
So 50 latent factors for that.

355
00:29:26,380 --> 00:29:30,620
But then for the purpose of visualization
for understanding what those factors are doing,

356
00:29:30,620 --> 00:29:37,210
we want a small number so that we can interpret
them more easily.

357
00:29:37,210 --> 00:29:43,170
So one thing you might want to try during
the week is taking one or two of your models

358
00:29:43,170 --> 00:29:45,810
and converting them to use the functional
API.

359
00:29:45,810 --> 00:29:51,850
Just as a little thing to try to start to
get a hang of how this API looks.

360
00:29:51,850 --> 00:29:55,520
Question: Are these functional models how
we would add additional information to images

361
00:29:55,520 --> 00:30:00,100
in CNN's -- say driving speed or turning rate?

362
00:30:00,100 --> 00:30:01,480
[Time: 30 minute mark]

363
00:30:01,480 --> 00:30:02,510
Answer: Yes, absolutely.

364
00:30:02,510 --> 00:30:08,280
In general, the idea of adding additional
information to a CNN is basically like adding

365
00:30:08,280 --> 00:30:10,090
metadata.

366
00:30:10,090 --> 00:30:12,770
This happens in collaborative filtering a
lot.

367
00:30:12,770 --> 00:30:18,590
You might have a collaborative filtering model
that as well as having the ratings table,

368
00:30:18,590 --> 00:30:22,670
you also have information about what genre
the movie is in.

369
00:30:22,670 --> 00:30:25,860
Maybe the demographic information about the
user.

370
00:30:25,860 --> 00:30:30,950
So you can incorporate all that stuff by having
additional inputs.

371
00:30:30,950 --> 00:30:35,870
With a CNN, let me give you an example.

372
00:30:35,870 --> 00:30:43,170
With the new Kaggle fish recognition competition,
one of the things it turns out is a useful

373
00:30:43,170 --> 00:30:47,390
predictor (it's a leakage problem) is the
size of the image.

374
00:30:47,390 --> 00:30:52,470
So you could have another input which is the
height and width of the image (just as integers)

375
00:30:52,470 --> 00:30:57,030
and have that as a separate input that is
concatenated to the output of your convolutional

376
00:30:57,030 --> 00:31:03,000
layer after the first flatten layer and then
your dense layers can incorporate both convolutional

377
00:31:03,000 --> 00:31:05,610
outputs and your metadata.

378
00:31:05,610 --> 00:31:10,530
Two great questions.

379
00:31:10,530 --> 00:31:15,470
You might remember from last week that this
whole thing about collaborative filtering

380
00:31:15,470 --> 00:31:28,860
was a journey to somewhere else, and the journey
is to NLP, Natural Language Processing.

381
00:31:28,860 --> 00:31:33,360
Question: This is a question about collaborative
filtering.

382
00:31:33,360 --> 00:31:42,669
So if we need to predict the missing values,
the NANs of 0.0, so the user hasn't watched

383
00:31:42,669 --> 00:31:43,860
the movie.

384
00:31:43,860 --> 00:31:45,620
What would be the prediction?

385
00:31:45,620 --> 00:31:55,620
How do we go about predicting that?

386
00:31:55,620 --> 00:32:02,840
Answer: The key purpose of creating this model
is so that you can make predictions for movie-user

387
00:32:02,840 --> 00:32:07,450
combinations you haven't seen before.

388
00:32:07,450 --> 00:32:16,870
The way you do that is to simply do something
like this -- you just call model.predict and

389
00:32:16,870 --> 00:32:22,790
pass in a movieId-userId pair that you haven't
seen before.

390
00:32:22,790 --> 00:32:28,840
And all that's going to do is it's going to
take the dot product of that movie's latent

391
00:32:28,840 --> 00:32:34,059
factors and that user's latent factors and
add on those biases and return you back the

392
00:32:34,059 --> 00:32:35,059
answer.

393
00:32:35,059 --> 00:32:39,630
It's that easy.

394
00:32:39,630 --> 00:32:45,210
If this was a Kaggle competition, that would
be how we would generate our submission for

395
00:32:45,210 --> 00:32:49,260
the Kaggle competition, would be to take that
test set, a bunch of user-movie pairs, that

396
00:32:49,260 --> 00:32:51,890
we hadn't seen before.

397
00:32:51,890 --> 00:32:58,330
Natural Language Processing.

398
00:32:58,330 --> 00:33:05,040
Collaborative filtering is extremely useful
of itself.

399
00:33:05,040 --> 00:33:10,660
Without any doubt, it is far more commerically
important right now than NLP is.

400
00:33:10,660 --> 00:33:18,470
Having said that, fast-ai's mission is to
impact society in as positive a way as possible,

401
00:33:18,470 --> 00:33:23,220
and doing a better job on predicting movies
is not necessarily the best way to do that,

402
00:33:23,220 --> 00:33:28,470
so we're maybe less excited about collaborative
filtering than some people in industry are.

403
00:33:28,470 --> 00:33:31,030
That's why its not our main destination.

404
00:33:31,030 --> 00:33:36,850
NLP, on the other hand, can be a very big
deal if you can do a good job, for example,

405
00:33:36,850 --> 00:33:45,210
of reading through lots of medical journal
articles or family histories or patient notes,

406
00:33:45,210 --> 00:33:50,230
you could be a long way towards creating a
fantastic diagnostic tool to use in the developing

407
00:33:50,230 --> 00:33:54,440
world to help bring medicine to people who
don't currently have it.

408
00:33:54,440 --> 00:33:57,780
Which is almost as good as telling them not
to watch Battlefield Earth.

409
00:33:57,780 --> 00:34:01,530
They're both important.

410
00:34:01,530 --> 00:34:04,490
Let's talk a little bit about NLP.

411
00:34:04,490 --> 00:34:09,069
In order to do this, we're going to look at
a particular dataset.

412
00:34:09,069 --> 00:34:15,379
This dataset is a classic example of what
people do with Natural Language Processing,

413
00:34:15,379 --> 00:34:17,909
and it's called Sentiment Analysis.

414
00:34:17,909 --> 00:34:23,319
Sentiment Analysis means that you take a piece
of text - could be a phrase, a sentence, a

415
00:34:23,319 --> 00:34:30,139
paragraph, or a whole document - and decide
whether or not that is a positive or negative

416
00:34:30,139 --> 00:34:33,589
sentiment.

417
00:34:33,589 --> 00:34:41,940
Keras actually comes with such a dataset,
which is called the IMDb Sentiment dataset.

418
00:34:41,940 --> 00:34:50,509
The IMDb sentiment dataset was originally
developed from the Stanford AI group, and

419
00:34:50,509 --> 00:35:02,750
the paper about it (I think it was actually
published in 2012) is this one here (Learning

420
00:35:02,750 --> 00:35:07,930
Word Vectors for Sentiment Analysis) and they
talk about all the details about what people

421
00:35:07,930 --> 00:35:09,460
want to do with sentiment analysis.

422
00:35:09,460 --> 00:35:11,069
[Time: 35 minute mark]

423
00:35:11,069 --> 00:35:16,759
In general, although academic papers tend
to be way more math-y than they should be,

424
00:35:16,759 --> 00:35:22,829
their introductory sections often do a great
job of capturing why this is an interesting

425
00:35:22,829 --> 00:35:26,099
problem, what kinds of approaches people have
taken and so forth.

426
00:35:26,099 --> 00:35:29,999
The other reason papers are super-helpful
is that you can skip down to the experiments

427
00:35:29,999 --> 00:35:34,029
section (every machine learning paper pretty
much has an experiments section) and find

428
00:35:34,029 --> 00:35:37,150
out what the score is.

429
00:35:37,150 --> 00:35:39,520
Here's the score section.

430
00:35:39,520 --> 00:35:46,039
So here they showed that using this dataset
they created of IMDb movie reviews along with

431
00:35:46,039 --> 00:35:56,430
their sentiment, their full model plus an
additional model, got a score of 88.33% accuracy

432
00:35:56,430 --> 00:35:57,430
in predicting sentiment.

433
00:35:57,430 --> 00:36:01,990
They had another one here where they also
added in some unlabeled data, we're not going

434
00:36:01,990 --> 00:36:05,410
to be looking at that today - that would be
a semi-superised learning problem.

435
00:36:05,410 --> 00:36:13,059
So today our goal is to beat 88.33 as being
the academic state-of-the-art for this dataset

436
00:36:13,059 --> 00:36:17,519
as of this time.

437
00:36:17,519 --> 00:36:23,339
So, to grab it we can just say "from keras.datasets
import imdb".

438
00:36:23,339 --> 00:36:28,390
Keras actually fiddles around with it in ways
that I don't like, so I actually copied and

439
00:36:28,390 --> 00:36:34,160
pasted from the Keras file these three lines
to import it directly, without screwing with

440
00:36:34,160 --> 00:36:35,160
it.

441
00:36:35,160 --> 00:36:43,289
So that's why, rather than using the Keras
dataset directly I'm using these 3 lines.

442
00:36:43,289 --> 00:36:54,470
There are 25,000 movie reviews in the training
set, and here is an example of one: "bromwell

443
00:36:54,470 --> 00:37:01,650
high is a cartoon comedy, it ran at the same
time as some other programs ..."

444
00:37:01,650 --> 00:37:08,259
So the dataset does not actually come to us
in quite this format, it actually comes to

445
00:37:08,259 --> 00:37:13,880
us in this format, which is a list of IDs.

446
00:37:13,880 --> 00:37:22,829
So these IDs then, we can look up in the word
index, which is something they provide.

447
00:37:22,829 --> 00:37:37,779
And so for example, if we look at idx_array.

448
00:37:37,779 --> 00:37:44,549
The word index, as you can see, basically
maps an integer to every word.

449
00:37:44,549 --> 00:37:49,790
It's in order of how frequently those words
appeared in this particular corpus, which

450
00:37:49,790 --> 00:37:51,420
is kind of handy.

451
00:37:51,420 --> 00:37:57,420
So then I also created a reverse index, which
goes from word to ID.

452
00:37:57,420 --> 00:38:07,630
So I can see in the very first training example,
the very first word is number 23022, so if

453
00:38:07,630 --> 00:38:13,520
I look up index to word 23022, idx2word[23022],
it is the word 'bromwell'.

454
00:38:13,520 --> 00:38:20,539
And so then I just go though and I map everything
in that first review to idx2word and join

455
00:38:20,539 --> 00:38:25,309
it together with a space, and that's how we
can turn the data they give us back into a

456
00:38:25,309 --> 00:38:29,530
movie review.

457
00:38:29,530 --> 00:38:33,829
As well as providing the reviews, they also
provide labels.

458
00:38:33,829 --> 00:38:39,710
1 is positive sentiment, 0 is negative sentiment.

459
00:38:39,710 --> 00:38:46,261
So our goal is to take these 25,000 reviews
that look like this, and predict whether it

460
00:38:46,261 --> 00:38:48,779
will be positive or negative in sentiment.

461
00:38:48,779 --> 00:38:55,009
And the data is actually provided to us as
a list of word IDs for each review.

462
00:38:55,009 --> 00:38:58,980
Is everybody clear on the problem we are trying
to solve and how it's laid out?

463
00:38:58,980 --> 00:39:03,119
Okay, you guys are great.

464
00:39:03,119 --> 00:39:05,269
So there's a couple of things we can do to
make it simpler.

465
00:39:05,269 --> 00:39:08,440
One is we can reduce the vocabulary.

466
00:39:08,440 --> 00:39:15,819
So currently there are some pretty unusual
words, like word #23022 is 'Bromwell'.

467
00:39:15,819 --> 00:39:21,619
And if we're trying to figure out how to deal
with all these different words, having to

468
00:39:21,619 --> 00:39:25,789
figure out the various ways in which the word
'Bromwell' is being used is probably not going

469
00:39:25,789 --> 00:39:28,920
to net us much for a lot of computation and
memory cost.

470
00:39:28,920 --> 00:39:32,300
So we're going to truncate the vocabulary
down to 5,000.

471
00:39:32,300 --> 00:39:37,499
And it's very easy to do that because the
words are already ordered by frequency, I

472
00:39:37,499 --> 00:39:45,460
simply go through everything in our training
set and I just say if the word ID is less

473
00:39:45,460 --> 00:39:48,839
than this vocab_size of 5,000, we'll leave
it as it is.

474
00:39:48,839 --> 00:39:54,180
Otherwise, we'll replace it with the number
5000.

475
00:39:54,180 --> 00:40:00,119
So at the end of this, we now have repaced
all of our rare words with a single ID.

476
00:40:00,119 --> 00:40:02,369
[Time: 40 minute mark]

477
00:40:02,369 --> 00:40:04,749
Here's a quick look at these sentences.

478
00:40:04,749 --> 00:40:10,789
The reviews are sometimes up to 2493 words
long (some people spend far too much time

479
00:40:10,789 --> 00:40:18,019
on IMDb), some are as short as 10 words, on
average they are 237 words.

480
00:40:18,019 --> 00:40:26,279
As you'll see, we actually need to make all
of our reviews the same length, so allowing

481
00:40:26,279 --> 00:40:34,769
this 2493 word review uses a lot of memory
and time, so we re going to decide to truncate

482
00:40:34,769 --> 00:40:39,880
every review to 500 words, and that's more
than twice as big as the mean.

483
00:40:39,880 --> 00:40:42,140
So we're not going to lose too much.

484
00:40:42,140 --> 00:40:52,529
Question: What if the word "5000" gets a bias?

485
00:40:52,529 --> 00:41:07,130
Answer: Whatever the word "5000" is Let's
find out what the word "5000" is, idx2word(5000)

486
00:41:07,130 --> 00:41:12,690
... I guess the year "1987".

487
00:41:12,690 --> 00:41:21,019
That's fine, we're about to learn a machine-learning
model and so the vast majority of the time

488
00:41:21,019 --> 00:41:27,140
it comes across the word #5000, it's actually
going to mean rare word; it's not going to

489
00:41:27,140 --> 00:41:29,660
specifically mean 1987.

490
00:41:29,660 --> 00:41:34,319
And it's going to learn to deal with that
as best as it can.

491
00:41:34,319 --> 00:41:38,910
The idea is the rare words don't appear too
often, so hopefully this is not going to cause

492
00:41:38,910 --> 00:41:39,910
too much trouble.

493
00:41:39,910 --> 00:41:43,430
Question: Doesn't just using frequency favor
stop-words

494
00:41:43,430 --> 00:41:50,940
Answer: We're not just using frequencies,
all we're doing, we're just truncating our

495
00:41:50,940 --> 00:41:54,260
vocabulary at this point.

496
00:41:54,260 --> 00:42:02,960
Question: So the 5000 word, can we just replace
it with some neutral word and we don't need

497
00:42:02,960 --> 00:42:04,549
to take care of that bias thing?

498
00:42:04,549 --> 00:42:07,489
Answer: There's really not going to be a bias
here.

499
00:42:07,489 --> 00:42:11,859
We're just replacing it with a random ID.

500
00:42:11,859 --> 00:42:20,739
The fact that occasionally the word 1987 actually
pops up is totally insignificant.

501
00:42:20,739 --> 00:42:24,499
We could replace it with -1.

502
00:42:24,499 --> 00:42:27,569
It's just a sentinal value which has no meaning.

503
00:42:27,569 --> 00:42:33,430
Comment: Also, we're getting rid of all the
words that are 5000 and beyond.

504
00:42:33,430 --> 00:42:35,130
It's not a single word.

505
00:42:35,130 --> 00:42:36,150
Response: That's correct.

506
00:42:36,150 --> 00:42:42,140
It represents all of the less common words.

507
00:42:42,140 --> 00:42:52,630
It's one of those design decisions which it's
not worth spending a lot of time thinking

508
00:42:52,630 --> 00:42:54,060
about because it's not significant.

509
00:42:54,060 --> 00:42:58,469
I just picked what happened to be easiest
at the time.

510
00:42:58,469 --> 00:43:04,640
As I said, I could have just as well used
-1 because it's not important.

511
00:43:04,640 --> 00:43:14,960
What is important is that we have to create
a rectangular matrix which we can pass to

512
00:43:14,960 --> 00:43:18,160
our machine learning model.

513
00:43:18,160 --> 00:43:22,109
Quite conveniently Keras comes with something
caled pad_sequences that does this for us.

514
00:43:22,109 --> 00:43:28,769
It takes everything greater than this length
and truncates it and everything less than

515
00:43:28,769 --> 00:43:34,980
this length and pads it with whatever we ask
for, which in this case is 0's.

516
00:43:34,980 --> 00:43:41,550
So at the end of this, the shape of our training
set is now a numpy array with 25,000 rows

517
00:43:41,550 --> 00:43:48,430
and 500 columns, and as you can see, it's
padded the front with 0's, such that it has

518
00:43:48,430 --> 00:43:51,240
500 words in it.

519
00:43:51,240 --> 00:43:53,170
Other than that, it's exactly the same as
before.

520
00:43:53,170 --> 00:43:59,359
And as you can see "bromwell" is not replaced
with 5000, but with 4999.

521
00:43:59,359 --> 00:44:02,979
So this is our same movie review again after
going through that padding process.

522
00:44:02,979 --> 00:44:07,599
Question: Does it matter if we pad from the
left or the right?

523
00:44:07,599 --> 00:44:12,700
Answer: I know that there's some reason that
Keras decided to pad the front rather than

524
00:44:12,700 --> 00:44:13,779
the back.

525
00:44:13,779 --> 00:44:16,829
I don't recall what it is.

526
00:44:16,829 --> 00:44:24,119
Since it's what it does by default, I don't
worry about it; I don't think it's important.

527
00:44:24,119 --> 00:44:30,960
Now that we have a rectangular matrix of numbers,
and we have some labels, we can use the exact

528
00:44:30,960 --> 00:44:35,119
techniques we've already learned to create
a model.

529
00:44:35,119 --> 00:44:39,960
As per usual, we should try and create the
simplest models we possible can to start with.

530
00:44:39,960 --> 00:44:45,579
And we know that the simplest model we can
is one with 1 hidden layer in the middle.

531
00:44:45,579 --> 00:44:51,299
This is the simplest model that we generally
think ought to be useful for just about everything.

532
00:44:51,299 --> 00:44:55,930
Now, here is why we started with collaborative
filtering - it is because we started with

533
00:44:55,930 --> 00:44:57,640
an embedding.

534
00:44:57,640 --> 00:45:05,119
So if you think about it, our input are word
IDs and we want to convert that into a vector,

535
00:45:05,119 --> 00:45:07,130
and that is what an embedding does.

536
00:45:07,130 --> 00:45:09,099
[Time: 45 minute mark]

537
00:45:09,099 --> 00:45:17,950
Rather than one-hot encoding this into a 5,000
column long huge input thing and then doing

538
00:45:17,950 --> 00:45:25,799
a matrix product, an embedding just says,
look up that movieId and grab that vector

539
00:45:25,799 --> 00:45:27,219
directly.

540
00:45:27,219 --> 00:45:34,430
So it's just a computational and memory shortcut
to creating a one-hot encoding followed by

541
00:45:34,430 --> 00:45:35,559
a matrix product.

542
00:45:35,559 --> 00:45:43,819
So we're creating an embedding where we're
going to have 5,000 latent factors, or 5,000

543
00:45:43,819 --> 00:45:53,450
embeddings, each one we're going to have 32
items, rather than 50.

544
00:45:53,450 --> 00:45:57,999
So then we're going to flatten that, have
our single dense layer, a bit of drop out,

545
00:45:57,999 --> 00:46:00,800
and then our output to a sigmoid.

546
00:46:00,800 --> 00:46:05,660
That's a pretty simple model.

547
00:46:05,660 --> 00:46:08,920
It's a good idea to go through and make sure
you understand why all these parameter counts

548
00:46:08,920 --> 00:46:10,069
are what they are.

549
00:46:10,069 --> 00:46:14,099
That's something you can do during the week
and double-check that you're comfortable with

550
00:46:14,099 --> 00:46:15,099
all of those.

551
00:46:15,099 --> 00:46:21,309
So this is the size of each of the weight
matrices at each point.

552
00:46:21,309 --> 00:46:36,069
We can fit it and after 
one epoch we have 88% accuracy on the validation

553
00:46:36,069 --> 00:46:37,069
set.

554
00:46:37,069 --> 00:46:52,380
And so let's just compare that to Stanford
where they had 88.3 and we have 88.04.

555
00:46:52,380 --> 00:46:56,069
So we're not yet there, but we're well on
the right track.

556
00:46:56,069 --> 00:46:58,950
Question: Why 32?

557
00:46:58,950 --> 00:47:07,229
Answer: This is always the question about
why have X number of filters in your convolutional

558
00:47:07,229 --> 00:47:11,339
layer, or why have X number of outputs in
your dense layer?

559
00:47:11,339 --> 00:47:17,680
It's just a case of trying things and seeing
what works, and also getting some intuition

560
00:47:17,680 --> 00:47:19,480
by looking at other models.

561
00:47:19,480 --> 00:47:22,789
In this case, I think 32 was the first I tried.

562
00:47:22,789 --> 00:47:30,239
I kind of felt like from my understanding
of really big embedding models (which you'll

563
00:47:30,239 --> 00:47:36,410
learn about shortly) even 50 dimensions is
enough to capture vocabularies of size 100,000

564
00:47:36,410 --> 00:47:41,160
or more, so I felt like 32 was likely to be
more than enough to capture a vocabulary of

565
00:47:41,160 --> 00:47:42,160
size 5,000.

566
00:47:42,160 --> 00:47:46,660
I tried it, I got a pretty good result and
basically left it there.

567
00:47:46,660 --> 00:47:53,119
If at some point I decided I wasn't getting
great results, I would try increasing it.

568
00:47:53,119 --> 00:47:57,670
Question: Why sigmoid in the final layer?

569
00:47:57,670 --> 00:48:03,200
Answer: You can always use a SoftMax instead
of a sigmoid.

570
00:48:03,200 --> 00:48:09,529
It just means that you would have to change
your labels - remember, our labels were just

571
00:48:09,529 --> 00:48:15,549
1s or 0s, a single column.

572
00:48:15,549 --> 00:48:21,960
If I wanted to use SoftMax I would have to
create two columns - it wouldn't just be one,

573
00:48:21,960 --> 00:48:25,380
it would be one-zero-one-zero-one-zero.

574
00:48:25,380 --> 00:48:30,549
In the past, I've generally stuck to using
SoftMax and then categorical cross-entropy

575
00:48:30,549 --> 00:48:34,320
loss, just to be consistent, because then
regardless of whether you have two classes

576
00:48:34,320 --> 00:48:38,059
or more than two classes, you can always get
the same thing.

577
00:48:38,059 --> 00:48:44,259
In this case I want to show the other way
that you can do this, which is to have a single

578
00:48:44,259 --> 00:48:45,380
column output.

579
00:48:45,380 --> 00:48:50,230
And remember signmoid is exactly the same
thing as SoftMax if you have a binary output.

580
00:48:50,230 --> 00:48:53,380
Question: And so here, 1 is positive and 0
is negative?

581
00:48:53,380 --> 00:48:54,380
Answer: Yes.

582
00:48:54,380 --> 00:48:57,652
And so, rather than using categorical cross_entropy,
we used binary_crossentropy, and again, it's

583
00:48:57,652 --> 00:48:58,680
exactly the same thing.

584
00:48:58,680 --> 00:49:02,759
It just means I don't have to worry about
one-hot encoding the output because it's just

585
00:49:02,759 --> 00:49:03,789
a binary output.

586
00:49:03,789 --> 00:49:10,329
Question: Do we know what the inter-rater
agreement is for this dataset?

587
00:49:10,329 --> 00:49:12,490
Answer: No we don't.

588
00:49:12,490 --> 00:49:16,520
It's not something I have looked at.

589
00:49:16,520 --> 00:49:21,759
The important thing as far as I'm concerned
is what is the benchmark that the Stanford

590
00:49:21,759 --> 00:49:22,759
people got.

591
00:49:22,759 --> 00:49:25,599
They compared it to a range of other previous
benchmarks and they found that their technique

592
00:49:25,599 --> 00:49:28,979
was the best, so that's my goal here.

593
00:49:28,979 --> 00:49:32,720
And I'm sure there have been other techniques
to come out since then (probably better),

594
00:49:32,720 --> 00:49:41,319
but I haven't seen them in any papers yet,
so this is my target.

595
00:49:41,319 --> 00:49:50,069
You can see that we can in one second of training,
get an accuracy which is pretty competitive,

596
00:49:50,069 --> 00:49:52,730
and it's just a simple neural net.

597
00:49:52,730 --> 00:49:58,209
Hopefully you're starting to get a sense that
a neural net with one hidden layer is a great

598
00:49:58,209 --> 00:50:00,329
starting point for nearly everything.

599
00:50:00,329 --> 00:50:05,839
You now know how to create a pretty good sentiment
analysis model, and before today, you didn't,

600
00:50:05,839 --> 00:50:06,839
so that's a good step.

601
00:50:06,839 --> 00:50:07,839
[Time: 50 minute mark]

602
00:50:07,839 --> 00:50:08,839
Question: Just to confirm, whenever we use
binary_crossentropy, we use sigmoid for the

603
00:50:08,839 --> 00:50:10,609
final layer.

604
00:50:10,609 --> 00:50:12,420
Answer: Yes.

605
00:50:12,420 --> 00:50:16,329
Question: Could you explain embedding again?

606
00:50:16,329 --> 00:50:21,789
What is the actual imput to the dense layer
for Word 2345?

607
00:50:21,789 --> 00:50:27,589
Answer: This is an embedding (col-H->col-V;
row-20->row-24).

608
00:50:27,589 --> 00:50:39,809
I think it would be particularly helpful if
we go back to our MovieLens recommendation

609
00:50:39,809 --> 00:50:41,569
dataset (lesson4.ipynb, Set up data).

610
00:50:41,569 --> 00:50:51,559
And remember that the actual data coming in
does not look like this, it looks like this.

611
00:50:51,559 --> 00:50:56,569
So then when we then come along and say, okay
what do we predit the rating would be for

612
00:50:56,569 --> 00:51:07,470
userId 1 for movieId 1172, we actually have
to go through our list of movieIds and find

613
00:51:07,470 --> 00:51:15,180
movieId 31, and then having found 31, then
look up its latent factor.

614
00:51:15,180 --> 00:51:20,059
Then we have to do the same thing for userId
1 and then find its latent factor, and then

615
00:51:20,059 --> 00:51:21,950
we have to multply the two together.

616
00:51:21,950 --> 00:51:28,550
So that step of taking a ID, finding it in
a list and returning the vector it is attached

617
00:51:28,550 --> 00:51:31,309
to, that is what an embedding is.

618
00:51:31,309 --> 00:51:40,779
So an embedding returns a vector which is
or length (in this case) 32.

619
00:51:40,779 --> 00:51:46,481
(note that the "None" here always means your
mini-batch size)

620
00:51:46,481 --> 00:51:57,109
So for each movie review, for each of the
500 words in that sequence, you're getting

621
00:51:57,109 --> 00:51:59,400
a 32-element vector.

622
00:51:59,400 --> 00:52:07,569
And so therefore you have a mini-batch size,
by 500 X 32 tensor coming out of this layer.

623
00:52:07,569 --> 00:52:16,339
That gets flattened (so 500*32=16,000), and
then that is the input to your first dense

624
00:52:16,339 --> 00:52:17,339
layer.

625
00:52:17,339 --> 00:52:24,799
Comment: I also think it might be helpful
to show that for a review, instead of having

626
00:52:24,799 --> 00:52:30,279
that word that's being entered as a sequence
of numbers, where that number is ...

627
00:52:30,279 --> 00:52:32,229
Response: Yes, that's right.

628
00:52:32,229 --> 00:52:39,140
So we look at this first review, and remember
the 23022 word review has now been truncated

629
00:52:39,140 --> 00:52:45,539
to 499, this is 309 so it's going to take
309 and look up the 309th vector in the embedding,

630
00:52:45,539 --> 00:52:53,930
and it's going to return it and it's going
to concatenate it to create this tensor.

631
00:52:53,930 --> 00:52:54,930
So that's all an embedding is.

632
00:52:54,930 --> 00:53:02,499
An embedding is a shortcut to a one-hot encoding,
followed by a matrix product.

633
00:53:02,499 --> 00:53:07,839
Question: Can you show us words which have
similar latent features?

634
00:53:07,839 --> 00:53:11,380
I'm hoping these words would be synonyms or
semantically similar.

635
00:53:11,380 --> 00:53:13,299
Answer: Yes, we'll see that shortly.

636
00:53:13,299 --> 00:53:17,760
Question: Who made the labels and why should
I believe them?

637
00:53:17,760 --> 00:53:19,530
It seems difficult and subjective.

638
00:53:19,530 --> 00:53:23,630
Answer: That's the whole point of sentiment
analysis and these kinds of things is that

639
00:53:23,630 --> 00:53:24,880
it's totally subjective.

640
00:53:24,880 --> 00:53:30,480
The interesting thing about NLP is that we're
trying to capture something which is very

641
00:53:30,480 --> 00:53:31,599
subjective.

642
00:53:31,599 --> 00:53:35,440
So in this case, you would have to read the
original paper (Learning Word Vectors for

643
00:53:35,440 --> 00:53:40,920
Seniment Analysis) to find out how they got
these particular labels.

644
00:53:40,920 --> 00:53:46,690
The way that people tend to get labels is
either in this case, the IMDb dataset.

645
00:53:46,690 --> 00:53:51,420
IMDb has ratings - so you could just say everything
higher than 8 is very positve, everything

646
00:53:51,420 --> 00:53:56,089
lower than 2 is very negative, and we'll throw
away everything in the middle.

647
00:53:56,089 --> 00:54:02,440
The other way people tend to label academic
datasets is to send it off to Amazon Mechanical

648
00:54:02,440 --> 00:54:07,640
Turk and pay them a few cents to label each
thing.

649
00:54:07,640 --> 00:54:10,859
That's the kinds of ways that you can label
stuff.

650
00:54:10,859 --> 00:54:16,349
Comment: And there are places where people
don't just use Mechanical Turk, but they specifically

651
00:54:16,349 --> 00:54:18,569
try to hire linguistics PhD's.

652
00:54:18,569 --> 00:54:19,690
Response: Yes.

653
00:54:19,690 --> 00:54:23,829
You certainly wouldn't want to do that for
this because the whole purpose here is to

654
00:54:23,829 --> 00:54:27,720
capture normal people's sentiments.

655
00:54:27,720 --> 00:54:37,789
For example, when I was in medicine, we went
through all these radiology reports and tried

656
00:54:37,789 --> 00:54:41,869
to capture which ones were critical findings
and which ones weren't critical findings and

657
00:54:41,869 --> 00:54:46,250
we used good radiologists, rather than Mechanical
Turk for that purpose.

658
00:54:46,250 --> 00:54:52,529
Question: So you're not considering any sentence
construction or bigrams?

659
00:54:52,529 --> 00:54:57,920
Just a Bag of Words and the literal set of
words that are being used?

660
00:54:57,920 --> 00:54:59,109
[Time: 55 minute mark]

661
00:54:59,109 --> 00:55:01,109
Answer: It's not actually just a Bag of Words.

662
00:55:01,109 --> 00:55:06,989
If you think about it, this dense layer here
has 1.6 million parameters and its connecting

663
00:55:06,989 --> 00:55:13,470
every one of those 500 inputs to our output.

664
00:55:13,470 --> 00:55:22,200
Not only that, but it's doing that for every
one of the incoming factors.

665
00:55:22,200 --> 00:55:30,640
So it's creating a pretty complex, big cartesian
product of all of these weights.

666
00:55:30,640 --> 00:55:35,859
It's taking account of the position of a word
in the overall sentence.

667
00:55:35,859 --> 00:55:40,640
It's not terribly sophisticated and it's not
taking into account its position compared

668
00:55:40,640 --> 00:55:47,980
to other words, but it is taking into account
where abouts it occurs in the whole review.

669
00:55:47,980 --> 00:55:57,069
It's not like the dumbest kind of model I
could come up with, it's a good starting point.

670
00:55:57,069 --> 00:56:01,540
But we would expect with a little bit of thought
(which we're about to use), we could do a

671
00:56:01,540 --> 00:56:04,920
lot better.

672
00:56:04,920 --> 00:56:06,949
So why don't we go ahead and do that.

673
00:56:06,949 --> 00:56:11,520
So the slightly better - hopefully you guys
have all predicted what that would be - a

674
00:56:11,520 --> 00:56:12,989
convolutional neural network.

675
00:56:12,989 --> 00:56:17,410
And the reason I hope you predicted that is:
A) we've already talked about how CNN's are

676
00:56:17,410 --> 00:56:19,959
taking over the world and
B)

677
00:56:19,959 --> 00:56:24,039
specifically they're taking over the world
any time we have some kind of ordered data.

678
00:56:24,039 --> 00:56:26,259
And clearly a sentence is ordered.

679
00:56:26,259 --> 00:56:28,229
One word comes after another word.

680
00:56:28,229 --> 00:56:29,699
It has a specific ordering.

681
00:56:29,699 --> 00:56:34,630
So therefore we can use a convolution.

682
00:56:34,630 --> 00:56:39,569
We can't use a 2D convolution because a sentence
is not in 2D, a sentence is in 1D.

683
00:56:39,569 --> 00:56:42,069
So we're going to use a 1D convolution.

684
00:56:42,069 --> 00:56:45,329
So a 1D convolution is even simpler than a
2D convolution.

685
00:56:45,329 --> 00:56:51,650
We're just going to grab a string of a few
words and we're going to take their embeddings,

686
00:56:51,650 --> 00:56:56,339
and we're going to take that string and we're
going to multiply it by some filter.

687
00:56:56,339 --> 00:57:01,549
And then we're going to move that sequence
along our sentence.

688
00:57:01,549 --> 00:57:10,989
So this is our normal next place we go as
we try to gradually increase the complexity,

689
00:57:10,989 --> 00:57:18,979
which is to grab our simplest possible CNN,
which is a Convolution, Dropout, MaxPooling

690
00:57:18,979 --> 00:57:22,930
and then Flatten that and we have our Dense
layer and our Output.

691
00:57:22,930 --> 00:57:28,140
So this is exactly like exactly what we did
when we looking at gradually improving our

692
00:57:28,140 --> 00:57:32,839
State Farm result, but rather than having
Convolution2D, we have Convolution1D.

693
00:57:32,839 --> 00:57:39,839
The parameters are exactly the same -- how
many filters do you want to create, and what

694
00:57:39,839 --> 00:57:41,640
is the size of your convolution.

695
00:57:41,640 --> 00:57:48,089
Originally I tried 3 here but 5 turned out
to be better so I'm looking at 5 words at

696
00:57:48,089 --> 00:57:54,490
a time and multiplying them by each one of
64 filters.

697
00:57:54,490 --> 00:58:02,219
So we're going to start with the same embedding
as before.

698
00:58:02,219 --> 00:58:10,710
We take our sentences and we turn them into
a 500x32 matrix for each of our inputs.

699
00:58:10,710 --> 00:58:13,380
We then put it through our convolution.

700
00:58:13,380 --> 00:58:20,369
And because our convolution has a border_mode='same',
we get back exactly the same shape as we gave

701
00:58:20,369 --> 00:58:21,369
it.

702
00:58:21,369 --> 00:58:25,599
We then put it through our 1D MaxPooling,
and that will halve its size and then we stick

703
00:58:25,599 --> 00:58:29,589
it through the same dense layers as we had
before.

704
00:58:29,589 --> 00:58:34,749
So that's a really simple convolutional network
for words.

705
00:58:34,749 --> 00:58:55,740
Compile it, run it, and we get 89.47, compared
to 88.33.

706
00:58:55,740 --> 00:59:00,269
So we've already broken the academic state-of-the-art
as of when this paper was written.

707
00:59:00,269 --> 00:59:06,459
Again, a simple convolutional neural network
gets us a very, very long way.

708
00:59:06,459 --> 00:59:16,890
Question: Convolution2D for images is easier
to understand, element-wise multiplication

709
00:59:16,890 --> 00:59:17,890
and addition.

710
00:59:17,890 --> 00:59:22,229
But what does it mean for a sequence of words?

711
00:59:22,229 --> 00:59:25,289
Answer: Don't think of it as a sequence of
words.

712
00:59:25,289 --> 00:59:27,460
Remember, it's been through an embedding.

713
00:59:27,460 --> 00:59:31,890
So it's a sequence of 32-element vectors.

714
00:59:31,890 --> 00:59:37,880
So it's doing exactly the same thing we were
doing in a 2D convolution, but rather than

715
00:59:37,880 --> 00:59:44,709
having 3 channels of color, we have 32 channels
of embedding.

716
00:59:44,709 --> 00:59:48,539
[Time: 1 hour mark]

717
00:59:48,539 --> 01:00:01,460
Just like in our convolution spreadsheet,
remember how in the second one, once we had

718
01:00:01,460 --> 01:00:13,130
two filters already, our filter had to be
a 3x3x2 tensor in order to allow us to create

719
01:00:13,130 --> 01:00:14,390
the second layer.

720
01:00:14,390 --> 01:00:25,819
For us, we now don't have a 3x3x2 tensor,
we have a 5x1x32, or more conveniently a 5x32

721
01:00:25,819 --> 01:00:26,819
matrix.

722
01:00:26,819 --> 01:00:32,589
So each convolution is going to go through
each of the 5 words and each of the 32 embeddings,

723
01:00:32,589 --> 01:00:37,749
do an element-wise multiplication and add
them all up.

724
01:00:37,749 --> 01:00:42,990
The important thing to remember is once we've
done the embedding layer, which is always

725
01:00:42,990 --> 01:00:48,099
going to be our first step, for every NLP
model, is that we don't have words anymore.

726
01:00:48,099 --> 01:00:55,029
We now have vectors which are attempting to
capture the information in that word in some

727
01:00:55,029 --> 01:01:02,099
way, just like our latent factors captured
information about a movie and a user in our

728
01:01:02,099 --> 01:01:03,749
collaborative filtering.

729
01:01:03,749 --> 01:01:06,259
We haven't yet looked at what they do.

730
01:01:06,259 --> 01:01:12,039
We wil in a moment, just like we did with
the movie vectors, but we do know from our

731
01:01:12,039 --> 01:01:19,599
experience that SGD is going to try to fill
out those 32 places with information about

732
01:01:19,599 --> 01:01:26,839
how that word is being used, which allow us
to make these predictions.

733
01:01:26,839 --> 01:01:31,859
Just like when you first learned about 2D
convolutions, it took you probably a few days

734
01:01:31,859 --> 01:01:37,259
of fiddling around with spreadsheets, and
pieces of paper, and Python, and checking

735
01:01:37,259 --> 01:01:41,459
inputs and outputs to get a really intuitive
understanding of what a 2D convolution is

736
01:01:41,459 --> 01:01:42,479
doing.

737
01:01:42,479 --> 01:01:47,239
You may find it is the same with a 1D convolution,
that it will take you probably a fifth of

738
01:01:47,239 --> 01:02:07,709
the time to get there because you've really
done all the hard work already.

739
01:02:07,709 --> 01:02:16,609
There's a couple of concepts that we come
across in this class that there is no way

740
01:02:16,609 --> 01:02:20,940
that me lecturing to you is going to be enough
for you to get an intuitive understanding

741
01:02:20,940 --> 01:02:21,940
of it.

742
01:02:21,940 --> 01:02:25,180
The first, clearly, is the 2D convolution.

743
01:02:25,180 --> 01:02:30,979
Hopefully, you've had lots of opportunities
to experiment and practice and read; these

744
01:02:30,979 --> 01:02:35,869
are things you have to tackly from many, many
different directions to understand a 2D convolution.

745
01:02:35,869 --> 01:02:41,130
2D convolutions, in a sense, are really 3D,
because if it's in full color, you've got

746
01:02:41,130 --> 01:02:44,299
3 channels, that's something you've all played
with.

747
01:02:44,299 --> 01:02:50,219
And once you have multiple filters, later
on in your image models, you still have 3D

748
01:02:50,219 --> 01:02:52,509
and you have more than 3 channels.

749
01:02:52,509 --> 01:02:56,279
You might have 32 filters, or 64 filters.

750
01:02:56,279 --> 01:03:04,940
In this lesson, we've introduced one much
simpler concept, but it's still new - the

751
01:03:04,940 --> 01:03:12,650
1D convolution, which is really a 2D convolution,
because just like with images, we have red-green-blue,

752
01:03:12,650 --> 01:03:18,099
now we have the 32 embedding factors.

753
01:03:18,099 --> 01:03:23,219
So that's something you'll definitely need
to experiment with.

754
01:03:23,219 --> 01:03:26,700
Create a model with just an embedding layer,
look at what the output is, what is its shape,

755
01:03:26,700 --> 01:03:36,430
what does it look like and then how does a
1D convolution modify that.

756
01:03:36,430 --> 01:03:43,809
Then trying to understand what an embedding
is is kind of your next big task.

757
01:03:43,809 --> 01:03:48,650
If you're not already feeling comfortable
with it and you haven't seen them before today,

758
01:03:48,650 --> 01:03:49,930
I'm sure you won't.

759
01:03:49,930 --> 01:03:52,670
This is a big, new concept.

760
01:03:52,670 --> 01:03:58,529
It's not in any way mathematically challenging
- it's literally looking up an array, and

761
01:03:58,529 --> 01:04:00,780
returning the thing at that ID.

762
01:04:00,780 --> 01:04:09,289
So an embedding looking at movieId 3 is go
to the third column of the matrix and return

763
01:04:09,289 --> 01:04:10,329
what you see.

764
01:04:10,329 --> 01:04:11,789
That's all an embedding does.

765
01:04:11,789 --> 01:04:17,390
It couldn't be mathematically simpler, it
is the simplest possible operation - return

766
01:04:17,390 --> 01:04:20,689
the thing at this index.

767
01:04:20,689 --> 01:04:27,150
But the kind of intuitive understanding of
what happens when you put an embedding in

768
01:04:27,150 --> 01:04:35,680
an SGD and learn a vector which turns out
to be useful is something which is kind of

769
01:04:35,680 --> 01:04:45,979
mind-blowing because as we saw from the MovieLens
example with just a dot product and this simple

770
01:04:45,979 --> 01:04:53,380
look-up something in an index operation, we
ended up with vectors which captured all kinds

771
01:04:53,380 --> 01:04:58,279
of interesting features about movies, without
us in any was asking it to.

772
01:04:58,279 --> 01:05:06,089
So I kind of wanted to make sure that you
guys felt that after this class, you're going

773
01:05:06,089 --> 01:05:11,410
to go away and try and find a dozen different
ways of looking at these concepts.

774
01:05:11,410 --> 01:05:12,599
[Time: 1.05 hour mark]

775
01:05:12,599 --> 01:05:16,479
One of these ways is to look at how other
people explain them.

776
01:05:16,479 --> 01:05:24,200
Chris Olah has one of the very very best technical
blogs I've come across and I've often referred

777
01:05:24,200 --> 01:05:25,359
to in this class.

778
01:05:25,359 --> 01:05:31,439
In his Understanding Convolutions post (colah.github.io/posts/2014-07-Understanding-Convolutions/),
he has a very interesting example of thinking

779
01:05:31,439 --> 01:05:38,140
about what a dropped ball does as a convolutional
operation, and he shows how you can think

780
01:05:38,140 --> 01:05:44,569
about a 1D convolution using this dropped
ball analogy.

781
01:05:44,569 --> 01:05:49,849
Particularly if you have some background in
mechanical or electrical engineering, I suspect

782
01:05:49,849 --> 01:05:52,789
you'll find this a very helpful example.

783
01:05:52,789 --> 01:05:58,539
There are many resources out there for thinking
about convolutions, and I hope some of you

784
01:05:58,539 --> 01:06:02,719
will share on the forums any that you come
across.

785
01:06:02,719 --> 01:06:11,430
Question: Essentially are we training the
input too?

786
01:06:11,430 --> 01:06:14,650
Answer: Yes, we are absolutely training the
input.

787
01:06:14,650 --> 01:06:24,039
Because the only input we have is 25,000 sequences
of 500 integers.

788
01:06:24,039 --> 01:06:29,619
And so we take each of those integers and
we replace them with a look-up into a 500

789
01:06:29,619 --> 01:06:31,630
column matrix.

790
01:06:31,630 --> 01:06:35,489
Initially that matrix is random.

791
01:06:35,489 --> 01:06:42,030
Just like in our Excel example, we started
with a random matrix (these are all random

792
01:06:42,030 --> 01:06:49,619
numbers) and then we created this loss function,
which was the sum-of-squares of differences

793
01:06:49,619 --> 01:06:52,549
between the dot-product and the rating.

794
01:06:52,549 --> 01:07:01,249
If we then used the gradient descent solver
in Excel to solve that, it attempts to modify

795
01:07:01,249 --> 01:07:10,749
the two embedding matrices (as you can see,
the objective is going down) to try and come

796
01:07:10,749 --> 01:07:17,279
us with the two matrices which give us the
best approximation of the original rating

797
01:07:17,279 --> 01:07:18,749
matrix.

798
01:07:18,749 --> 01:07:27,339
So this Excel spreadsheet is something which
you can play with and do exactly what our

799
01:07:27,339 --> 01:07:33,510
first MovieLens example is doing in Python.

800
01:07:33,510 --> 01:07:45,829
The only difference is the version in Python
also has L2 regularization.

801
01:07:45,829 --> 01:07:48,440
So this one's just finished here.

802
01:07:48,440 --> 01:07:51,410
It's come up that these are no longer random.

803
01:07:51,410 --> 01:07:57,150
We've now got 2 embedding matrices which have
got the loss function down from 40 to 5.6.

804
01:07:57,150 --> 01:08:03,220
So you can see now that these ratings are
very close to what they're meant to be.

805
01:08:03,220 --> 01:08:08,469
So this is exactly what Keras and SGD are
doing in our Python example.

806
01:08:08,469 --> 01:08:19,759
Question: You said we got an embedding in
which each word is a vector of 32 elements?

807
01:08:19,759 --> 01:08:20,779
Answer: Exactly.

808
01:08:20,779 --> 01:08:30,920
Each word in our vocabulary of 5,000 is being
converted into a vector of 32 elements.

809
01:08:30,920 --> 01:08:37,029
Question: What would be the equivalent dense
network if we didn't use a 2D embedding in

810
01:08:37,029 --> 01:08:39,479
the initial model?

811
01:08:39,479 --> 01:08:44,939
A dense layer with input size of embedding
size half size?

812
01:08:44,939 --> 01:08:48,718
Answer: I actually don't actually know what
that is.

813
01:08:48,719 --> 01:08:58,130
Question: Does it matter that encoded values
which are close by in case of color in case

814
01:08:58,130 --> 01:09:02,839
of pictures, which is not word vectors.

815
01:09:02,839 --> 01:09:09,389
For example, 254 and 255 are close as colors
but as words they have no relationship.

816
01:09:09,389 --> 01:09:15,930
Answer: The important thing to realize is
that the word IDs are not used mathematically

817
01:09:15,930 --> 01:09:20,738
in any way at all other than as an index to
look up into integer.

818
01:09:20,738 --> 01:09:26,169
So the fact this this is movie 27, the number
27 is not used in any way.

819
01:09:26,170 --> 01:09:31,350
We just take the number 27 and find its vector.

820
01:09:31,350 --> 01:09:37,009
What's important is the values of each latent
factor as to whether they are close together.

821
01:09:37,009 --> 01:09:42,198
So in the movie example there was some latent
factor "Is it a Hollywood blockbuster?" and

822
01:09:42,198 --> 01:09:47,448
there was some latent factors that were about
"Is it a violent movie or not?".

823
01:09:47,448 --> 01:09:51,129
It is the similarity on those factors that
matter.

824
01:09:51,130 --> 01:09:59,520
The ID is never ever used other than as an
index to simply index into a matrix to return

825
01:09:59,520 --> 01:10:00,520
the vector that we found.

826
01:10:00,520 --> 01:10:01,520
[Time: 1.10 hour mark]

827
01:10:01,520 --> 01:10:06,840
As Ynette was mentioning, in our case now
for the word embeddings we are looking up

828
01:10:06,840 --> 01:10:15,980
in our embeddings to return a 32-element vector
of floats that are initially random and the

829
01:10:15,980 --> 01:10:24,030
model is trying to learn the 32 floats for
each of the words that is semantically useful.

830
01:10:24,030 --> 01:10:28,590
In a moment, we're going to look at some visualizations
of that to try and understand what it's actually

831
01:10:28,590 --> 01:10:29,590
learned.

832
01:10:29,590 --> 01:10:37,790
Question: Can I use the drop-off on the embedding
as well as on the next layer?

833
01:10:37,790 --> 01:10:42,150
What is the signficance and what is the difference
between the two?

834
01:10:42,150 --> 01:10:44,460
Answer: Great question.

835
01:10:44,460 --> 01:10:50,510
You can apply the drop-out parameter to the
embedding layer itself and what that does

836
01:10:50,510 --> 01:11:00,100
is it zeroes out at random 20% of each of
these 32 embeddings to each word.

837
01:11:00,100 --> 01:11:06,120
So its avoiding overfitting the specifics
of each word's embeddding.

838
01:11:06,120 --> 01:11:13,510
This dropout, on the other hand, is removing
at random some of the words effectively, some

839
01:11:13,510 --> 01:11:16,000
of the whole vectors.

840
01:11:16,000 --> 01:11:23,739
The significance of which one to remove where
is not something that I've seen anybody research

841
01:11:23,739 --> 01:11:30,969
in depth, so I'm not sure that we have an
answer that says use this amount in this place.

842
01:11:30,969 --> 01:11:36,340
I just tried a few different values in different
places and it seems that putting the same

843
01:11:36,340 --> 01:11:40,679
amount of drop-out on all these different
spots seems to work pretty well in my experiments.

844
01:11:40,679 --> 01:11:44,150
So it's a reasonable rule-of-thumb.

845
01:11:44,150 --> 01:11:49,010
If you find you're massively overfitting or
massively underfitting, try playing around

846
01:11:49,010 --> 01:11:52,469
with different values and report back to us
on the forum.

847
01:11:52,469 --> 01:11:56,469
Perhaps you'll find some different, better
configurations that I've come up with.

848
01:11:56,469 --> 01:12:06,099
I'm sure some of you will.

849
01:12:06,099 --> 01:12:08,900
Let's think about what's going on here.

850
01:12:08,900 --> 01:12:15,050
We're taking each of the 5,000 words in our
vocabulary and we're replacing them with a

851
01:12:15,050 --> 01:12:21,750
32-element long vector, which we are training
to hopefully capture all of the information

852
01:12:21,750 --> 01:12:26,909
about what this word means and what it does
and how it works.

853
01:12:26,909 --> 01:12:31,750
You might expect intuitively that somebody
might have done this before.

854
01:12:31,750 --> 01:12:38,110
Just like with ImageNet and VGG, you can get
a pre-trained network that says if you've

855
01:12:38,110 --> 01:12:43,480
got an image that looks a bit like a dog,
we've trained a network which has seen lots

856
01:12:43,480 --> 01:12:50,900
of dogs, so it will probably take your dog
image and return useful predictions, because

857
01:12:50,900 --> 01:12:53,099
we've done lots of dog images before.

858
01:12:53,099 --> 01:12:59,070
The interesting thing here is your dog picture
and the VGG author's dog pictures are not

859
01:12:59,070 --> 01:13:03,449
the same; they're going to be different in
all kinds of ways.

860
01:13:03,449 --> 01:13:09,199
So to get pre-trained weights for images,
you have to give somebody a whole pre-trained

861
01:13:09,199 --> 01:13:16,010
network, which is like 500 MBytes worth of
weights and a whole architecture.

862
01:13:16,010 --> 01:13:17,780
Words are much easier.

863
01:13:17,780 --> 01:13:24,809
In a document, the word "dog" always appears
the same way -- it's the word "dog".

864
01:13:24,809 --> 01:13:29,239
It doesn't have different liking conditions,
or facial expressions or whatever, it's just

865
01:13:29,239 --> 01:13:30,290
the word "dog".

866
01:13:30,290 --> 01:13:36,761
The cool thing is that in NLP we don't have
to pass around pre-trained networks, we can

867
01:13:36,761 --> 01:13:43,480
pass around pre-trained embeddings, or as
they're commonly known, pre-trained word vectors.

868
01:13:43,480 --> 01:13:49,909
That is to say other people have already created
big models with big text corpuses where they've

869
01:13:49,909 --> 01:13:57,610
attempted to build a 32-element vector (or
however long vector) which captures all of

870
01:13:57,610 --> 01:14:02,400
the useful information about what that word
is and how it behaves.

871
01:14:02,400 --> 01:14:25,230
So, for example, if we google "word embeddings
download", lots of questions and answers and

872
01:14:25,230 --> 01:14:31,880
pages about where we can download pre-trained
word embeddings.

873
01:14:31,880 --> 01:14:40,159
Question: That's pretty cool.

874
01:14:40,159 --> 01:14:49,659
But I guess what is non-intuitive to me is
I think this means that if I can train a corpus

875
01:14:49,659 --> 01:14:56,710
on the works of Shakespeare somehow that tells
me something about how I can understand movie

876
01:14:56,710 --> 01:14:57,710
reviews.

877
01:14:57,710 --> 01:15:04,060
I imagine that in some sense that's true about
how language is structured, but the meaning

878
01:15:04,060 --> 01:15:08,199
of the word "dog" in Shakespeare is probably
a little bit different.

879
01:15:08,199 --> 01:15:11,040
Answer: We're getting to that now.

880
01:15:11,040 --> 01:15:19,010
[Time: 1.15 hour mark]

881
01:15:19,010 --> 01:15:26,719
So the word vectors that I'm going to be using
(and I don't strongly recommend, but slightly

882
01:15:26,719 --> 01:15:29,199
recommend) the GloVe word vectors.

883
01:15:29,199 --> 01:15:34,429
The other main competition to these is called
the word2vec word vectors.

884
01:15:34,429 --> 01:15:39,090
The GloVe word vectors come from a researcher
named Jeffrey Pennington from Stanford.

885
01:15:39,090 --> 01:15:44,679
The word2vec word vectors come from Google.

886
01:15:44,679 --> 01:15:51,840
I will mention that the TensorFlow documentation
for the word2vec vectors is fantastic, so

887
01:15:51,840 --> 01:15:56,280
I would definitely highly recommend checking
this out.

888
01:15:56,280 --> 01:16:04,290
The GloVe word vectors have been been pre-trained
on a number of different corpuses.

889
01:16:04,290 --> 01:16:13,469
One of them has been pre-trained on all of
Wikipedia and a huge database full of newspaper

890
01:16:13,469 --> 01:16:21,989
articles; 6 billion words, covering a 400,000
size vocabularly.

891
01:16:21,989 --> 01:16:27,820
They provide a 50 dimensional, 100 dimensional,
200 dimensional and 300 dimensional pre-trained

892
01:16:27,820 --> 01:16:29,179
vectors.

893
01:16:29,179 --> 01:16:36,800
They have another one which has been trained
on 40 billion words of a huge dump of the

894
01:16:36,800 --> 01:16:39,409
entire Internet.

895
01:16:39,409 --> 01:16:44,719
And they have another one which has been pre-trained
on 2 billion tweets, of which all of the Donald

896
01:16:44,719 --> 01:16:50,550
Trup tweets have been carefully cleaned out
prior to usage.

897
01:16:50,550 --> 01:16:58,760
So in my case what I've done is I've downloaded
the 6 billion token version and I will show

898
01:16:58,760 --> 01:17:01,840
you what one of these looks like.

899
01:17:01,840 --> 01:17:13,760
Question: Are you losing context because of
the capital letters, punctuation?

900
01:17:13,760 --> 01:17:19,630
Answer: We'll look at that in a moment.

901
01:17:19,630 --> 01:17:27,510
Sometimes these are cased, so you can see
for example this particular one includes case.

902
01:17:27,510 --> 01:17:32,119
There are 2.2 million items in the vocabulary
in this.

903
01:17:32,119 --> 01:17:33,119
Sometimes they're uncased.

904
01:17:33,119 --> 01:17:36,989
And we'll look at punctuation in a moment.

905
01:17:36,989 --> 01:17:44,340
Here is the start of the GloVe 50-dimensional
word vectors trained on a word corpus of 6

906
01:17:44,340 --> 01:17:45,340
billion.

907
01:17:45,340 --> 01:17:57,219
Here is the word "the" and here are 
the 50 floats which attempt to capture all

908
01:17:57,219 --> 01:18:01,260
of the information in the word "the".

909
01:18:01,260 --> 01:18:08,239
Punctuation - here is the word full stop,
and here are the 50 floats that attempt to

910
01:18:08,239 --> 01:18:14,000
capture all of the information captured by
a full stop.

911
01:18:14,000 --> 01:18:15,360
Here is the word "in".

912
01:18:15,360 --> 01:18:17,030
Here is the word double quote.

913
01:18:17,030 --> 01:18:19,650
Here is apostrophe-s.

914
01:18:19,650 --> 01:18:24,030
So you can see that the GloVe authors have
tokenized their text in a particular way and

915
01:18:24,030 --> 01:18:30,380
the idea that apostrophe-s should be treated
as a thing makes a lot of sense.

916
01:18:30,380 --> 01:18:34,710
It certainly has that thing-y-ness in the
English language.

917
01:18:34,710 --> 01:18:42,079
And so the way the authors of a word embedding
corpus have chosen to tokenize their text

918
01:18:42,079 --> 01:18:43,530
definitely matters.

919
01:18:43,530 --> 01:18:48,212
One of the things I like about GloVe is that
they've been pretty smart, in my opinion,

920
01:18:48,212 --> 01:18:49,530
about how they've done this.

921
01:18:49,530 --> 01:18:52,880
Question: What was the target variable?

922
01:18:52,880 --> 01:19:00,989
Answer: The question is how does one create
word vectors in general, what is the model

923
01:19:00,989 --> 01:19:04,360
you're creating and what are the labels you're
building.

924
01:19:04,360 --> 01:19:12,440
One of the 
things that we talked about getting to at

925
01:19:12,440 --> 01:19:15,290
some point is unsupervised learning.

926
01:19:15,290 --> 01:19:17,489
This is a great example of unsupevised learning.

927
01:19:17,489 --> 01:19:25,340
We want to take 840 billion tokens of an Internet
dump and build a model of something.

928
01:19:25,340 --> 01:19:27,530
What do we build a model of?

929
01:19:27,530 --> 01:19:29,410
This is a great example of unsupervised learning.

930
01:19:29,410 --> 01:19:31,830
We're trying to capture some structure of
this data.

931
01:19:31,830 --> 01:19:38,010
In this case, how does English look, work
and feel.

932
01:19:38,010 --> 01:19:42,679
The way that this is done, at least in the
word2vec example, is quite cool.

933
01:19:42,679 --> 01:19:55,219
What do is they take every sentence of 11
words long, every 11 long string of words

934
01:19:55,219 --> 01:19:58,489
that appears in the corpus.

935
01:19:58,489 --> 01:20:03,929
The first thing they do is they create a copy
of it, an exact copy.

936
01:20:03,929 --> 01:20:06,980
[Time: 1.20 hour mark]

937
01:20:06,980 --> 01:20:17,480
In the copy they delete the middle word and
replace it with some random word.

938
01:20:17,480 --> 01:20:24,691
So we now have 2 strings of 11 words, one
of which makes sense because it's real, one

939
01:20:24,691 --> 01:20:29,219
of which probably doesn't make sense because
the middle word has been replaced with something

940
01:20:29,219 --> 01:20:30,219
random.

941
01:20:30,219 --> 01:20:38,610
And so the model task that they create is
the label is 1 if it is a real sentence and

942
01:20:38,610 --> 01:20:44,119
0 if it is a fake sentence; that is the task
they give it.

943
01:20:44,119 --> 01:20:48,639
You can see it's not a directly useful task.

944
01:20:48,639 --> 01:20:53,570
Unless somebody comes along and says I just
found this corpus in which somebody has replaced

945
01:20:53,570 --> 01:20:56,599
half of the middle words with random words.

946
01:20:56,599 --> 01:21:00,260
But it is something where in order to be able
to tackle this task, you're going to have

947
01:21:00,260 --> 01:21:02,400
to know something about language.

948
01:21:02,400 --> 01:21:05,610
You're going to have to be able to recognize
that this sentence doesn't make sense and

949
01:21:05,610 --> 01:21:07,500
this sentence does make sense.

950
01:21:07,500 --> 01:21:10,180
So this is a great example of unsupervised
learning.

951
01:21:10,180 --> 01:21:15,980
Generally speaking, in deep-learning, unsupervised
learning means coming up with a task which

952
01:21:15,980 --> 01:21:20,659
is as close to the task you're going to be
eventually interested in as possible, but

953
01:21:20,659 --> 01:21:25,699
which doesn't require labels, or where the
labels are really cheap to generate.

954
01:21:25,699 --> 01:21:44,530
Question: The language aspect - how does 

955
01:21:44,530 --> 01:21:49,080
this change across languages?

956
01:21:49,080 --> 01:21:59,600
Answer: So it turns out that the embeddings
that is created when you look at Hindu and

957
01:21:59,600 --> 01:22:05,570
Japanese turn out to be nearly the same.

958
01:22:05,570 --> 01:22:15,880
So one way to translate language is to create
a bunch of word vectors in English for various

959
01:22:15,880 --> 01:22:22,150
words, and then to create a bunch of word
vectors in say, Japanese, for various words.

960
01:22:22,150 --> 01:22:30,010
Then what you can do is say I want to translate
this word, which might be "queen" to Japanese.

961
01:22:30,010 --> 01:22:36,260
You can basically look up and find the nearest
word in the same vector space in the Japanese

962
01:22:36,260 --> 01:22:39,820
corpus and it turns out it works.

963
01:22:39,820 --> 01:22:43,400
So it is a fascinating thing about language.

964
01:22:43,400 --> 01:22:50,420
Google has just announced that they replaced
Google Translate with a neural translation

965
01:22:50,420 --> 01:22:54,219
system, and part of what that is doing is
basically doing this.

966
01:22:54,219 --> 01:23:00,840
Here are some interesting examples of some
word embeddings.

967
01:23:00,840 --> 01:23:06,119
The word embeddings for king-queen has the
same distance and direction as the word embedding

968
01:23:06,119 --> 01:23:07,380
for man-woman.

969
01:23:07,380 --> 01:23:10,801
Ditto for walked-walking and swam-swimming.

970
01:23:10,801 --> 01:23:15,050
Ditto for Spain-Madrid and Italy-Rome.

971
01:23:15,050 --> 01:23:22,179
So the embeddings that have to get learned
in order to solve this stupid meaningless

972
01:23:22,179 --> 01:23:27,679
random sentence task are quite amazing.

973
01:23:27,679 --> 01:23:35,070
So I've actually downloading those GloVe embeddings
and I've pre-processed them and I'm going

974
01:23:35,070 --> 01:23:39,780
to upload them for you shortly into a form
that's going to be really easy for you to

975
01:23:39,780 --> 01:23:40,949
use in Python.

976
01:23:40,949 --> 01:23:46,840
And I've created load_glove which preprocessed
stuff that I created for you.

977
01:23:46,840 --> 01:23:48,320
It's going to give you three things.

978
01:23:48,320 --> 01:23:56,070
It's going to give you the word vectors, which
is the 400,000 x 50 dimensional vectors, a

979
01:23:56,070 --> 01:24:04,980
list of the words, and a list of the word
indexes.

980
01:24:04,980 --> 01:24:13,150
So you can now take a word and call word2vec
(w2v) and get back its 50 dimensional array.

981
01:24:13,150 --> 01:24:16,320
And so then I drew a picture.

982
01:24:16,320 --> 01:24:23,350
In order to turn a 50 dimensional vector into
something 2-dimensional that I can plot we

983
01:24:23,350 --> 01:24:25,409
have to do something called dimensionality
reduction.

984
01:24:25,409 --> 01:24:30,280
There's a particular technique (the details
don't really matter) called TSNE, which attempts

985
01:24:30,280 --> 01:24:36,869
to find a way of taking your high-dimensional
information and plot it in 2 dimensions, such

986
01:24:36,869 --> 01:24:41,599
that things that were close in the 50 dimensions
are still close in the 2 dimensions.

987
01:24:41,599 --> 01:24:51,030
So I used TSNE to plot the first 350 most
common words, and here they all are.

988
01:24:51,030 --> 01:24:57,599
You can see that punctuation appears close
to one another, numerals appear close to one

989
01:24:57,599 --> 01:24:58,659
another.

990
01:24:58,659 --> 01:25:03,480
Seasons, games, leagues played are all close
to each another.

991
01:25:03,480 --> 01:25:06,500
Various things about politics, school and
university.

992
01:25:06,500 --> 01:25:10,000
President, general, prime, minister and bush
are close.

993
01:25:10,000 --> 01:25:12,170
[Time: 1.25 hour mark]

994
01:25:12,170 --> 01:25:19,150
This is a great example of where this TNSE
2 dimensional projection is misleading about

995
01:25:19,150 --> 01:25:22,500
the level of complexity that's in these word
vectors.

996
01:25:22,500 --> 01:25:27,179
In a different projection, bush would be very
close to tree.

997
01:25:27,179 --> 01:25:33,900
The 2 dimensional projection is losing a lot
of information, the true detail here is a

998
01:25:33,900 --> 01:25:41,570
lot more complex than us mere humans can see
on a page, but hopefully you get a sense of

999
01:25:41,570 --> 01:25:42,570
this.

1000
01:25:42,570 --> 01:25:50,059
All I've done here is taken those 50 dimensional
word vectors and I've plotted them in 2 dimensions.

1001
01:25:50,059 --> 01:26:00,400
So you can see that when you learn a word
embedding - and not just a word embedding,

1002
01:26:00,400 --> 01:26:05,429
we've seen for movies, we were able to plot
some movies in 2 dimensions and see how they

1003
01:26:05,429 --> 01:26:07,800
relate to each other, and we can do the same
thing for words.

1004
01:26:07,800 --> 01:26:14,679
In general, when you have some high cardinality
categorical variable - could be lots of movies,

1005
01:26:14,679 --> 01:26:20,750
lots of reviewers, lots of words, or whatever
- you can turn it into a useful lower dimensional

1006
01:26:20,750 --> 01:26:24,120
space using this very simple technique of
creating an embedding.

1007
01:26:24,120 --> 01:26:28,880
Question: The explanation on how unsupervised
learning was word2vec was pretty smart.

1008
01:26:28,880 --> 01:26:31,260
How was it done in GloVe?

1009
01:26:31,260 --> 01:26:33,510
Answer: I don't recall how it was done in
GloVe.

1010
01:26:33,510 --> 01:26:35,340
I believe it was something similar.

1011
01:26:35,340 --> 01:26:40,719
I should mention that both GloVe and word2vec
did not use deep-learning.

1012
01:26:40,719 --> 01:26:45,920
They actually tried to create a linear model.

1013
01:26:45,920 --> 01:26:51,139
And the reason they did that was they specifically
wanted to create representations which had

1014
01:26:51,139 --> 01:26:56,949
these kinds of linear relationships because
they felt this would be a useful characteristic

1015
01:26:56,949 --> 01:26:59,470
of these representations.

1016
01:26:59,470 --> 01:27:07,340
I'm not even sure if anybody has even tried
to create a similarly useful representation

1017
01:27:07,340 --> 01:27:10,230
using a deeper model, or whether it turns
out any better.

1018
01:27:10,230 --> 01:27:15,590
Obviously with these linear models, it saves
a lot of computational time as well.

1019
01:27:15,590 --> 01:27:22,739
The embeddings, however, even though they
were built using linear models, we can now

1020
01:27:22,739 --> 01:27:27,610
use them as inputs to deep models, which is
what we're about to do.

1021
01:27:27,610 --> 01:27:38,909
Question: So, Google SyntaxNet, the new model
that just came out, is that the one you were

1022
01:27:38,909 --> 01:27:39,909
mentioning before?

1023
01:27:39,909 --> 01:27:40,909
Answer: No, I was mentioning word2vec.

1024
01:27:40,909 --> 01:27:48,579
Word2vec has been around for 2 or 2.5 years.

1025
01:27:48,579 --> 01:27:53,809
SyntaxNet is a whole framework.

1026
01:27:53,809 --> 01:28:01,539
Question: I think it might have been Parsey?

1027
01:28:01,539 --> 01:28:09,309
That one is the one where it was 97% on NLP
and it also returns parts of speech.

1028
01:28:09,309 --> 01:28:18,469
It will tell you, if you give it a sentence
- this is a word, this is an action verb ...

1029
01:28:18,469 --> 01:28:19,920
Answer: Parsey McParseface.

1030
01:28:19,920 --> 01:28:24,750
All of these word vectors do all of these
things in that high-dimensional space.

1031
01:28:24,750 --> 01:28:30,260
So you can see here (swim-swam), there's information
about tense.

1032
01:28:30,260 --> 01:28:35,309
It's very easy to create a word vector and
use it to create a part of speech recognizer.

1033
01:28:35,309 --> 01:28:40,449
You just need a fairly small labeled corpus
(and it's actually pretty easy to download

1034
01:28:40,449 --> 01:28:46,179
a rather large labeled corpus) and build a
simple model that goes from word vector to

1035
01:28:46,179 --> 01:28:48,619
part of speech.

1036
01:28:48,619 --> 01:28:55,409
There's a really interesting paper, Exploring
the Limits of Language Modeling.

1037
01:28:55,409 --> 01:29:04,010
That Parsey McParseface thing got far more
PR than it deserved.

1038
01:29:04,010 --> 01:29:11,500
It was not really an advance over the state-of-the-art
language models of the time, but since that

1039
01:29:11,500 --> 01:29:15,710
time there have been a lot more interesting
things.

1040
01:29:15,710 --> 01:29:19,580
One of the more interesting papers is Exploring
the Limits of Language Modeling, which is

1041
01:29:19,580 --> 01:29:28,770
looking at what happens when you take a very,
very, very large dataset and spend s-loads

1042
01:29:28,770 --> 01:29:34,760
of Google's money on lots and lots of GPUs
for a very long time.

1043
01:29:34,760 --> 01:29:41,760
They have some genuine massive improvements
to the state-of-the-art in language modeling.

1044
01:29:41,760 --> 01:29:46,030
In general, when we're talking about language
modeling, we're talking about everything from

1045
01:29:46,030 --> 01:29:52,880
"Is this a noun or a verb" to "Is this a happy
sentence or a sad sentence" to "Is this formal

1046
01:29:52,880 --> 01:29:57,349
speech or informal speech", and so on and
so forth.

1047
01:29:57,349 --> 01:30:02,679
And all those things NLP researchers do, we
can now do super-easily with these embeddings.

1048
01:30:02,679 --> 01:30:05,239
[Time: 1.30 hour mark]

1049
01:30:05,239 --> 01:30:13,560
Question: Did they use an optimizer Adam to
generate all those latent factors?

1050
01:30:13,560 --> 01:30:19,390
Answer: So this uses two techniques.

1051
01:30:19,390 --> 01:30:22,130
One of which you know, and one of which you're
about to know.

1052
01:30:22,130 --> 01:30:27,010
Convolutional Neural Networks and Recurrent
Neural Networks, specifically a type called

1053
01:30:27,010 --> 01:30:28,010
LSTM.

1054
01:30:28,010 --> 01:30:32,250
So you can check out this paper to see how
they compare.

1055
01:30:32,250 --> 01:30:36,469
Since this time, there has been an even newer
paper which has furthered the state of the

1056
01:30:36,469 --> 01:30:40,949
art in language modeling, and it's using a
convolutional neural network.

1057
01:30:40,949 --> 01:30:46,510
Right now, CNNs with pre-trained word embeddings
are the state-of-the-art.

1058
01:30:46,510 --> 01:31:04,340
So, given that we can now download these pre-trained
word embeddings, that leads us to the question

1059
01:31:04,340 --> 01:31:12,550
of why are we using randomly generated word
embeddings when we do our sentiment analysis.

1060
01:31:12,550 --> 01:31:18,590
That doesn't seem like a very good idea, and
indeed it is not an even remotely good idea.

1061
01:31:18,590 --> 01:31:21,170
You should never do that.

1062
01:31:21,170 --> 01:31:30,060
From now on, you should now always use pre-trained
word-embeddings, every time you do NLP.

1063
01:31:30,060 --> 01:31:35,639
Over the next few weeks, we will be gradually
be making this easier and easier.

1064
01:31:35,639 --> 01:31:39,750
At this stage, it requires slightly less than
a screen of code.

1065
01:31:39,750 --> 01:31:46,790
You have to load the embeddings off disk,
create your word vectors, your words and your

1066
01:31:46,790 --> 01:31:47,790
word indexes.

1067
01:31:47,790 --> 01:31:54,030
And the next thing you have to do - the word
indexes that come from GloVe are going to

1068
01:31:54,030 --> 01:32:01,030
be different to the word indexes in your vocabulary.

1069
01:32:01,030 --> 01:32:06,440
In our case, 23022 was the word "Bromwell",
in the GloVe case it is probably not the word

1070
01:32:06,440 --> 01:32:07,440
"Bromwell".

1071
01:32:07,440 --> 01:32:14,191
So this little piece of code is something
that is simply mapping from one index to the

1072
01:32:14,191 --> 01:32:16,340
other index.

1073
01:32:16,340 --> 01:32:26,360
So this create embedding function (create_emb)
is then going to create an embedding matrix

1074
01:32:26,360 --> 01:32:33,640
where the indexes are the indexes in the IMDb
dataset, and the embeddings are the embeddings

1075
01:32:33,640 --> 01:32:35,409
from GloVe.

1076
01:32:35,409 --> 01:32:43,480
So that's what emb now contains - the GloVe
word vectors indexed according to the IMDb

1077
01:32:43,480 --> 01:32:44,489
dataset.

1078
01:32:44,489 --> 01:32:51,760
So now I have simply copied and pasted the
previous code and I have added weights equals

1079
01:32:51,760 --> 01:32:57,139
pre-trained embeddings; since we think these
weights are pretty good, I've set trainable

1080
01:32:57,139 --> 01:32:59,599
to False (weights=[emb], trainable=False).

1081
01:32:59,599 --> 01:33:04,989
I won't leave it at False, because we're going
to fine-tune them, but we'll start it at False.

1082
01:33:04,989 --> 01:33:09,830
One particular reason that we cannot leave
it at False is that sometimes I have had to

1083
01:33:09,830 --> 01:33:15,400
create a random embedding, because sometimes
the word that I looked up in GloVe didn't

1084
01:33:15,400 --> 01:33:16,699
exist.

1085
01:33:16,699 --> 01:33:23,900
For example, anything that finishes with apostrophe-s,
in GloVe they tokenize that to have the word

1086
01:33:23,900 --> 01:33:29,489
and apostrophe-s as separate tokens, but in
IMDb, they were combined into one token, so

1087
01:33:29,489 --> 01:33:32,060
all of those things, there aren't vectors
for them.

1088
01:33:32,060 --> 01:33:38,340
So I just randomly created embeddings for
anything that I couldn't find in the GloVe

1089
01:33:38,340 --> 01:33:39,590
vocabulary.

1090
01:33:39,590 --> 01:33:45,080
For now, let's just start using the embeddings
we are given.

1091
01:33:45,080 --> 01:33:50,679
We will set this to non-trainable and we will
train a convolutional neural network using

1092
01:33:50,679 --> 01:33:56,190
those embeddings for the IMDb task.

1093
01:33:56,190 --> 01:33:59,840
After the two epochs, we have 89.8.

1094
01:33:59,840 --> 01:34:09,610
Previously, with random embeddings we had
89.5, and the academic state-of-the-art was

1095
01:34:09,610 --> 01:34:11,830
88.3.

1096
01:34:11,830 --> 01:34:16,699
So we've made significant improvements.

1097
01:34:16,699 --> 01:34:23,460
Let's now go ahead and say first layer trainable
is true (model.layers[0].trainable=True),

1098
01:34:23,460 --> 01:34:26,850
decrease the learning rate a bit (model.optimizer.lr=1e-4)
and do just one more epoch, and we're now

1099
01:34:26,850 --> 01:34:29,909
up to 90.1.

1100
01:34:29,909 --> 01:34:34,030
So we got way beyond the academic state-of-the-art
here.

1101
01:34:34,030 --> 01:34:41,090
We're kind of cheating because we're now not
just building a model, we're now using a pre-trained

1102
01:34:41,090 --> 01:34:44,930
word embedding model that somebody else has
provided for us.

1103
01:34:44,930 --> 01:34:48,510
But why would you ever not do that if that
exists?

1104
01:34:48,510 --> 01:34:54,219
So you can see that we've had a big jump and
it's only taken us 12 seconds to train this

1105
01:34:54,219 --> 01:34:55,219
network.

1106
01:34:55,219 --> 01:34:56,219
[Time: 1.35 hour mark]

1107
01:34:56,219 --> 01:35:06,800
So we started out with the pre-trained word
embeddings, we set them initially to non-trainable

1108
01:35:06,800 --> 01:35:11,440
in order to just train the layers that used
them, waited until that was stable (which

1109
01:35:11,440 --> 01:35:18,389
took really 2 epochs), and then we set them
to trainable and did one more little fine-tuning

1110
01:35:18,389 --> 01:35:19,389
step.

1111
01:35:19,389 --> 01:35:26,670
This kind of approach of these 3 epochs of
training is likely to work for a lot of NLP

1112
01:35:26,670 --> 01:35:27,969
stuff that you'll find.

1113
01:35:27,969 --> 01:35:35,820
Question: Do you not need to compile the model
after resetting the layer to trainable=True?

1114
01:35:35,820 --> 01:35:38,739
Answer: No you don't.

1115
01:35:38,739 --> 01:35:46,590
The architecture of the model has not changed
in any way, it's just changed the metadata

1116
01:35:46,590 --> 01:35:48,110
attached to it.

1117
01:35:48,110 --> 01:35:51,530
There's never any harm in compiling the model.

1118
01:35:51,530 --> 01:35:57,230
Sometimes if you forget to compile, it just
continues to use the old model, so best to

1119
01:35:57,230 --> 01:36:01,600
err on the side of using it.

1120
01:36:01,600 --> 01:36:07,420
Something I thought was pretty cool was during
the week, one of our students here had an

1121
01:36:07,420 --> 01:36:11,840
extremely popular post appear all over the
place (I saw it on the front page of Hacker

1122
01:36:11,840 --> 01:36:17,920
News) talking about how his company, Quid,
uses deep-learning, I'm very happy to see

1123
01:36:17,920 --> 01:36:21,250
with small data.

1124
01:36:21,250 --> 01:36:28,239
For those of you that don't know it, Quid
is a very successful start-up that is processing

1125
01:36:28,239 --> 01:36:33,010
millions and millions of documents - patents
and things like that - and providing enterprise

1126
01:36:33,010 --> 01:36:38,730
customers with really cool visualizations
and interactive tools that lets them analyze

1127
01:36:38,730 --> 01:36:40,929
huge datasets.

1128
01:36:40,929 --> 01:36:44,989
This is by Ben Bowles, one of our students
here, and he talked about how he compared

1129
01:36:44,989 --> 01:36:53,860
three different approaches to NLP classification
tasks, one of which involved some pretty complex

1130
01:36:53,860 --> 01:37:02,239
and slow to develop carefully engineered features.

1131
01:37:02,239 --> 01:37:08,630
But Model 3 in this example was a convolutional
neural network, so I think this is pretty

1132
01:37:08,630 --> 01:37:13,960
cool and I was hoping to talk to Ben about
this piece of work.

1133
01:37:13,960 --> 01:37:22,830
Jeremy: Ben, could you give us a little bit
of context about what you were doing in this

1134
01:37:22,830 --> 01:37:24,769
project?

1135
01:37:24,769 --> 01:37:31,530
Ben: The task is about detecting marketing
language in company descriptions, so it has

1136
01:37:31,530 --> 01:37:33,469
a flavor very similar to sentiment analysis.

1137
01:37:33,469 --> 01:37:38,969
You have two classes of things, they're kind
of different in some kind of semantic way.

1138
01:37:38,969 --> 01:37:39,969
Marketing vs informative.

1139
01:37:39,969 --> 01:37:41,550
And you've got some examples here.

1140
01:37:41,550 --> 01:37:45,159
One was, "Our patent-pending support system
is engineered and designed to bring comfort

1141
01:37:45,159 --> 01:37:48,860
and style" - more marketing, I guess.

1142
01:37:48,860 --> 01:37:53,469
"Spatial Scanning software for mobile devices"
is more informative.

1143
01:37:53,469 --> 01:38:00,610
The semantics of the marketing language is
like, "Oh, this is exciting!" - there are

1144
01:38:00,610 --> 01:38:07,380
certain semantics around which marketing tends
to cluster and I realized that this would

1145
01:38:07,380 --> 01:38:08,780
be a nice task for deep-learning.

1146
01:38:08,780 --> 01:38:11,710
Jeremy: How were these labeled?

1147
01:38:11,710 --> 01:38:14,699
Ben: Basically by a couple of us.

1148
01:38:14,699 --> 01:38:23,060
We found some good ones and we found the bad
ones and literally tried it out.

1149
01:38:23,060 --> 01:38:26,060
Literally as hacky as you could imagine.

1150
01:38:26,060 --> 01:38:27,760
Super, super scrappy.

1151
01:38:27,760 --> 01:38:33,119
But it actually ended up being quite useful
for use.

1152
01:38:33,119 --> 01:38:38,050
That's the lesson there - sometimes scrappy
gets you most of the way you need.

1153
01:38:38,050 --> 01:38:44,060
Think about how you can get your data for
your project - you can just create it.

1154
01:38:44,060 --> 01:38:45,060
Jeremy: Exactly.

1155
01:38:45,060 --> 01:38:54,400
I love this lesson because when I talk to
big executives, they're all about their 5-year

1156
01:38:54,400 --> 01:39:00,869
metadata and data-like repository infrastructure
program, at the end of which maybe they'll

1157
01:39:00,869 --> 01:39:02,369
try to get some value out of it.

1158
01:39:02,369 --> 01:39:08,849
While start-ups are like, okay what have we
got that we can do by Monday; let's throw

1159
01:39:08,849 --> 01:39:10,500
it together and see if it works.

1160
01:39:10,500 --> 01:39:16,369
The latter approach is so much better because
by Monday you know if it looks good, you know

1161
01:39:16,369 --> 01:39:21,159
which kind of things are important and you
can decide on how much its worth investing

1162
01:39:21,159 --> 01:39:22,159
in.

1163
01:39:22,159 --> 01:39:28,250
One of the things I wanted to show is your
convolutional neural network did something

1164
01:39:28,250 --> 01:39:29,810
pretty neat.

1165
01:39:29,810 --> 01:39:35,679
I want to use this same neat trick for our
convolutional neural network, it's a multi-size

1166
01:39:35,679 --> 01:39:37,639
CNN.

1167
01:39:37,639 --> 01:39:47,050
I mentioned earlier that when I built this
CNN and I tried using a filter size of 5 and

1168
01:39:47,050 --> 01:39:50,760
I found it better than 3.

1169
01:39:50,760 --> 01:39:56,440
What Ben points out in his blog post is that
there's a neat paper in which they describe

1170
01:39:56,440 --> 01:40:02,199
doing something interesting - not just using
one size convolution, but trying a few sized

1171
01:40:02,199 --> 01:40:04,570
convolutions.

1172
01:40:04,570 --> 01:40:07,650
You can see here is that this is a great use
of the functional API.

1173
01:40:07,650 --> 01:40:08,829
[Time: 1.40 mark]

1174
01:40:08,829 --> 01:40:14,739
So I haven't exactly used your code, I've
rewritten it a little bit.

1175
01:40:14,739 --> 01:40:22,969
Let's try size 3 and size 4 and size 5 convolutional
filters and let's create one big convolutional

1176
01:40:22,969 --> 01:40:28,099
filter size 3 and then size 4 and then size
5.

1177
01:40:28,099 --> 01:40:33,469
And then for each one, using the functional
API, we'll add MaxPooling, we'll flatten it

1178
01:40:33,469 --> 01:40:37,010
and we'll add it to a list of these different
convolutions.

1179
01:40:37,010 --> 01:40:42,429
And then at the end we will merge them all
together by simply concatenating them -- so

1180
01:40:42,429 --> 01:40:49,159
we'll now have a single vector containing
the result of the 3 and 4 and 5 sized convolutions

1181
01:40:49,159 --> 01:40:51,690
(why settle for one!?).

1182
01:40:51,690 --> 01:40:58,199
And then let's return that whole model as
a little sub-model, which in Ben's code equaled

1183
01:40:58,199 --> 01:40:59,199
"graph".

1184
01:40:59,199 --> 01:41:05,040
I assume you called this graph because people
tend to think of these things and call them

1185
01:41:05,040 --> 01:41:06,599
a computational graph.

1186
01:41:06,599 --> 01:41:17,480
A computational graph is a computation expressed
as a graph of various inputs and outputs.

1187
01:41:17,480 --> 01:41:24,860
Once you've got this little multi-layer convolution
layer module, you can stick it inside a standard

1188
01:41:24,860 --> 01:41:34,880
sequential model by simply replacing the Convolution1D
and MaxPooling1D piece with graph, where graph

1189
01:41:34,880 --> 01:41:40,989
is the concatenated version of all of these
different scales of convolution.

1190
01:41:40,989 --> 01:41:51,320
Trying this out, I got a slightly better answer
again, which is 90.63% -- and I hadn't seen

1191
01:41:51,320 --> 01:41:55,280
that paper before, so thank you for giving
that great idea.

1192
01:41:55,280 --> 01:41:59,099
Jeremy: Did you have anything to add about
this multi-scale convolution idea?

1193
01:41:59,099 --> 01:42:05,150
Ben: Not really, other than I think it's super
cool.

1194
01:42:05,150 --> 01:42:09,110
I'm still trying to figure out all the ins
and outs of exactly how it works.

1195
01:42:09,110 --> 01:42:13,289
In some ways, the implementation is easier
than understanding.

1196
01:42:13,289 --> 01:42:14,750
Jeremy: That's exactly right.

1197
01:42:14,750 --> 01:42:25,611
In a lot of these things, the math is ridiculously
simple, then you throw it at an SGD and let

1198
01:42:25,611 --> 01:42:31,000
it do billions and billions of calculations
in a fraction of a second, and what it comes

1199
01:42:31,000 --> 01:42:32,090
up with is hard to grasp.

1200
01:42:32,090 --> 01:42:33,090
Question: You are using capital-M Merge in
this example?

1201
01:42:33,090 --> 01:42:38,190
Do you want to talk about that?

1202
01:42:38,190 --> 01:42:41,090
Answer: Not really.

1203
01:42:41,090 --> 01:42:46,840
Ben used capital-M Merge and I just did the
same thing.

1204
01:42:46,840 --> 01:43:02,510
Were it me, I would have used small-m merge,
so we'll have to agree to disagree here.

1205
01:43:02,510 --> 01:43:05,610
We have a few minutes to talk about something
enormous.

1206
01:43:05,610 --> 01:43:16,880
We're going to do a brief introduction to
RNNs and next week week we will do a deep

1207
01:43:16,880 --> 01:43:18,120
dive.

1208
01:43:18,120 --> 01:43:24,910
Everything we've learned so far about Convolutional
Neural Networks does not necessarily do a

1209
01:43:24,910 --> 01:43:32,610
great job of solving a problem like, how would
you model this.

1210
01:43:32,610 --> 01:43:42,110
This model has to recognize when you have
a start tag and know to close that tag, but

1211
01:43:42,110 --> 01:43:48,940
over a longer period of time that it's in
a weird XML-y comment thing and to know that

1212
01:43:48,940 --> 01:43:52,330
it has to finish off that weird XML-y comment
thing.

1213
01:43:52,330 --> 01:43:59,079
Which means it has to keep memory about what
happened in the distant past if you're going

1214
01:43:59,079 --> 01:44:07,010
to do modeling with a thing that looks like
this.

1215
01:44:07,010 --> 01:44:13,480
With that kind of memory, it can handle long-term
dependencies.

1216
01:44:13,480 --> 01:44:19,510
Also, think about these two different sentences
("I went to Nepal in 2009", "In 2009, I went

1217
01:44:19,510 --> 01:44:20,510
to Nepal").

1218
01:44:20,510 --> 01:44:24,420
They both mean, effectively, the same thing,
but in order to realize that, you're going

1219
01:44:24,420 --> 01:44:28,880
to have to keep some kind of state that knows
that after this ("In 2009") has been read

1220
01:44:28,880 --> 01:44:32,250
in, you're now talking about something that
happened in 2009.

1221
01:44:32,250 --> 01:44:37,949
And then you then have to remember all the
way to the end of the sentence when it was

1222
01:44:37,949 --> 01:44:41,260
that the thing happened that you did in Nepal.

1223
01:44:41,260 --> 01:44:44,949
So we want to create some kind of stateful
representation.

1224
01:44:44,949 --> 01:44:50,849
Furthermore, it would be nice (if we're going
to deal with big long pieces of language like

1225
01:44:50,849 --> 01:44:55,551
this with a lot of structure) to be able to
handle variable-length sequences - we can

1226
01:44:55,551 --> 01:45:00,239
handle some things that might be really long
and some things that might be really short.

1227
01:45:00,239 --> 01:45:06,510
These are all things that Convolutional Neural
Networks don't necessarily do all that well.

1228
01:45:06,510 --> 01:45:10,699
So we're going to look at something else,
a Recurrent Neural Network, which handles

1229
01:45:10,699 --> 01:45:11,699
that kind of thing well.

1230
01:45:11,699 --> 01:45:12,699
[Time: 1.45 hour mark]

1231
01:45:12,699 --> 01:45:17,719
Here's a great example of a good use of a
Recurrent Neural Network.

1232
01:45:17,719 --> 01:45:23,510
At the top here, you can see that there is
a Convolutional Neural Network that is looking

1233
01:45:23,510 --> 01:45:30,639
at images of house numbers.

1234
01:45:30,639 --> 01:45:37,380
These images are coming from really big Google
StreetView pictures and it has to figure out,

1235
01:45:37,380 --> 01:45:42,659
what part of the image should I look at next
in order to figure out the house number.

1236
01:45:42,659 --> 01:45:47,730
You can see that there is a little square
box that is scanning through and figuring

1237
01:45:47,730 --> 01:45:49,750
I want to look at this piece next.

1238
01:45:49,750 --> 01:45:57,140
At the bottom, it's showing you what it's
actually seeing after each time step.

1239
01:45:57,140 --> 01:46:01,920
So the thing that is figuring out where to
look next is a Recurrent Neural Network.

1240
01:46:01,920 --> 01:46:06,950
It's something that is taking its previous
state and figuring out what should its next

1241
01:46:06,950 --> 01:46:08,330
state be.

1242
01:46:08,330 --> 01:46:16,659
This kind of model is called an Attentional
Model and it's a really interesting field

1243
01:46:16,659 --> 01:46:22,719
of research when it comes to things like very
large images - images which might be too big

1244
01:46:22,719 --> 01:46:29,440
for a single Convolutional Neural Network
with our current hardware constraints.

1245
01:46:29,440 --> 01:46:37,750
On the left is another great example of a
Recurrent Neural Network, an Android and iOS

1246
01:46:37,750 --> 01:46:38,990
text extry system called SwiftKey.

1247
01:46:38,990 --> 01:46:47,489
SwiftKey had a post-up a few months ago where
they announced that they had just replaced

1248
01:46:47,489 --> 01:46:53,139
their language model with a neural network
of this kind, which basically looked at your

1249
01:46:53,139 --> 01:46:58,610
previous words and figured out what word are
you likely to be typing in next, and then

1250
01:46:58,610 --> 01:47:01,260
it could predict that word.

1251
01:47:01,260 --> 01:47:10,400
A final example - Andrej Karpathy showed a
really cool thing where he was able to generate

1252
01:47:10,400 --> 01:47:14,850
random mathematical papers by generating random
LaTeX.

1253
01:47:14,850 --> 01:47:23,300
And to generate random LaTeX you have to learn
things like \begin{proof} and \end{proof}

1254
01:47:23,300 --> 01:47:26,250
and these kind of long-term dependencies.

1255
01:47:26,250 --> 01:47:27,769
He was able to do that successfully.

1256
01:47:27,769 --> 01:47:33,510
So this is a randomly generated piece of LaTeX
which is being created with a Recurrent Neural

1257
01:47:33,510 --> 01:47:36,429
Network.

1258
01:47:36,429 --> 01:47:41,710
So today I am not going to show you exactly
how it works, I'm going to kind of try and

1259
01:47:41,710 --> 01:47:43,920
give you an intuition.

1260
01:47:43,920 --> 01:47:52,630
I'm going to start out by showing you how
to think about neural networks as computational

1261
01:47:52,630 --> 01:48:04,150
graphs (this is coming back to the idea that
Ben used earlier, computational graphs).

1262
01:48:04,150 --> 01:48:11,040
Here is a picture of a single hidden layer
basic neural network.

1263
01:48:11,040 --> 01:48:21,250
We can think of it as having an input of size
batch_size and contain width of number of

1264
01:48:21,250 --> 01:48:23,730
inputs.

1265
01:48:23,730 --> 01:48:29,610
This arrow represents something that we're
doing to that matrix.

1266
01:48:29,610 --> 01:48:36,289
Each of the boxes represents a matrix and
each of the arrows represents one or more

1267
01:48:36,289 --> 01:48:37,909
things we do to that.

1268
01:48:37,909 --> 01:48:43,590
In this case we do a matrix product and then
we throw it through a rectified linear unit.

1269
01:48:43,590 --> 01:48:50,030
And then we get a circle - which represents
a matrix, but it's a hidden layer which is

1270
01:48:50,030 --> 01:48:54,699
of size batch_size by number of activations.

1271
01:48:54,699 --> 01:48:59,870
Number of activations is just - when we created
that dense layer, we would have said "Dense(n",

1272
01:48:59,870 --> 01:49:06,559
and that n is some number representing the
number of activations we create.

1273
01:49:06,559 --> 01:49:12,369
And then we put that through another operation,
which in this case is a matrix product followed

1274
01:49:12,369 --> 01:49:14,110
by a SoftMax.

1275
01:49:14,110 --> 01:49:21,860
And the triangle here represents our output
matrix, which is of size batch_size by, if

1276
01:49:21,860 --> 01:49:24,469
it's ImageNet, 1,000.

1277
01:49:24,469 --> 01:49:32,390
So this is my little way of representing the
computation graph of a basic neural network

1278
01:49:32,390 --> 01:49:34,989
with a single hidden layer.

1279
01:49:34,989 --> 01:49:41,100
I'm now going to create some slightly more
complex models, but I'm going to slightly

1280
01:49:41,100 --> 01:49:43,429
reduce the amount of stuff on the screen.

1281
01:49:43,429 --> 01:49:50,020
One thing to note is that batch_size appears
all the time, so I'm going to get rid of it.

1282
01:49:50,020 --> 01:49:53,219
So here's the same thing where I've removed
batch_size.

1283
01:49:53,219 --> 01:49:59,000
Also the specific activation function, it's
probably reLu everywhere except the last layer

1284
01:49:59,000 --> 01:50:01,300
where it's SoftMax, so I can remove that as
well.

1285
01:50:01,300 --> 01:50:02,329
[Time: 1.50 hour mark]

1286
01:50:02,329 --> 01:50:09,170
So let's now look at what a convolutional
neural network with a single dense hidden

1287
01:50:09,170 --> 01:50:10,480
layer would look like.

1288
01:50:10,480 --> 01:50:16,010
So we'll have our input, which this time will
be (remember, I've removed batch size) number

1289
01:50:16,010 --> 01:50:21,460
of channels by height by width (Input:#channel*h*w),
the operation (we're ignoring the activation

1290
01:50:21,460 --> 01:50:26,610
function) is going to be a convolution followed
by a MaxPool (remember, any shape is representing

1291
01:50:26,610 --> 01:50:27,610
a matrix).

1292
01:50:27,610 --> 01:50:34,179
So that gives us a matrix which will be size
number of filters by height/2 by width/2 [Conv1:#filters*(h/2)*(w/2)],

1293
01:50:34,179 --> 01:50:36,619
since we did a MaxPooling.

1294
01:50:36,619 --> 01:50:38,960
And then we take that and we Flatten it.

1295
01:50:38,960 --> 01:50:44,900
I put Flatten in parantheses because mathematically
Flatten does nothing at all.

1296
01:50:44,900 --> 01:50:49,869
Flattening is just telling Keras to think
of it as a vector.

1297
01:50:49,869 --> 01:50:54,531
It doesn't actually calculate anything, it
doesn't move anything, it doesn't do anything.

1298
01:50:54,531 --> 01:50:59,650
It just says think of it as being a different
shape, that's is why I put it in parantheses.

1299
01:50:59,650 --> 01:51:03,739
So then let's take a matrix product (remember,
I'm not putting in the activation functions

1300
01:51:03,739 --> 01:51:05,320
anymore).

1301
01:51:05,320 --> 01:51:10,980
So that would be our dense layer and gives
us our first fully connected layer which will

1302
01:51:10,980 --> 01:51:15,260
be of size number of activations [FC1:#activations]
and then we put that through a final Matrix

1303
01:51:15,260 --> 01:51:19,099
product to get an output of size number of
classes [Output:#classes].

1304
01:51:19,099 --> 01:51:24,059
So here is how we can represent a convolutional
neural network with a single dense hidden

1305
01:51:24,059 --> 01:51:25,059
layer.

1306
01:51:25,059 --> 01:51:27,400
Question: What is the number of activations?

1307
01:51:27,400 --> 01:51:30,530
Answer: The number of activations is the same
as we had last time.

1308
01:51:30,530 --> 01:51:37,219
It is whatever the n was when we wrote Dense(n).

1309
01:51:37,219 --> 01:51:45,219
Just like the number of filters is when we
write Convolution2D, we say number of filters,

1310
01:51:45,219 --> 01:51:51,329
followed by its size (C2D,n,3)

1311
01:51:51,329 --> 01:51:57,659
So I'm going to create a slightly more complex
computation graph, but again I'm going to

1312
01:51:57,659 --> 01:52:00,400
slightly simplify what I put on the screen.

1313
01:52:00,400 --> 01:52:04,310
This time I'm going to remove all of the layer
operations.

1314
01:52:04,310 --> 01:52:11,321
Now that we have removed the activation function,
you can see that in every case we have some

1315
01:52:11,321 --> 01:52:17,489
kind of linear thing, either a matrix product
or a convolution, and optionally there might

1316
01:52:17,489 --> 01:52:19,389
also be a MaxPool.

1317
01:52:19,389 --> 01:52:23,519
So really this is not adding much additional
information, so I'm going to get rid of that

1318
01:52:23,519 --> 01:52:25,030
[remove: Convolution; max_pool, (Flatten);
matrix product, Matrix product].

1319
01:52:25,030 --> 01:52:27,400
So we're now not showing the layer operations.

1320
01:52:27,400 --> 01:52:33,599
Remember, every arrow is representing one
or more layer operations which will generally

1321
01:52:33,599 --> 01:52:38,320
be a convolution followed by a matrix product
followed by an activation function, and maybe

1322
01:52:38,320 --> 01:52:42,059
there will be a MaxPooling in there as well.

1323
01:52:42,059 --> 01:52:50,860
So let's say we wanted to predict the third
word of a 3-word stream based on the previous

1324
01:52:50,860 --> 01:52:52,219
2 words.

1325
01:52:52,219 --> 01:52:58,440
Now there's all kinds of ways we can do this,
but here's one interesting way, which you

1326
01:52:58,440 --> 01:53:02,510
will now recognize you could do with Keras'
functional API.

1327
01:53:02,510 --> 01:53:13,210
Which is we could take Word-1 input, and that
could be either one-hot encoded thing, in

1328
01:53:13,210 --> 01:53:19,380
which case its size would be vocab size, or
it could be an embedding.

1329
01:53:19,380 --> 01:53:21,659
Doesn't really matter either way.

1330
01:53:21,659 --> 01:53:30,119
We then stick that through a layer operation
to get a matrix output which is our first

1331
01:53:30,119 --> 01:53:32,579
fully connected layer [FC1:#activations].

1332
01:53:32,579 --> 01:53:38,860
And this thing here we could then take and
then put through another layer operation,

1333
01:53:38,860 --> 01:53:45,949
but this time we could also add in the Word-2
input, either of vocab size or the embedding

1334
01:53:45,949 --> 01:53:47,510
of it.

1335
01:53:47,510 --> 01:53:50,199
Put that through a layer operation of its
own.

1336
01:53:50,199 --> 01:53:55,780
Then when we have two arrows coming in together,
that represents a merge.

1337
01:53:55,780 --> 01:54:02,030
A merge could either be done as a sum or a
concat.

1338
01:54:02,030 --> 01:54:06,260
I'm not going to say one is better than the
other; they're two ways that we can take two

1339
01:54:06,260 --> 01:54:09,650
input vectors and combine them together.

1340
01:54:09,650 --> 01:54:18,289
So now, at this point we have the input from
Word-2 (after sticking that through a layer),

1341
01:54:18,289 --> 01:54:24,110
we have the input from Word-1 (after sticking
that through 2 layers), merge them together

1342
01:54:24,110 --> 01:54:28,070
and stick that through another layer to get
our output, which we could then compare to

1343
01:54:28,070 --> 01:54:37,249
Word-3 and try to train that to recognize
Word-3 from Word 1 and Word 2.

1344
01:54:37,249 --> 01:54:38,530
You could try this.

1345
01:54:38,530 --> 01:54:45,480
You could try to build this network from some
corpus you find online and see how it goes.

1346
01:54:45,480 --> 01:54:52,310
Pretty obviously then you could bring it up
another level to try to predict the fourth

1347
01:54:52,310 --> 01:54:58,150
word of a three-word string, using Word-1
and Word-2 and Word-3.

1348
01:54:58,150 --> 01:55:00,199
[Time: 1.55 hour mark]

1349
01:55:00,199 --> 01:55:07,910
The reason I'm doing it in this way is each
time I'm going through another layer operation

1350
01:55:07,910 --> 01:55:10,489
- bringing in Word-1 and going through a layer
operation, bringing in Word-2 and going through

1351
01:55:10,489 --> 01:55:14,909
a layer operation, bringing in Word-3 and
going through a layer operation - is I am

1352
01:55:14,909 --> 01:55:17,860
collecting state.

1353
01:55:17,860 --> 01:55:23,311
Each of these things has the ability to capture
state about all of the words that have come

1354
01:55:23,311 --> 01:55:27,829
so far and the order in which they've arrived.

1355
01:55:27,829 --> 01:55:35,790
By the time I get to Word-4, this matrix has
had the opportunity to learn what does it

1356
01:55:35,790 --> 01:55:40,769
need to know about the previous words' orderings
and how they're connected to each other and

1357
01:55:40,769 --> 01:55:44,749
so forth, in order to predict this fourth
word.

1358
01:55:44,749 --> 01:55:47,690
We're actually capturing state here.

1359
01:55:47,690 --> 01:55:55,159
It's important to note we have not yet built
a model in Keras which has input coming in

1360
01:55:55,159 --> 01:56:00,520
anywhere other than the first layer, but there's
no reason we can't.

1361
01:56:00,520 --> 01:56:06,100
One of you asked a great question earlier
-- could we use this to bring in metadata,

1362
01:56:06,100 --> 01:56:11,170
like the speed a car was going, to add in
the the convolutional neural network's image

1363
01:56:11,170 --> 01:56:12,170
data.

1364
01:56:12,170 --> 01:56:13,170
I said, yes we can.

1365
01:56:13,170 --> 01:56:18,369
In this case, we're doing the same thing;
we're bringing in an additional words worth

1366
01:56:18,369 --> 01:56:19,800
of data.

1367
01:56:19,800 --> 01:56:25,219
Each time you see a convolutional arrow coming
in, that represents a merge operation.

1368
01:56:25,219 --> 01:56:31,530
So here's a perfectly reasonable way of trying
to predict the fourth word from the previous

1369
01:56:31,530 --> 01:56:35,199
three words.

1370
01:56:35,199 --> 01:56:38,429
This leads to a really interesting question.

1371
01:56:38,429 --> 01:56:47,249
What if instead we bring in our Word-1 and
then we had a layer operation in order to

1372
01:56:47,249 --> 01:56:49,699
create a hidden state.

1373
01:56:49,699 --> 01:56:56,449
And that would be enough to predict Word-2.

1374
01:56:56,449 --> 01:57:06,640
And then to predict Word-3 could we just do
a layer operation and generate itself, and

1375
01:57:06,640 --> 01:57:08,909
then that could be used to predict Word-3.

1376
01:57:08,909 --> 01:57:14,210
And run it again to predict Word-4, and again
to predict Word-5.

1377
01:57:14,210 --> 01:57:19,219
This is called an RNN.

1378
01:57:19,219 --> 01:57:25,980
Everything you see here is exactly the same
structurally as I've shown before.

1379
01:57:25,980 --> 01:57:33,409
The colored-in areas represent matrices and
the arrows represent layer operations.

1380
01:57:33,409 --> 01:57:39,140
One of the really interesting things about
an RNN is each of these arrows (here you see

1381
01:57:39,140 --> 01:57:42,730
3 arrows), there's only one weight matrix
attached to those.

1382
01:57:42,730 --> 01:57:49,269
In other words, it's the equivalent thing
of saying every time you see an arrow from

1383
01:57:49,269 --> 01:57:58,370
a circle to a circle, those two weight matrices
have to be exactly the same.

1384
01:57:58,370 --> 01:58:06,599
Everytime you see an arrow from a rectangle
to a circle, those 3 matrices have to be exactly

1385
01:58:06,599 --> 01:58:07,599
the same.

1386
01:58:07,599 --> 01:58:13,099
Finally, you've got an arrow from a circle
to a triangle, and that weight matrix is separate.

1387
01:58:13,099 --> 01:58:19,239
The idea being that if you have a word coming
in and being added to some state, why would

1388
01:58:19,239 --> 01:58:23,829
you want to treat it differently depending
on whether its the first word in a string

1389
01:58:23,829 --> 01:58:28,380
or the third word in a string, given that
generally speaking, we kind of split up strings

1390
01:58:28,380 --> 01:58:30,500
pretty much at random anyway.

1391
01:58:30,500 --> 01:58:43,030
One of the nice things about it going back
to itself is that you can very clearly see

1392
01:58:43,030 --> 01:58:50,679
there is one layer operation (one weight matrix)
for input-to-hidden, one for hidden-to-hidden

1393
01:58:50,679 --> 01:58:59,320
(circle-to-circle), and one for hidden-to-output
(circle-to-triangle).

1394
01:58:59,320 --> 01:59:03,860
We're going to talk about that in a lot more
detail next week.

1395
01:59:03,860 --> 01:59:15,949
For now, I'm going to quickly show you something
in the last minute.

1396
01:59:15,949 --> 01:59:23,130
We can train something which takes, for example,
the text of all the texts of Nietzsche.

1397
01:59:23,130 --> 01:59:28,210
Here's a bit of his text, I just read it in
here.

1398
01:59:28,210 --> 01:59:40,550
We could split it up, go through the whole
text and grab every sequence of length 40.

1399
01:59:40,550 --> 01:59:43,520
Then I've created an RNN.

1400
01:59:43,520 --> 01:59:50,250
It's goal is to take the sentences which represents
the indexes from i to i+40 and predict the

1401
01:59:50,250 --> 01:59:54,179
sentence from i+1 to i+40+1.

1402
01:59:54,179 --> 02:00:02,539
So every string of length maxlen, I'm trying
to predict the string one word after that.

1403
02:00:02,539 --> 02:00:04,380
[Time: 2 hour mark]

1404
02:00:04,380 --> 02:00:10,400
I can take that now and create a model, which
has an LSTM (an LSTM is a kind of Recurrent

1405
02:00:10,400 --> 02:00:17,789
Neural Network, we'll talk about it next week),
starts with an embedding.

1406
02:00:17,789 --> 02:00:30,719
Then I can train that by passing in my sentences,
and my sentence one character later.

1407
02:00:30,719 --> 02:00:37,909
Then I can say let's try and generate 300
characters by building a prediction - what

1408
02:00:37,909 --> 02:00:39,739
do you think the next character will be.

1409
02:00:39,739 --> 02:00:45,639
I have to see it with something, so I seeded
it with something very Nietzsche-n, "ethics

1410
02:00:45,639 --> 02:00:50,210
is a basic foundation of all that".

1411
02:00:50,210 --> 02:00:55,030
After training it for only a few seconds,
I get "ethics is a basic foundation of all

1412
02:00:55,030 --> 02:01:02,429
thatscrriets sdi ,s lrrbmh ..."

1413
02:01:02,429 --> 02:01:08,730
This Nietzsche corpus is slightly annoying
- it has carriage returns after every line,

1414
02:01:08,730 --> 02:01:15,969
and it's got some pretty hideous formatting.

1415
02:01:15,969 --> 02:01:23,800
So then I train it for another 30 seconds,
and it's getting to the point where it kind

1416
02:01:23,800 --> 02:01:27,810
of understands punctuation and spacing.

1417
02:01:27,810 --> 02:01:35,119
And then I train it for 640 seconds and it's
starting to create real words.

1418
02:01:35,119 --> 02:01:39,749
And then I train it for another 640 seconds.

1419
02:01:39,749 --> 02:01:45,400
Interestingly, each section of Nietzsche starts
with a numbered section which looks exactly

1420
02:01:45,400 --> 02:01:46,400
like this.

1421
02:01:46,400 --> 02:01:49,019
It's even starting to learn to close its quotation
marks.

1422
02:01:49,019 --> 02:01:54,739
It's also learned that at the start of a chapter
it always has three blank lines, so it's learned

1423
02:01:54,739 --> 02:01:57,670
to start chapters.

1424
02:01:57,670 --> 02:02:06,019
After another 640 seconds and another 640
seconds, it's actually gotten to a point where

1425
02:02:06,019 --> 02:02:11,070
its saying some things which are so obscure
and difficult to understand it could really

1426
02:02:11,070 --> 02:02:12,659
be Nietsche.

1427
02:02:12,659 --> 02:02:22,380
These car RNN models are fun and all, but
the reason that this is interesting is that

1428
02:02:22,380 --> 02:02:28,309
we are showing that we only provided a small
amount of text and it was able to generate

1429
02:02:28,309 --> 02:02:33,429
text out here, because it has state, it has
recurrence.

1430
02:02:33,429 --> 02:02:38,040
What this means is that we could use this
to generate something like SwiftKey, where

1431
02:02:38,040 --> 02:02:42,920
as you're typing, it's saying this is the
next thing you're going to type.

1432
02:02:42,920 --> 02:02:51,171
I'd like you to think about, during the week,
whether this is likely to help our IMDb sentiment

1433
02:02:51,171 --> 02:02:55,329
model or not, that will be an interesting
thing to talk about.

1434
02:02:55,329 --> 00:00:00,000
Next week we will look into the details of
how RNNs work.

